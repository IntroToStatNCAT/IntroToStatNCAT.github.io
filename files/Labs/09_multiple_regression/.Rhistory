LSL2.tr.mse = c()
Best.LSL2.lamda = c()
LSL2.beta.norm = c()
LSL2.test.mse = c()
LSL2.Beta.Sensitivity = c()
LSL2.Beta.Specificity = c()
for (i in 1:M){
# Spliting full dataset into training and testing data (70-30 split)
split = sample(1:nrow(sim.data), size = floor(0.7*nrow(sim.data)), replace = F)
train.dat = sim.data[split,]
test.dat = cbind(1,sim.data[-split,])
# LASSO Cross Validation Coordinate Descent
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
Mat.LASSO.CV = matrix(unlist(LASSO.CV), nrow = lam.n, byrow = T)
Best.LASSO = Mat.LASSO.CV[which.min(Mat.LASSO.CV[,2]),] #Acquiring the row with the smallest MSE
LASSO.tr.mse[i] = Best.LASSO[2]
Best.LASSO.lamda[i] = Best.LASSO[1]
Best.LASSO.beta = Best.LASSO[-c(1,2)]
LASSO.beta.norm[i] = norm(Beta.True - Best.LASSO.beta, type = "2")
Best.LASSO.beta.Ind = ifelse(Best.LASSO.beta == 0, 0, 1)
Beta.Conf.Mat.LASSO = confusionMatrix(factor(Beta.True.Ind, levels = c(0,1)),
factor(Best.LASSO.beta.Ind, levels = c(0,1)),
positive = "1")
LASSO.Beta.Sensitivity[i] = Beta.Conf.Mat.LASSO $byClass[1]
LASSO.Beta.Specificity[i] = Beta.Conf.Mat.LASSO $byClass[2]
LASSO.test.pred = test.dat[,-2] %*% Best.LASSO.beta
LASSO.test.mse[i] = mean((test.dat[,2]-LASSO.test.pred)^2)
# Elastic Net Cross Validation (0.5)
train.dat.enet = cbind(1,sim.data[split,])
x.tr = train.dat.enet[,-2]
y.tr = train.dat.enet[,2]
grid = 10^seq(10,-5,length=lam.n)
ENET.MOD = glmnet(x.tr, y.tr, alpha = 0.5, lambda = grid)
ENET.CV = cv.glmnet(x.tr, y.tr, alpha = 0.5)
Best.ENET.lamda[i] = ENET.CV$lambda.min
range = c(1,3:(p+2))
Best.ENET.beta = predict(ENET.MOD, type = "coefficients", s = Best.ENET.lamda[i])[range,]
ENET.tr.mse[i] = mean((y.br = x.tr%*%Best.ENET.beta)^2)
ENET.beta.norm[i] = norm(Beta.True - Best.ENET.beta, type = "2")
Best.ENET.beta.Ind = ifelse(Best.ENET.beta == 0, 0, 1)
Beta.Conf.Mat.ENET = confusionMatrix(factor(Beta.True.Ind, levels = c(0,1)),
factor(Best.ENET.beta.Ind, levels = c(0,1)),
positive = "1")
ENET.Beta.Sensitivity[i] = Beta.Conf.Mat.ENET $byClass[1]
ENET.Beta.Specificity[i] = Beta.Conf.Mat.ENET $byClass[2]
ENET.test.pred = predict(ENET.MOD, s = Best.ENET.lamda[i], newx = test.dat[,-2])
ENET.test.mse[i] = mean((test.dat[,2] - ENET.test.pred)^2)
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
Mat.LSL2.CV = matrix(unlist(LSL2.CV), nrow = lam.n, byrow = T)
Best.LSL2 = Mat.LSL2.CV[which.min(Mat.LSL2.CV[,2]),] #Acquiring the row with the smallest MSE
LSL2.tr.mse[i] = Best.LSL2[2]
Best.LSL2.lamda[i] = Best.LSL2[1]
Best.LSL2.beta = Best.LSL2[-c(1,2)]
LSL2.beta.norm[i] = norm(Beta.True - Best.LSL2.beta, type = "2")
Best.LSL2.beta.Ind = ifelse(Best.LSL2.beta == 0, 0, 1)
Beta.Conf.Mat.LSL2 = confusionMatrix(factor(Beta.True.Ind, levels = c(0,1)),
factor(Best.LSL2.beta.Ind, levels = c(0,1)),
positive = "1")
LSL2.Beta.Sensitivity[i] = Beta.Conf.Mat.LSL2$byClass[1]
LSL2.Beta.Specificity[i] = Beta.Conf.Mat.LSL2$byClass[2]
LSL2.test.pred = test.dat[,-2] %*% Best.LSL2.beta
LSL2.test.mse[i] = mean((test.dat[,2]-LSL2.test.pred)^2)
print(i)
flush.console()
}
LASSO.results = data.frame(LASSO.tr.mse, LASSO.test.mse, Best.LASSO.lamda,
LASSO.beta.norm, LASSO.Beta.Sensitivity, LASSO.Beta.Specificity)[1:250,]
ENET.results = data.frame(ENET.tr.mse, ENET.test.mse, Best.ENET.lamda,
ENET.beta.norm, ENET.Beta.Sensitivity, ENET.Beta.Specificity)[1:250,]
#GRL.results = data.frame(GRL.tr.mse, GRL.test.mse, Best.GRL.lamda,
#GRL.beta.norm, GRL.Beta.Sensitivity, GRL.Beta.Specificity)
LSL2.results = data.frame(LSL2.tr.mse, LSL2.test.mse, Best.LSL2.lamda,
LSL2.beta.norm, LSL2.Beta.Sensitivity, LSL2.Beta.Specificity)[1:250,]
write.csv(LASSO.results, "LASSO_G1_rho_0.2.csv", row.names = F)
write.csv(ENET.results, "ENET_G1_rho_0.2.csv", row.names = F)
write.csv(LSL2.results, "LSL2_G1_rho_0.2.csv", row.names = F)
#G1 - RHo = 0.9#############
M = 250 # Number of Monte Carlo Simulations
set.seed(12345)
n = 200; p = 100 # 200 x 100 matrix
sim.data = data.gen.G1(n, p, 0.3, 0.9) #Multicollinear matrix with sigma = 0.3, rho = 0.9
eps = 0.1
tol = 1e-8; max.iter = 500
Beta.True = c(2, rep(2,5), rep(0, p-5))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
lam.n = 50;
lam1.LASSO = seq(0.00001,0.5,length = lam.n)
lam1.LSL2 = seq(0.6,0.9,length = lam.n)
# Empty vectors for LASSO
LASSO.tr.mse = c()
Best.LASSO.lamda = c()
LASSO.beta.norm = c()
LASSO.test.mse = c()
LASSO.Beta.Sensitivity = c()
LASSO.Beta.Specificity = c()
# Empty vectors for ENET (0.5)
ENET.tr.mse = c()
Best.ENET.lamda = c()
ENET.test.mse = c()
ENET.beta.norm = c()
ENET.Beta.Sensitivity = c()
ENET.Beta.Specificity = c()
# Empty vectors for Group LASSO
GRL.tr.mse = c()
Best.GRL.lamda = c()
GRL.test.mse = c()
GRL.beta.norm = c()
GRL.Beta.Sensitivity = c()
GRL.Beta.Specificity = c()
# Empty vectors for LSL2 LogSum L2
LSL2.tr.mse = c()
Best.LSL2.lamda = c()
LSL2.beta.norm = c()
LSL2.test.mse = c()
LSL2.Beta.Sensitivity = c()
LSL2.Beta.Specificity = c()
# Spliting full dataset into training and testing data (70-30 split)
split = sample(1:nrow(sim.data), size = floor(0.7*nrow(sim.data)), replace = F)
train.dat = sim.data[split,]
test.dat = cbind(1,sim.data[-split,])
# LASSO Cross Validation Coordinate Descent
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
lam1.LASSO = seq(0.00001,0.999,length = lam.n)
lam1.LSL2 = seq(0.6,0.9,length = lam.n)
# LASSO Cross Validation Coordinate Descent
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
M = 250 # Number of Monte Carlo Simulations
set.seed(12345)
n = 200; p = 100 # 200 x 100 matrix
sim.data = data.gen.G1(n, p, 0.3, 0.8) #Multicollinear matrix with sigma = 0.3, rho = 0.9
eps = 0.1
tol = 1e-8; max.iter = 500
Beta.True = c(2, rep(2,5), rep(0, p-5))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
lam.n = 50;
lam1.LASSO = seq(0.00001,0.999,length = lam.n)
lam1.LSL2 = seq(0.6,0.9,length = lam.n)
# Empty vectors for LASSO
LASSO.tr.mse = c()
Best.LASSO.lamda = c()
LASSO.beta.norm = c()
LASSO.test.mse = c()
LASSO.Beta.Sensitivity = c()
LASSO.Beta.Specificity = c()
# Empty vectors for ENET (0.5)
ENET.tr.mse = c()
Best.ENET.lamda = c()
ENET.test.mse = c()
ENET.beta.norm = c()
ENET.Beta.Sensitivity = c()
ENET.Beta.Specificity = c()
# Empty vectors for Group LASSO
GRL.tr.mse = c()
Best.GRL.lamda = c()
GRL.test.mse = c()
GRL.beta.norm = c()
GRL.Beta.Sensitivity = c()
GRL.Beta.Specificity = c()
# Empty vectors for LSL2 LogSum L2
LSL2.tr.mse = c()
Best.LSL2.lamda = c()
LSL2.beta.norm = c()
LSL2.test.mse = c()
LSL2.Beta.Sensitivity = c()
LSL2.Beta.Specificity = c()
# Spliting full dataset into training and testing data (70-30 split)
split = sample(1:nrow(sim.data), size = floor(0.7*nrow(sim.data)), replace = F)
train.dat = sim.data[split,]
test.dat = cbind(1,sim.data[-split,])
# LASSO Cross Validation Coordinate Descent
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
M = 250 # Number of Monte Carlo Simulations
set.seed(12345)
n = 200; p = 100 # 200 x 100 matrix
sim.data = data.gen.G1(n, p, 0.3, 0.6) #Multicollinear matrix with sigma = 0.3, rho = 0.6
eps = 0.1
tol = 1e-8; max.iter = 500
Beta.True = c(2, rep(2,5), rep(0, p-5))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
lam.n = 50;
lam1.LASSO = seq(0.00001,0.999,length = lam.n)
lam1.LSL2 = seq(0.6,0.9,length = lam.n)
# Empty vectors for LASSO
LASSO.tr.mse = c()
Best.LASSO.lamda = c()
LASSO.beta.norm = c()
LASSO.test.mse = c()
LASSO.Beta.Sensitivity = c()
LASSO.Beta.Specificity = c()
# Empty vectors for ENET (0.5)
ENET.tr.mse = c()
Best.ENET.lamda = c()
ENET.test.mse = c()
ENET.beta.norm = c()
ENET.Beta.Sensitivity = c()
ENET.Beta.Specificity = c()
# Empty vectors for Group LASSO
GRL.tr.mse = c()
Best.GRL.lamda = c()
GRL.test.mse = c()
GRL.beta.norm = c()
GRL.Beta.Sensitivity = c()
GRL.Beta.Specificity = c()
# Empty vectors for LSL2 LogSum L2
LSL2.tr.mse = c()
Best.LSL2.lamda = c()
LSL2.beta.norm = c()
LSL2.test.mse = c()
LSL2.Beta.Sensitivity = c()
LSL2.Beta.Specificity = c()
# Spliting full dataset into training and testing data (70-30 split)
split = sample(1:nrow(sim.data), size = floor(0.7*nrow(sim.data)), replace = F)
train.dat = sim.data[split,]
test.dat = cbind(1,sim.data[-split,])
# LASSO Cross Validation Coordinate Descent
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
Mat.LASSO.CV = matrix(unlist(LASSO.CV), nrow = lam.n, byrow = T)
Best.LASSO = Mat.LASSO.CV[which.min(Mat.LASSO.CV[,2]),] #Acquiring the row with the smallest MSE
lam.n = 100;
lam1.LASSO = seq(0.01,0.999,length = lam.n)
# LASSO Cross Validation Coordinate Descent
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
View(Mat.LASSO.CV)
Mat.LASSO.CV = matrix(unlist(LASSO.CV), nrow = lam.n, byrow = T)
Best.LASSO = Mat.LASSO.CV[which.min(Mat.LASSO.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LASSO.CV)
#G1 - RHo = 0.8#############
M = 250 # Number of Monte Carlo Simulations
set.seed(12345)
n = 200; p = 100 # 200 x 100 matrix
sim.data = data.gen.G1(n, p, 0.3, 0.6) #Multicollinear matrix with sigma = 0.3, rho = 0.6
eps = 0.01
tol = 1e-8; max.iter = 500
Beta.True = c(2, rep(2,5), rep(0, p-5))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
lam.n = 100;
lam1.LASSO = seq(0.01,0.999,length = lam.n)
lam1.LSL2 = seq(0.6,0.9,length = lam.n)
# Empty vectors for LASSO
LASSO.tr.mse = c()
Best.LASSO.lamda = c()
LASSO.beta.norm = c()
LASSO.test.mse = c()
LASSO.Beta.Sensitivity = c()
LASSO.Beta.Specificity = c()
# Empty vectors for ENET (0.5)
ENET.tr.mse = c()
Best.ENET.lamda = c()
ENET.test.mse = c()
ENET.beta.norm = c()
ENET.Beta.Sensitivity = c()
ENET.Beta.Specificity = c()
# Empty vectors for Group LASSO
GRL.tr.mse = c()
Best.GRL.lamda = c()
GRL.test.mse = c()
GRL.beta.norm = c()
GRL.Beta.Sensitivity = c()
GRL.Beta.Specificity = c()
# Empty vectors for LSL2 LogSum L2
LSL2.tr.mse = c()
Best.LSL2.lamda = c()
LSL2.beta.norm = c()
LSL2.test.mse = c()
LSL2.Beta.Sensitivity = c()
LSL2.Beta.Specificity = c()
# Spliting full dataset into training and testing data (70-30 split)
split = sample(1:nrow(sim.data), size = floor(0.7*nrow(sim.data)), replace = F)
train.dat = sim.data[split,]
test.dat = cbind(1,sim.data[-split,])
# LASSO Cross Validation Coordinate Descent
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
Mat.LASSO.CV = matrix(unlist(LASSO.CV), nrow = lam.n, byrow = T)
Best.LASSO = Mat.LASSO.CV[which.min(Mat.LASSO.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LASSO.CV)
lam1.LSL2 = seq(0.01,0.999,length = lam.n)
eps = 0.1
tol = 1e-8; max.iter = 500
Beta.True = c(2, rep(2,5), rep(0, p-5))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
lam.n = 100;
lam1.LASSO = seq(0.01,0.999,length = lam.n)
lam1.LSL2 = seq(0.01,0.999,length = lam.n)
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
eps = 0.01
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
M = 250 # Number of Monte Carlo Simulations
set.seed(12345)
n = 200; p = 100 # 200 x 100 matrix
sim.data = data.gen.G1(n, p, 0.3, 0.7) #Multicollinear matrix with sigma = 0.3, rho = 0.6
eps = 0.01
tol = 1e-8; max.iter = 500
Beta.True = c(2, rep(2,5), rep(0, p-5))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
lam.n = 100;
lam1.LASSO = seq(0.01,0.999,length = lam.n)
lam1.LSL2 = seq(0.01,0.999,length = lam.n)
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
lam.n = 50;
lam1.LASSO = seq(0.01,0.999,length = lam.n)
lam1.LSL2 = seq(0.01,0.5,length = lam.n)
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
lam1.LSL2 = seq(0.5,0.99,length = lam.n)
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
lam1.LSL2 = seq(0.55,0.99,length = lam.n)
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
lam1.LSL2 = seq(0.6,0.99,length = lam.n)
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
lam1.LSL2 = seq(0.65,0.99,length = lam.n)
# LogSum L2 Cross Validation
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
# LogSum L2 Cross Validation
lam1.LSL2 = seq(0.7,0.99,length = lam.n)
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
# LogSum L2 Cross Validation
lam1.LSL2 = seq(0.74,0.99,length = lam.n)
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
# LogSum L2 Cross Validation
lam1.LSL2 = seq(0.001,0.45,length = lam.n)
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
Mat.LSL2.CV = matrix(unlist(LSL2.CV), nrow = lam.n, byrow = T)
Best.LSL2 = Mat.LSL2.CV[which.min(Mat.LSL2.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LSL2.CV)
algo.lasso = function(lambda, n, p, sim.data, tol, max.iter){
beta = rep(0, p+1)
m = 0
conv = 1 #Convergence
X = cbind(1, sim.data[,-1])
y = as.matrix(sim.data[,1])
z <- sapply(data.frame(X), function(x) sum(x^2))
while (conv > tol && m < max.iter){
new.beta <- w <- rep(0, p+1)
for (j in 1:(p+1)){
w[j] = t(X[,j]) %*% (y - X[,-j] %*% beta[-j])#sum(W*X[,j]*(Z-Z_tilde)) #sum(Z_tilde)
new.beta[j] = ifelse( w[j]/z[j] < -lambda/2, (w[j] + lambda/2)/z[j], ifelse(w[j]/z[j] > lambda/2, (w[j] - lambda/2)/z[j], 0))
}
m = m + 1
conv = sum(abs(new.beta) - abs(beta)) #max(abs((new.beta - beta))^2)
if(is.na(conv)){
conv = 0
}
beta = new.beta
}
MSE = mean((y-X%*%beta)^2)
return(c("Lambda" = lambda, "MSE" = MSE, beta))
}
# Logsum L2 Coordinate descent################################################################################
algo.logsuml2 = function(lam1, n, p, sim.data, eps, tol, max.iter){
lam2 = 1 - lam1
lam = 2 * sqrt(lam1*(1+2*lam2))-(1+2*lam2)*eps
beta = rep(0, p+1)
m = 0
conv = 1 #Convergence
X = cbind(1, sim.data[,-1])
y = as.matrix(sim.data[,1])
while (conv > tol && m < max.iter){
new.beta <- w <- rep(0, p+1)
z <- sapply(data.frame(X), function(x) sum(x^2))
for (j in 1:(p+1)){
w[j] = t(X[,j]) %*% (y - X[,-j] %*% beta[-j]) #sum(W*X[,j]*(Z-Z_tilde)) #sum(Z_tilde)
b_sqrt = (abs(w[j]) + eps*(1+2*lam2))^2 - 4*lam1*(1 + 2*lam2)
b = ((abs(w[j]) - eps*(1+2*lam2)) + sqrt(max(0,b_sqrt)))/(2*(1+2*lam2)*z[j]) #
new.beta[j] = ifelse(abs(w[j]/z[j]) >= lam,
ifelse(w[j] < 0, -b, b), 0)
}
m = m + 1
conv = sum(abs(new.beta - beta)) #max(abs((new.beta - beta))^2)
if(is.na(conv)){
conv = 0
}
beta = new.beta
}
final.result = ifelse(abs(w/z) >= lam, w/z, 0)
MSE = mean((y-X%*%final.result)^2)
return(c("Lam1" = lam1, "MSE" = MSE, final.result))
}
# LASSO Cross Validation Coordinate Descent
lam1.LASSO = seq(0.6,0.999,length = lam.n)
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
Mat.LASSO.CV = matrix(unlist(LASSO.CV), nrow = lam.n, byrow = T)
Best.LASSO = Mat.LASSO.CV[which.min(Mat.LASSO.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LASSO.CV)
# LASSO Cross Validation Coordinate Descent
lam1.LASSO = seq(0.0001,0.9999,length = lam.n)
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
Mat.LASSO.CV = matrix(unlist(LASSO.CV), nrow = lam.n, byrow = T)
Best.LASSO = Mat.LASSO.CV[which.min(Mat.LASSO.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LASSO.CV)
lam.n = 100;
# LASSO Cross Validation Coordinate Descent
lam1.LASSO = seq(0.0001,0.9999,length = lam.n)
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
M = 250 # Number of Monte Carlo Simulations
set.seed(12345)
n = 200; p = 100 # 200 x 100 matrix
sim.data = data.gen.G1(n, p, 0.3, 0.55) #Multicollinear matrix with sigma = 0.3, rho = 0.6
eps = 0.01
tol = 1e-8; max.iter = 500
Beta.True = c(2, rep(2,5), rep(0, p-5))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
lam.n = 100;
lam1.LASSO = seq(0.01,0.999,length = lam.n)
lam1.LSL2 = seq(0.65,0.99,length = lam.n)
# LASSO Cross Validation Coordinate Descent
lam1.LASSO = seq(0.0001,0.9999,length = lam.n)
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
Mat.LASSO.CV = matrix(unlist(LASSO.CV), nrow = lam.n, byrow = T)
Best.LASSO = Mat.LASSO.CV[which.min(Mat.LASSO.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LASSO.CV)
# LogSum L2 Cross Validation
lam1.LSL2 = seq(0.001,0.999,length = lam.n)
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
Mat.LSL2.CV = matrix(unlist(LSL2.CV), nrow = lam.n, byrow = T)
Best.LSL2 = Mat.LSL2.CV[which.min(Mat.LSL2.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LSL2.CV)
eps = 0.1
# LogSum L2 Cross Validation
lam1.LSL2 = seq(0.001,0.999,length = lam.n)
LSL2.CV = lapply(lam1.LSL2, FUN = algo.logsuml2, n, p, train.dat, eps, tol, max.iter)
Mat.LSL2.CV = matrix(unlist(LSL2.CV), nrow = lam.n, byrow = T)
Best.LSL2 = Mat.LSL2.CV[which.min(Mat.LSL2.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LSL2.CV)
#G2 - Rho = 0.2#############
M = 250 # Number of Monte Carlo Simulations
set.seed(12345)
n = 200; p = 100 # 200 x 100 matrix
sim.data = data.gen.G2(n, p, 0.3, 0.2) #Multicollinear matrix with sigma = 0.3, rho = 0.2
eps = 0.1
tol = 1e-8; max.iter = 500
Beta.True = c(rep(2,5),c(1.5, -2.7, 3, -2, 5.3), rep(3,10), rep(0, p-20))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
lam.n = 100;
lam1.LASSO = seq(0.001,0.999,length = lam.n)
lam1.LSL2 = seq(0.001,0.999,length = lam.n)
# LASSO Cross Validation Coordinate Descent
LASSO.CV = lapply(lam1.LASSO, FUN = algo.lasso, n, p, train.dat, tol, max.iter)
Mat.LASSO.CV = matrix(unlist(LASSO.CV), nrow = lam.n, byrow = T)
Best.LASSO = Mat.LASSO.CV[which.min(Mat.LASSO.CV[,2]),] #Acquiring the row with the smallest MSE
View(Mat.LASSO.CV)
library(tidyverse)
library(glmnet)
library(caret)
library(gglasso)
library(parallel)
GGlasso.mod = gglasso(x.tr, y.tr)
group = c(rep(1,6),rep(2,p-5))
GGlasso.CV = cv.gglasso(x.tr,y.tr, group = group)
Best.GGlasso.lamda = GGlasso.CV$lambda.min
View(GGlasso.mod)
GGlasso.mod = gglasso(x.tr, y.tr, lambda = Best.GGlasso.lamda)
View(GGlasso.mod)
Best.GGlasso.beta = c(GGlasso.mod$b0, GGlasso.mod$beta[-1])
GGlasso.pred = predict(GGlasso.mod, s = Best.GGlasso.lamda, type = "link", newx = x.tr)#[range,]
GGlasso.tr.mse = mean((GGlasso.pred -  x.tr%*%Best.GGlasso.beta)^2)
GGlasso.beta.norm = norm(Beta.True - Best.GGlasso.beta, type = "2")
Beta.True = c(2, rep(2,5), rep(0, p-5))
Beta.True.Ind = ifelse(Beta.True == 0, 0, 1)
GGlasso.beta.norm = norm(Beta.True - Best.GGlasso.beta, type = "2")
Best.GGlasso.beta.Ind = ifelse(Best.GGlasso.beta == 0, 0, 1)
Beta.Conf.Mat.GGlasso = confusionMatrix(factor(Beta.True.Ind, levels = c(0,1)),
factor(Best.GGlasso.beta.Ind, levels = c(0,1)),
positive = "1")
Beta.Conf.Mat.GGlasso
library(tidyverse)
library(glmnet)
library(gglasso)
# Logsum L2 Coordinate descent################################################################################
algo.logsuml2 = function(lam1, n, p, sim.data, eps, tol, max.iter){
lam2 = 1 - lam1
lam = 2 * sqrt(lam1*(1+2*lam2))-(1+2*lam2)*eps
beta = rep(0, p+1)
m = 0
conv = 1 #Convergence
X = cbind(1, sim.data[,-1])
y = as.matrix(sim.data[,1])
while (conv > tol && m < max.iter){
new.beta <- w <- rep(0, p+1)
z <- sapply(data.frame(X), function(x) sum(x^2))
for (j in 1:(p+1)){
w[j] = t(X[,j]) %*% (y - X[,-j] %*% beta[-j]) #sum(W*X[,j]*(Z-Z_tilde)) #sum(Z_tilde)
b_sqrt = (abs(w[j]) + eps*(1+2*lam2))^2 - 4*lam1*(1 + 2*lam2)
b = ((abs(w[j]) - eps*(1+2*lam2)) + sqrt(max(0,b_sqrt)))/(2*(1+2*lam2)*z[j]) #
new.beta[j] = ifelse(abs(w[j]/z[j]) >= lam,
ifelse(w[j] < 0, -b, b), 0)
}
m = m + 1
conv = sum(abs(new.beta - beta)) #max(abs((new.beta - beta))^2)
if(is.na(conv)){
conv = 1
}
beta = new.beta
}
final.result = ifelse(abs(w/z) >= lam, w/z, 0)
MSE = mean((y-X%*%final.result)^2, na.rm = T)
return(c("Lam1" = lam1, "MSE" = MSE, final.result))
}
bcTCGA = readRDS("bcTCGA.rds")
data.full = cbind("y" = bcTCGA$y, bcTCGA$X)
set.seed(12345)
p = dim(bcTCGA$X)[2]
split = sample(1:nrow(data.full), size = floor(0.7*nrow(data.full)), replace = F)
train.dat = data.full[split,]
test.dat = cbind(1,data.full[-split,])
setwd("D:/A&T/GRA/Class Material/Labs/09_multiple_regression")
