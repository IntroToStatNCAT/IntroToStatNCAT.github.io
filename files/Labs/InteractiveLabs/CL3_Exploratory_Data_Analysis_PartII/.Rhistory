X.m1 = SimDesign::rmvnorm(n, mean = rep(0, p), sigma = Sig)
colnames(X.m1) = paste0("X", 1:p)
# Interaction effects
id1 = unlist(sapply(1:p, function(x) x:p))
id2 = rep(1:p, p:1)
X.i = sapply(1:length(id1), function(x) X.m1[, id2[x]]*X.m1[, id1[x]])
colnames(X.i) = sapply(1:length(id1), function(x) paste0("X", id2[x], "X", id1[x]))
X = cbind(X.m1, X.i)
}
else{
X.m2 = matrix(runif(n*p), nrow = n, ncol = p)
colnames(X.m2) = paste0("X", 1:p)
# Standardization (Mean Clustering)
X.m2 = apply(X.m2, 2, scale, scale = FALSE) # The quadratic term of a centered variable can avoid collinearity
# Interaction effects
id1 = unlist(sapply(1:p, function(x) x:p))
id2 = rep(1:p, p:1)
X.i = sapply(1:length(id1), function(x) X.m2[, id2[x]]*X.m2[, id1[x]])
colnames(X.i) = sapply(1:length(id1), function(x) paste0("X", id2[x], "X", id1[x]))
X = cbind(X.m2, X.i)
Xt = 2*(X - 0.5)
}
}
# Models from Jain and Xu (2021)
if (mod==1){
Y = 1 + 0.2*X[,1] + 0.3*X[,2] + 0.4*X[,3] + 0.3*X[,1]*X[,2] + rnorm(n, mean = 0, sd = sqrt(.25))
}
if (mod==2){
Y = 1 + 0.4*X[,3] + 0.3*X[,1]*X[,2] + rnorm(n,mean = 0, sd = sqrt(0.25))
}
if (mod==3){
Y = 1 + 0.4*X[,3] + 0.3*(X[,1]>0.5 & X[,2]>0.5) + rnorm(n, mean = 0, sd = sqrt(.25))
}
if (mod==4){
Y = 1 + 0.2*X[,1] + 0.3*X[,2] + 0.3*X[,1]*X[,2] + 0.3*(X[,3]>0.5 & X[,4]>0.5) + rnorm(n, mean = 0, sd = sqrt(.25))
}
Dat = data.frame(cbind(Y, X))
# colnames(Dat) = c("Y", paste0("x", 1:p))
return(Dat)
}
# train.DF <- DF.Gen(n=100,p=25,mod=1)
####################################################################
############### Interaction Selection Algorithms ###################
####################################################################
mse.fun <- function(x,y){mean((x-y)^2)}
LASSO.func <- function(train.DF, test.DF) {
LASSO.time <- system.time(
{
LASSO.Mod <- cv.glmnet(x = as.matrix(train.DF[,-1]), y = as.matrix(train.DF$Y),family = "gaussian", alpha = 1)
LASSO.coef = coef(LASSO.Mod, s="lambda.min")
LASSO.pred.tr = predict(LASSO.Mod, as.matrix(train.DF[,-1]), s = "lambda.min")
LASSO.pred.ts = predict(LASSO.Mod, as.matrix(test.DF[,-1]), s = "lambda.min")
LASSO.MSE.tr = mse.fun(LASSO.pred.tr, train.DF$Y)
LASSO.MSE.ts = mse.fun(LASSO.pred.ts, test.DF$Y)
rm(LASSO.Mod)
}
)
return(list(LASSO.time, LASSO.coef, LASSO.MSE.tr, LASSO.MSE.ts))
}
NCP.func <- function(train.DF, test.DF, method) {
NCP.time <- system.time(
{
NCP.Mod <- cv.ncvreg(X = as.matrix(train.DF[,-1]), y = as.matrix(train.DF$Y), family = "gaussian", penalty = method)
NCP.coef <- coef(NCP.Mod)
NCP.pred.tr = predict(NCP.Mod, as.matrix(train.DF[,-1]))
NCP.pred.ts = predict(NCP.Mod, as.matrix(test.DF[,-1]))
NCP.MSE.tr = mse.fun(NCP.pred.tr, train.DF$Y)
NCP.MSE.ts = mse.fun(NCP.pred.ts, test.DF$Y)
rm(NCP.Mod)
}
)
return(list(NCP.time, NCP.coef, NCP.MSE.tr, NCP.MSE.ts))
}
###################################################
# RAMP ALGORITHM for three penalty functions and two types of interactions
RAMP.func <- function(train.DF, test.DF, penalty.val, hier.val){
RAMP.Time <- system.time(
temp.tr <- RAMP(X = as.matrix(train.DF[, -1]), y = as.matrix(train.DF[, 1]), family = "gaussian",
penalty = penalty.val, gamma = NULL, inter = TRUE, hier = hier.val, eps = 1e-15,
tune = "EBIC", penalty.factor = rep(1, ncol(train.DF[, -1])), inter.penalty.factor = 1,
max.iter = 200, n.lambda = 100, ebic.gamma = 1, refit = TRUE, trace = FALSE))
# RAMP.Time, temp.tr$mainInd, temp.tr$interInd, temp.tr$beta.m, temp.tr$beta.i
temp.fit <- predict(temp.tr, as.matrix(train.DF[, -1]))
temp.pred <- predict(temp.tr, as.matrix(test.DF[, -1]))
mse.tr <- mse.fun(temp.fit, train.DF[,1] )
mse.ts <- mse.fun(temp.pred, test.DF[,1] )
return(list(RAMP.Time, temp.tr$mainInd, temp.tr$interInd, temp.tr$beta.m, temp.tr$beta.i, mse.tr, mse.ts))
}
###################################################
Hier.func <- function(train.DF, test.DF){
Hier.time <- system.time(
{
Hier.path <- hierNet.path(x = as.matrix(train.DF[, -1]), y = as.vector(train.DF[, 1]))
Hier.cv <- hierNet.cv(Hier.path, x = as.matrix(train.DF[, -1]), y = as.vector(train.DF[, 1]))
Hier.tr <- hierNet(x = as.matrix(train.DF[, -1]), y = as.vector(train.DF[, 1]),
lam = Hier.cv$lamhat.1se, strong=TRUE)
}
)
Hier.main <- Hier.tr$bp-Hier.tr$bn
Hier.inter <- Hier.tr$th
Hier.fit <- predict(Hier.tr, as.matrix(train.DF[, -1]))
Hier.pred <- predict(Hier.tr, as.matrix(test.DF[, -1]))
mse.tr <- mse.fun(Hier.fit, train.DF[,1] )
mse.ts <- mse.fun(Hier.pred, test.DF[,1] )
return(list(Hier.time, Hier.main, Hier.inter, mse.tr, mse.ts))
}
###################################################
RF.func <- function(train.DF, test.DF) {
RF.Time <- system.time(
{ RF.imp = Boruta(Y~., data = train.DF, doTrace = 2, maxRuns = 300)
RF.imp.var = rownames(attStats(RF.imp))[attStats(RF.imp)$decision=="Confirmed"]
RF.tr <- randomForest(Y~., data= train.DF)
RF.fit <- predict(RF.tr, as.matrix(train.DF[,-1]))
RF.pred <- predict(RF.tr, as.matrix(test.DF[,-1]))
RF.mse.tr <- mse.fun(RF.fit, train.DF$Y)
RF.mse.ts <- mse.fun(RF.pred, test.DF$Y)
rm(RF.tr)
}
)
return(list(RF.Time, RF.imp.var, RF.mse.tr, RF.mse.ts))
}
iRF.func <- function(train.DF, test.DF) {
iRF.Time <- system.time(
{
iRF.tr <- iRF(x = as.matrix(train.DF[, -1]), y = as.matrix(train.DF[, 1]),
n.iter = 5,
ntree = 500,
n.core = core.num - 2,
mtry.select.prob = rep(1/ncol(train.DF[, -1]), ncol(train.DF[, -1])),
keep.impvar.quantile = NULL,
interactions.return = 5,
wt.pred.accuracy = FALSE,
select.iter = FALSE,
cutoff.unimp.feature = 0,
rit.param = list(depth = 20, ntree = 100, nchild = 2, class.id = 1, class.cut = NULL),
varnames.grp = NULL,
n.bootstrap = 30,
bootstrap.forest = TRUE,
verbose = TRUE)
iRF.fit <- predict(iRF.tr$rf.list[[5]], as.matrix(train.DF[,-1]))
iRF.pred <- predict(iRF.tr$rf.list[[5]], as.matrix(test.DF[,-1]))
iRF.inter <- iRF.tr$interaction[[5]]
iRF.mse.tr <- mse.fun(iRF.fit, train.DF$Y)
iRF.mse.ts <- mse.fun(iRF.pred, test.DF$Y)
rm(iRF.tr)
}
)
return(list(iRF.Time, iRF.inter, iRF.mse.tr, iRF.mse.ts))
}
# generating data and splitting to train/test
DF.Sim <- DF.Gen(n = n.val, p = p.val, mod = mod)
train.id = sample(n, floor(0.60*n), replace = FALSE, prob = NULL)
train.DF = DF.Sim[train.id, ]
test.DF = DF.Sim[-train.id, ]
train.DF1 = DF.Sim[train.id, 1:(p+1) ] #y and X.main only
test.DF1 = DF.Sim[-train.id, 1:(p+1)] #y and X.main only
LASSO.res = LASSO.func(train.DF, test.DF)
SCAD.res = NCP.func(train.DF, test.DF, method="SCAD")
MCP.res = NCP.func(train.DF, test.DF, method="MCP")
LASSO.W.res  = RAMP.func(train.DF1, test.DF1, penalty.val="LASSO", hier.val="Weak")  # Main effects only
LASSO.S.res  = RAMP.func(train.DF1, test.DF1, penalty.val="LASSO", hier.val="Strong")  # Main effects only
SCAD.W.res  = RAMP.func(train.DF1, test.DF1, penalty.val="SCAD", hier.val="Weak")  # Main effects only
SCAD.S.res  = RAMP.func(train.DF1, test.DF1, penalty.val="SCAD", hier.val="Strong")  # Main effects only
MCP.W.res  = RAMP.func(train.DF1, test.DF1, penalty.val="MCP", hier.val="Weak")  # Main effects only
MCP.S.res  = RAMP.func(train.DF1, test.DF1, penalty.val="MCP", hier.val="Strong")  # Main effects only
Hier.res = Hier.func(train.DF1, test.DF1)
iRF.res = iRF.func(train.DF1, test.DF1)                                        # Main effects only
View(Hier.res)
library(tidyverse)
library(MASS) # for mvrnorm
library(mvnfast) # for dmvn
library(Matrix)
library(caret)
library(glmnet)
library(hierNet)
library(ncvreg)
library(RAMP)
library(iRF)
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(Boruta)       # Selection of important features for RF
library(parallel)
(core.num <- detectCores())
DF.Gen <- function(n, p, mod){
# Models from Duroux and Scornet (2018)
{
if(mod < 3){
dependt.Cov = matrix(c(1, 0.3, 0.3, 0.6, 0.6,
0.3, 1, 0.3, 0.2, 0.1,
0.3, 0.3, 1, 0.2, 0.1,
0.6, 0.2, 0.2, 1, 0.1,
0.6, 0.1, 0.1, 0.1, 1), nrow = 5, byrow = TRUE)
Zero.Mat = matrix(0, nrow = 5, ncol = p-5)
Sig = rbind(cbind(dependt.Cov, Zero.Mat), cbind(t(Zero.Mat), diag(p-5)))
# Main effects only
X.m1 = SimDesign::rmvnorm(n, mean = rep(0, p), sigma = Sig)
colnames(X.m1) = paste0("X", 1:p)
# Interaction effects
id1 = unlist(sapply(1:p, function(x) x:p))
id2 = rep(1:p, p:1)
X.i = sapply(1:length(id1), function(x) X.m1[, id2[x]]*X.m1[, id1[x]])
colnames(X.i) = sapply(1:length(id1), function(x) paste0("X", id2[x], "X", id1[x]))
X = cbind(X.m1, X.i)
}
else{
X.m2 = matrix(runif(n*p), nrow = n, ncol = p)
colnames(X.m2) = paste0("X", 1:p)
# Standardization (Mean Clustering)
X.m2 = apply(X.m2, 2, scale, scale = FALSE) # The quadratic term of a centered variable can avoid collinearity
# Interaction effects
id1 = unlist(sapply(1:p, function(x) x:p))
id2 = rep(1:p, p:1)
X.i = sapply(1:length(id1), function(x) X.m2[, id2[x]]*X.m2[, id1[x]])
colnames(X.i) = sapply(1:length(id1), function(x) paste0("X", id2[x], "X", id1[x]))
X = cbind(X.m2, X.i)
Xt = 2*(X - 0.5)
}
}
# Models from Jain and Xu (2021)
if (mod==1){
Y = 1 + 0.2*X[,1] + 0.3*X[,2] + 0.4*X[,3] + 0.3*X[,1]*X[,2] + rnorm(n, mean = 0, sd = sqrt(.25))
}
if (mod==2){
Y = 1 + 0.4*X[,3] + 0.3*X[,1]*X[,2] + rnorm(n,mean = 0, sd = sqrt(0.25))
}
if (mod==3){
Y = 1 + 0.4*X[,3] + 0.3*(X[,1]>0.5 & X[,2]>0.5) + rnorm(n, mean = 0, sd = sqrt(.25))
}
if (mod==4){
Y = 1 + 0.2*X[,1] + 0.3*X[,2] + 0.3*X[,1]*X[,2] + 0.3*(X[,3]>0.5 & X[,4]>0.5) + rnorm(n, mean = 0, sd = sqrt(.25))
}
Dat = data.frame(cbind(Y, X))
# colnames(Dat) = c("Y", paste0("x", 1:p))
return(Dat)
}
# train.DF <- DF.Gen(n=100,p=25,mod=1)
####################################################################
############### Interaction Selection Algorithms ###################
####################################################################
mse.fun <- function(x,y){mean((x-y)^2)}
LASSO.func <- function(train.DF, test.DF) {
LASSO.time <- system.time(
{
LASSO.Mod <- cv.glmnet(x = as.matrix(train.DF[,-1]), y = as.matrix(train.DF$Y),family = "gaussian", alpha = 1)
LASSO.coef = coef(LASSO.Mod, s="lambda.min")
LASSO.pred.tr = predict(LASSO.Mod, as.matrix(train.DF[,-1]), s = "lambda.min")
LASSO.pred.ts = predict(LASSO.Mod, as.matrix(test.DF[,-1]), s = "lambda.min")
LASSO.MSE.tr = mse.fun(LASSO.pred.tr, train.DF$Y)
LASSO.MSE.ts = mse.fun(LASSO.pred.ts, test.DF$Y)
rm(LASSO.Mod)
}
)
return(list(LASSO.time, LASSO.coef, LASSO.MSE.tr, LASSO.MSE.ts))
}
NCP.func <- function(train.DF, test.DF, method) {
NCP.time <- system.time(
{
NCP.Mod <- cv.ncvreg(X = as.matrix(train.DF[,-1]), y = as.matrix(train.DF$Y), family = "gaussian", penalty = method)
NCP.coef <- coef(NCP.Mod)
NCP.pred.tr = predict(NCP.Mod, as.matrix(train.DF[,-1]))
NCP.pred.ts = predict(NCP.Mod, as.matrix(test.DF[,-1]))
NCP.MSE.tr = mse.fun(NCP.pred.tr, train.DF$Y)
NCP.MSE.ts = mse.fun(NCP.pred.ts, test.DF$Y)
rm(NCP.Mod)
}
)
return(list(NCP.time, NCP.coef, NCP.MSE.tr, NCP.MSE.ts))
}
###################################################
# RAMP ALGORITHM for three penalty functions and two types of interactions
RAMP.func <- function(train.DF, test.DF, penalty.val, hier.val){
RAMP.Time <- system.time(
temp.tr <- RAMP(X = as.matrix(train.DF[, -1]), y = as.matrix(train.DF[, 1]), family = "gaussian",
penalty = penalty.val, gamma = NULL, inter = TRUE, hier = hier.val, eps = 1e-15,
tune = "EBIC", penalty.factor = rep(1, ncol(train.DF[, -1])), inter.penalty.factor = 1,
max.iter = 200, n.lambda = 100, ebic.gamma = 1, refit = TRUE, trace = FALSE))
# RAMP.Time, temp.tr$mainInd, temp.tr$interInd, temp.tr$beta.m, temp.tr$beta.i
temp.fit <- predict(temp.tr, as.matrix(train.DF[, -1]))
temp.pred <- predict(temp.tr, as.matrix(test.DF[, -1]))
mse.tr <- mse.fun(temp.fit, train.DF[,1] )
mse.ts <- mse.fun(temp.pred, test.DF[,1] )
return(list(RAMP.Time, temp.tr$mainInd, temp.tr$interInd, temp.tr$beta.m, temp.tr$beta.i, mse.tr, mse.ts))
}
###################################################
Hier.func <- function(train.DF, test.DF){
Hier.time <- system.time(
{
Hier.path <- hierNet.path(x = as.matrix(train.DF[, -1]), y = as.vector(train.DF[, 1]))
Hier.cv <- hierNet.cv(Hier.path, x = as.matrix(train.DF[, -1]), y = as.vector(train.DF[, 1]))
Hier.tr <- hierNet(x = as.matrix(train.DF[, -1]), y = as.vector(train.DF[, 1]),
lam = Hier.cv$lamhat.1se, strong=TRUE)
}
)
Hier.main <- Hier.tr$bp-Hier.tr$bn
Hier.inter <- Hier.tr$th
Hier.fit <- predict(Hier.tr, as.matrix(train.DF[, -1]))
Hier.pred <- predict(Hier.tr, as.matrix(test.DF[, -1]))
mse.tr <- mse.fun(Hier.fit, train.DF[,1] )
mse.ts <- mse.fun(Hier.pred, test.DF[,1] )
return(list(Hier.time, Hier.main, Hier.inter, mse.tr, mse.ts))
}
###################################################
RF.func <- function(train.DF, test.DF) {
RF.Time <- system.time(
{ RF.imp = Boruta(Y~., data = train.DF, doTrace = 2, maxRuns = 300)
RF.imp.var = rownames(attStats(RF.imp))[attStats(RF.imp)$decision=="Confirmed"]
RF.tr <- randomForest(Y~., data= train.DF)
RF.fit <- predict(RF.tr, as.matrix(train.DF[,-1]))
RF.pred <- predict(RF.tr, as.matrix(test.DF[,-1]))
RF.mse.tr <- mse.fun(RF.fit, train.DF$Y)
RF.mse.ts <- mse.fun(RF.pred, test.DF$Y)
rm(RF.tr)
}
)
return(list(RF.Time, RF.imp.var, RF.mse.tr, RF.mse.ts))
}
iRF.func <- function(train.DF, test.DF) {
iRF.Time <- system.time(
{
iRF.tr <- iRF(x = as.matrix(train.DF[, -1]), y = as.matrix(train.DF[, 1]),
n.iter = 5,
ntree = 500,
n.core = core.num - 2,
mtry.select.prob = rep(1/ncol(train.DF[, -1]), ncol(train.DF[, -1])),
keep.impvar.quantile = NULL,
interactions.return = 5,
wt.pred.accuracy = FALSE,
select.iter = FALSE,
cutoff.unimp.feature = 0,
rit.param = list(depth = 20, ntree = 100, nchild = 2, class.id = 1, class.cut = NULL),
varnames.grp = NULL,
n.bootstrap = 30,
bootstrap.forest = TRUE,
verbose = TRUE)
iRF.fit <- predict(iRF.tr$rf.list[[5]], as.matrix(train.DF[,-1]))
iRF.pred <- predict(iRF.tr$rf.list[[5]], as.matrix(test.DF[,-1]))
iRF.inter <- iRF.tr$interaction[[5]]
iRF.mse.tr <- mse.fun(iRF.fit, train.DF$Y)
iRF.mse.ts <- mse.fun(iRF.pred, test.DF$Y)
rm(iRF.tr)
}
)
return(list(iRF.Time, iRF.inter, iRF.mse.tr, iRF.mse.ts))
}
set.seed(12345)
iter = 1
n.val <- 2000 #sample size
p.val <- 100 #number of predictors
n <- 2000 #sample size
p <- 100 #number of predictors
mod <- 1
# generating data and splitting to train/test
DF.Sim <- DF.Gen(n = n.val, p = p.val, mod = mod)
train.id = sample(n, floor(0.60*n), replace = FALSE, prob = NULL)
train.DF = DF.Sim[train.id, ]
test.DF = DF.Sim[-train.id, ]
knitr::opts_chunk$set(echo = TRUE)
Dat.URL <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
# myfile <- readLines(Dat.URL)
# head(myfile)
DF <- read.csv(Dat.URL, header = FALSE, sep = ",", quote = "\"'")
names(DF) <- c('id_number', 'diagnosis', 'radius_mean',
'texture_mean', 'perimeter_mean', 'area_mean',
'smoothness_mean', 'compactness_mean',
'concavity_mean','concave_points_mean',
'symmetry_mean', 'fractal_dimension_mean',
'radius_se', 'texture_se', 'perimeter_se',
'area_se', 'smoothness_se', 'compactness_se',
'concavity_se', 'concave_points_se',
'symmetry_se', 'fractal_dimension_se',
'radius_worst', 'texture_worst',
'perimeter_worst', 'area_worst',
'smoothness_worst', 'compactness_worst',
'concavity_worst', 'concave_points_worst',
'symmetry_worst', 'fractal_dimension_worst')
DF$id_number <- NULL
head(DF)
write.csv(DF,"C:/Users/13363/Desktop/RPackage.Demo/Carmax analytics/DatBreast.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# Dropbox cj.nzekwe@gmail.com
# D1 <- read_table2("https://www.dropbox.com/home/Nzekwe%20(1)/PhD/JSM2021/Breast%20Cancer/training_data/Features.txt", col_names = FALSE)
# D2 <- read_table2("https://www.dropbox.com/home/Nzekwe%20(1)/PhD/JSM2021/Breast%20Cancer/training_data/Info.txt", col_names = FALSE)
DF.1 <- read_table2("C:/Users/13363/Documents/SPRING2021/Dissertation_997/Cancer Codes/Dataset/training_data/Features.txt", col_names = FALSE)
library(markdown)
library(readr)
library(excel.link)
library(Amelia)
library(mice)
library(viridis)
library(hrbrthemes)
library(RColorBrewer)
library(MASS)
library(psych)
library(psycho)
library(Matrix)
library(data.table) # for fread(), a faster read.table.
library(tidyverse) # ggplot and associated packages.
library(caret) # confusionMatrix() and createDataPartition().
library(corrplot) # corrplot() to visualize correlation between variables.
library(rpart) # rpart() for the decision tree model.
library(rpart.plot) # to plot the rpart() model.
library(pROC) # roc() for getting the AUC of the ROC.
library(gbm) # for gbm() and the associated functions.
library(xgboost) # for xgboost().
library(precrec) # for evalmod() to find AUPRC.
library(kableExtra)
library(parallel)
library(doParallel)
library(GGally)
library(RAMP)
library(randomForest)
library(class)
library(graphics)
library(glmnet)
library(ncvreg)
library(stats)
library(iRF)
library(RCurl) #Library to read data from a URL
library(caret)
library(ROSE)
library(tictoc)
# Dropbox cj.nzekwe@gmail.com
# D1 <- read_table2("https://www.dropbox.com/home/Nzekwe%20(1)/PhD/JSM2021/Breast%20Cancer/training_data/Features.txt", col_names = FALSE)
# D2 <- read_table2("https://www.dropbox.com/home/Nzekwe%20(1)/PhD/JSM2021/Breast%20Cancer/training_data/Info.txt", col_names = FALSE)
DF.1 <- read_table2("C:/Users/13363/Documents/SPRING2021/Dissertation_997/Cancer Codes/Dataset/training_data/Features.txt", col_names = FALSE)
DF.2 <- read_table2("C:/Users/13363/Documents/SPRING2021/Dissertation_997/Cancer Codes/Dataset/training_data/Info.txt", col_names = FALSE)
DF.3 <- DF.2 %>%
rename(BC = X1) %>%
dplyr::select(BC)
?grade_code
library(MASS)
library(tidyverse)
library(openintro)
library(ggplot2)
library(learnr)
library(gradethis) #remotes::install_github("rstudio/gradethis")
library(learnrhash) #devtools::install_github("rundel/learnrhash")
tutorial_options(exercise.timelimit = 120, exercise.checker = gradethis::grade_learnr)
mpg %>%
summarize(Mean = mean(hwy),
Median = median(hwy),
SD = sd(hwy),
iqr = IQR(hwy),
Min = min(hwy),
Max = max(hwy),
n = n())
mpg %>%
summarize(Mean = mean(hwy),
Median = median(hwy),
SD = sd(hwy),
iqr = IQR(hwy),
Q1 = quantile(hwy, 0.25),
Q3 = quantile(hwy, 0.75)
Min = min(hwy),
mpg %>%
summarize(Mean = mean(hwy),
Median = median(hwy),
SD = sd(hwy),
iqr = IQR(hwy),
Q1 = quantile(hwy, 0.25),
Q3 = quantile(hwy, 0.75),
Min = min(hwy),
Max = max(hwy),
n = n())
mpg %>%
summarize(Mean = mean(hwy),
Median = median(hwy),
SD = sd(hwy),
Q1 = quantile(hwy, 0.25),
Q3 = quantile(hwy, 0.75),
Min = min(hwy),
Max = max(hwy),
n = n())
ggplot(data = mpg, aes(x = hwy)) + geom_density()
ggplot(data = mpg, aes(x = cty)) + geom_density()
1 - pnorm(q = 600, mean = dqmean, sd = dqsd)
1 - pnorm(q = 600, mean = 25, sd = 1)
1 - pnorm(q = 600, mean = 25, sd = 15)
1 - pnorm(q = 50, mean = 25, sd = 15)
pnorm(q = 50, mean = 25, sd = 15)
data(mpg)
data("mpg")
library(tidyverse)
library(openintro)
library(MASS)
library(readr)
library(learnr)
library(ggplot2)
library(gradethis) #remotes::install_github("rstudio/gradethis")
library(learnrhash) #devtools::install_github("rundel/learnrhash")
library(gapminder)
library(emo) #devtools::install_github("hadley/emo")
library(png)
library(grid)
tutorial_options(exercise.timelimit = 120,
exercise.checker = gradethis::grade_learnr)
gradethis_setup(exercise.reveal_solution = FALSE)
library(tidyverse)
library(openintro)
library(MASS)
library(readr)
library(learnr)
library(ggplot2)
library(gradethis) #remotes::install_github("rstudio/gradethis")
library(learnrhash) #devtools::install_github("rundel/learnrhash")
library(gapminder)
library(emo) #devtools::install_github("hadley/emo")
library(png)
library(grid)
tutorial_options(exercise.timelimit = 120,
exercise.checker = gradethis::grade_learnr)
gradethis_setup(exercise.reveal_solution = FALSE)
