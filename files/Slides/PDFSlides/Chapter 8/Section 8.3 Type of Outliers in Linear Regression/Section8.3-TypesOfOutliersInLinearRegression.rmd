---
title: "Chapter 8"
subtitle: "Introduction to linear regression^[These notes use content from OpenIntro Statistics Slides by Mine Cetinkaya-Rundel.]"
author: |
  | Department of Mathematics & Statistics
  | North Carolina A\&T State University
#date: "8/19/2020"
urlcolor: blue
header-includes:
    \usepackage{multirow}
    \usepackage{graphicx}
    \graphicspath{{images/}}
    \usepackage{subfigure}
    \usepackage{multicol}
    \usepackage[utf8]{inputenc}
    \usepackage[english]{babel}
    \usepackage{bm}
    \usepackage{amsmath}
    \usepackage{color}
    \usepackage{tikz}
    \usetikzlibrary{shapes}
    \usepackage{mathtools}
    \usepackage{textcomp}
    \usepackage{fdsymbol}
    \usepackage{siunitx}
    \usepackage{xcolor,pifont}
    \usepackage{hyperref}
    \usepackage{mathtools}
    \usepackage[geometry]{ifsym}
output: 
  beamer_presentation:
    fig_caption: true
    latex_engine: xelatex
    
    
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=F, message=F, warning=F}
library(readr)
library(openintro)
library(here)
library(tidyverse)
library(xtable)
library(broom)
library(faraway)
data(COL)
```


# Types of outliers in linear regression

## Types of outliers

\begin{columns}

\begin{column}{0.5\textwidth}

\alert{How do outliers influence the least squares in this plot?}

To answer this question think of where the regression line would be with and without the outlier(s). Without the outliers the regression line would be steeper, and lie closer to the larger group of observations. With the outliers the line is pulled up and away from some of the observations in the larger group. 


\end{column}

\begin{column}{0.5\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 4
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 2, mfrow = c(2,1))
lmPlot(x, y, col = COL[1,2], lCol = COL[4], lwd = 3, cex = 2)

```

\end{column}

\end{columns}


## Types of outliers

\begin{columns}

\begin{column}{0.5\textwidth}

\alert{How do outliers influence the least squares in this plot?}

\end{column}

\begin{column}{0.5\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 2, mfrow = c(2,1))
lmPlot(x, y, col = COL[1,2], lCol = COL[4], lwd = 3, cex = 2)

```

\end{column}

\end{columns}


## Types of outliers

\begin{columns}

\begin{column}{0.5\textwidth}

\alert{How do outliers influence the least squares in this plot?}

Without the outlier there is no evident relationship between $x$ and $y$.

\end{column}

\begin{column}{0.5\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 2, mfrow = c(2,1))
lmPlot(x, y, col = COL[1,2], lCol = COL[4], lwd = 3, cex = 2)

```

\end{column}

\end{columns}

## Some terminology

- **Outliers** are points that lie away from the cloud of points.

\pause

- Outliers that lie horizontally away from the center of the cloud are called **high leverage** points.

\pause

- High leverage points that actually influence the \underline{slope} of the regression line are called **influential** points.

\pause

- In order to determine if a point is influential, visualize the regression line with and without the point. Does the slope of the line change considerably? If so, then the point is influential. If not, then it's not an influential point. 

## Influential points

Data are available on the log of the surface temperature and the log of the light intensity of 47 stars in the star cluster CYG OB1.

\begin{columns}

\begin{column}{0.7\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
data(star)

par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 2, cex.axis = 2)

plot(light ~ temp, data = star, pch=19, cex = 2, col=COL[1,2], xlab = "log(temp)", ylab = "log(light intensity)")

abline(lm(light[temp>4]~temp[temp>4], data = star), col = COL[4], lwd = 3)
abline(lm(light~temp, data = star), col = COL[2], lwd = 3, lty = 2)

legend("top", inset = 0.05, c("w/ outliers","w/o outliers"), lty = c(2,1), lwd = c(2,3), col = c(COL[2],COL[4]), cex = 2)

```

\end{column}

\begin{column}{0.3\textwidth}

\includegraphics[width=1\columnwidth]{cyg.pdf}

\end{column}

\end{columns}


## Types of outliers

\begin{columns}

\begin{column}{0.3\textwidth}

\alert{Which of the below best describes the outlier?}

A) Influential

B) High Leverage

C) None of the above

D) There are no outliers

\end{column}

\begin{column}{0.7\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 6
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 2, mfrow = c(2,1))
lmPlot(x, y, col = COL[1,2], lCol = COL[4], lwd = 3, cex = 2)
```

\end{column}

\end{columns}


## Types of outliers

\begin{columns}

\begin{column}{0.3\textwidth}

\alert{Which of the below best describes the outlier?}

A) Influential

B) \alert{High Leverage}

C) None of the above

D) There are no outliers

\end{column}

\begin{column}{0.7\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 6
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 2, mfrow = c(2,1))
lmPlot(x, y, col = COL[1,2], lCol = COL[4], lwd = 3, cex = 2)
```

\end{column}

\end{columns}


## Types of outliers

\begin{columns}

\begin{column}{0.3\textwidth}

\alert{Does this outlier influence the slope of the regression line?}

\end{column}

\begin{column}{0.7\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 1
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
#temp = lm(y~x)
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 2, cex.axis = 2, mfrow = c(2,1))
lmPlot(x, y, col = COL[1,2], lCol = COL[4], lwd = 3, cex = 2)
#abline(temp, lty = 3)
```

\end{column}

\end{columns}


## Types of outliers

\begin{columns}

\begin{column}{0.3\textwidth}

\alert{Does this outlier influence the slope of the regression line?}

Not much...

\end{column}

\begin{column}{0.7\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 1
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
#temp = lm(y~x)
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 2, cex.axis = 2, mfrow = c(2,1))
lmPlot(x, y, col = COL[1,2], lCol = COL[4], lwd = 3, cex = 2)
#abline(temp, lty = 3)
```

\end{column}

\end{columns}


## Recap

\alert{Which of following is \underline{true}?}

A) Influential points always change the intercept of the regression line.

B) Influential points always reduce $R^2$.

C) It is much more likely for a low leverage point to be influential, than a high leverage point.

D) When the data set includes an influential point, the relationship between the explanatory variable and the response variable is always nonlinear.

E) None of the above.


## Recap

\alert{Which of following is \underline{true}?}

A) Influential points always change the intercept of the regression line.

B) Influential points always reduce $R^2$.

C) It is much more likely for a low leverage point to be influential, than a high leverage point.

D) When the data set includes an influential point, the relationship between the explanatory variable and the response variable is always nonlinear.

E) \alert{None of the above.}


## Recap

\begin{columns}

\begin{column}{0.5\textwidth}

\centering{$R = 0.08, R^2 = 0.0064$}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 2, cex.axis = 2, mfrow = c(2,1))
lmPlot(x, y, col = COL[1,2], lCol = COL[4], lwd = 3, cex = 2)

```


\end{column}

\begin{column}{0.5\textwidth}

\centering{$R = 0.79, R^2 = 0.6241$}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
set.seed(238)

n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42,
	runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19,
	c(-17,-20,-21,-19), 12, -23.2)
i = 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 2, cex.axis = 2, mfrow = c(2,1))
lmPlot(x[1:70], y[1:70], col = COL[1,2], lCol = COL[4], cex = 2,lwd = 3, xlim = range(x), ylim = range(y))
#abline(temp, lty = 3)
```

\end{column}

\end{columns}