---
title: "Chapter 9"
subtitle: "Multiple and logistic regression^[These notes use content from OpenIntro Statistics Slides by Mine Cetinkaya-Rundel.]"
author: |
  | Department of Mathematics & Statistics
  | North Carolina A\&T State University
#date: "8/19/2020"
urlcolor: blue
header-includes:
    \usepackage{multirow}
    \usepackage{graphicx}
    \usepackage{subfigure}
    \usepackage{multicol}
    \usepackage[utf8]{inputenc}
    \usepackage[english]{babel}
    \usepackage{bm}
    \usepackage{amsmath}
    \usepackage{color}
    \usepackage{tikz}
    \usetikzlibrary{shapes}
    \usepackage{mathtools}
    \usepackage{textcomp}
    \usepackage{fdsymbol}
    \usepackage{siunitx}
    \usepackage{xcolor,pifont}
    \usepackage{hyperref}
    \usepackage{mathtools}
    \usepackage[geometry]{ifsym}
output: 
  beamer_presentation:
    fig_caption: true
    latex_engine: xelatex
    
    
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=F, message=F, warning=F}
library(readr)
library(openintro)
library(here)
library(tidyverse)
library(xtable)
library(broom)
library(DAAG)
library(Sleuth3)
library(ROCR)
data(allbacks)
data(COL)
```

# Introduction to multiple regression

## Multuple regression

- Simple linear regression: Bivariate - two variables: $y$ and $x$.

- Multiple linear regression: Multiple variables: $y$ and $x_1, x_2, \cdots$


## Poverty vs. Region (east, west)

\centering{$\widehat{poverty} = 11.17 + 0.38 \times west$}

\begin{itemize}

\item Explanatory variable: region, \textbf{reference level:} east

\item \textbf{Intercept:} The estimated average poverty percentage in eastern states is 11.17\%
\pause
\begin{itemize}
\item This is the value we get if we plug in \textcolor{red}{0} for the explanatory variable
\end{itemize}

\pause

\item \textbf{Slope:} The estimated average poverty percentage in western states is 0.38\% higher than eastern states.
\pause
\begin{itemize}
\item Then, the estimated average poverty percentage in western states is 11.17 + 0.38 =  11.55\%.
\pause
\item This is the value we get if we plug in \textcolor{red}{1} for the explanatory variable
\end{itemize}

\end{itemize}
  

## Poverty vs. Region (northeast, midwest, west, south)

\alert{Which region (northeast, midwest, west, or south) is the reference level?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 9.50 & 0.87 & 10.94 & 0.00 \\ 
region4midwest & 0.03 & 1.15 & 0.02 & 0.98 \\ 
region4west & 1.79 & 1.13 & 1.59 & 0.12 \\ 
region4south & 4.16 & 1.07 & 3.87 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}


\begin{enumerate}[A)]
\item northeast
\item midwest
\item west
\item south
\item cannot tell
\end{enumerate}

## Poverty vs. Region (northeast, midwest, west, south)

\alert{Which region (northeast, midwest, west, or south) is the reference level?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 9.50 & 0.87 & 10.94 & 0.00 \\ 
region4midwest & 0.03 & 1.15 & 0.02 & 0.98 \\ 
region4west & 1.79 & 1.13 & 1.59 & 0.12 \\ 
region4south & 4.16 & 1.07 & 3.87 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}


\begin{enumerate}[A)]
\item \alert{northeast}
\item midwest
\item west
\item south
\item cannot tell
\end{enumerate}


## Poverty vs. Region (northeast, midwest, west, south)

\alert{Which region (northeast, midwest, west, or south) has the lowest poverty percentage?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 9.50 & 0.87 & 10.94 & 0.00 \\ 
region4midwest & 0.03 & 1.15 & 0.02 & 0.98 \\ 
region4west & 1.79 & 1.13 & 1.59 & 0.12 \\ 
region4south & 4.16 & 1.07 & 3.87 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}


\begin{enumerate}[A)]
\item northeast
\item midwest
\item west
\item south
\item cannot tell
\end{enumerate}

## Poverty vs. Region (northeast, midwest, west, south)

\alert{Which region (northeast, midwest, west, or south) has the lowest poverty percentage?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 9.50 & 0.87 & 10.94 & 0.00 \\ 
region4midwest & 0.03 & 1.15 & 0.02 & 0.98 \\ 
region4west & 1.79 & 1.13 & 1.59 & 0.12 \\ 
region4south & 4.16 & 1.07 & 3.87 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}


\begin{enumerate}[A)]
\item \alert{northeast}
\item midwest
\item west
\item south
\item cannot tell
\end{enumerate}


## Weights of books

\begin{columns}

\begin{column}{0.6\textwidth}

\footnotesize
\begin{center}
\begin{tabular}{rrrc}
  \hline
 & weight (g) & volume (cm$^\text{3}$) & cover \\ 
  \hline
1 & 800 & 885 & hc \\ 
  2 & 950 & 1016 & hc \\ 
  3 & 1050 & 1125 & hc \\ 
  4 & 350 & 239 & hc \\ 
  5 & 750 & 701 & hc \\ 
  6 & 600 & 641 & hc \\ 
  7 & 1075 & 1228 & hc \\ 
  8 & 250 & 412 & pb \\ 
  9 & 700 & 953 & pb \\ 
  10 & 650 & 929 & pb \\ 
  11 & 975 & 1492 & pb \\ 
  12 & 350 & 419 & pb \\ 
  13 & 950 & 1010 & pb \\ 
  14 & 425 & 595 & pb \\ 
  15 & 725 & 1034 & pb \\ 
   \hline
\end{tabular}
\end{center}

\end{column}

\begin{column}{0.4\textwidth}

\includegraphics[width=1\textwidth]{book.pdf}

\end{column}

\end{columns}


## Weights of books

\begin{columns}

\begin{column}{0.4\textwidth}

\alert{The scatterplot shows the relationship between weights and volumes of books as well as the regression output. Which of the below is correct?}

\end{column}

\begin{column}{0.6\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
# scatterplot: weight vs. volume

m1 = lm(weight ~ volume, data = allbacks)

par(mar=c(4,4,1,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.7, cex.axis = 1.5)

plot(weight ~ volume, data = allbacks, pch = 19, col = COL[1,2], xlab = expression(volume~(cm^{3})), ylab = "weight (g)", cex = 2)
abline(m1, lwd = 3, col = COL[4])
text(x = 600, y = 1000, expression(paste(widehat(weight)," = 108 + 0.7 volume")), cex = 1.8, col = COL[1], pos = 1)
text(x = 600, y = 900, expression(paste(R^{2},"= 80%")), cex = 1.7, col = COL[1], pos = 1)

```

\end{column}

\end{columns}

\begin{enumerate}[A)]
\item Weights of 80\% of the books can be predicted accurately using this model.
\item Books that are 10 cm$^\text{3}$ over average are expected to weigh 7 g over average.
\item The correlation between weight and volume is $R = 0.80^2 = 0.64$.
\item The model underestimates the weight of the book with the highest volume.
\end{enumerate}

## Weights of books

\begin{columns}

\begin{column}{0.4\textwidth}

\alert{The scatterplot shows the relationship between weights and volumes of books as well as the regression output. Which of the below is correct?}

\end{column}

\begin{column}{0.6\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
# scatterplot: weight vs. volume

m1 = lm(weight ~ volume, data = allbacks)

par(mar=c(4,4,1,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.7, cex.axis = 1.5)

plot(weight ~ volume, data = allbacks, pch = 19, col = COL[1,2], xlab = expression(volume~(cm^{3})), ylab = "weight (g)", cex = 2)
abline(m1, lwd = 3, col = COL[4])
text(x = 600, y = 1000, expression(paste(widehat(weight)," = 108 + 0.7 volume")), cex = 1.8, col = COL[1], pos = 1)
text(x = 600, y = 900, expression(paste(R^{2},"= 80%")), cex = 1.7, col = COL[1], pos = 1)

```

\end{column}

\end{columns}

\begin{enumerate}[A)]
\item Weights of 80\% of the books can be predicted accurately using this model.
\item \alert{Books that are 10 cm$^\text{3}$ over average are expected to weigh 7 g over average.}
\item The correlation between weight and volume is $R = 0.80^2 = 0.64$.
\item The model underestimates the weight of the book with the highest volume.
\end{enumerate}

## Modeling weights of books using volume

\small
```{r}
summary(m1)
```

## Weights of hardcover and paperback books

\alert{Can you identify a trend in the relationship between volume and weight of hardcover and paper books?}

```{r, echo=F, message=F, warning=F, out.width="80%",fig.align='center'}
# scatterplot: weight vs. volume and cover

ch = rep(NA, dim(allbacks)[1])
ch[allbacks$cover == "hb"] = 15
ch[allbacks$cover == "pb"] = 17

color = rep(NA, dim(allbacks)[1])
color[allbacks$cover == "hb"] = COL[1,2]
color[allbacks$cover == "pb"] = COL[2,2]

par(mar=c(4,4,0.25,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.8, cex.axis = 1.25)

plot(weight ~ volume, data = allbacks, col = color, xlab = expression(volume~(cm^{3})), ylab = "weight (g)", pch = ch, cex = 2)
legend("topleft", inset = 0.05, c("hardcover","paperback"), col = c(COL[1,2],COL[2,2]), pch = c(15,17), cex = 1.8)
```


## Weights of hardcover and paperback books

\alert{Can you identify a trend in the relationship between volume and weight of hardcover and paper books?}

Paperbacks generally weight less than hardcover books after controlling for the book's volume.

```{r, echo=F, message=F, warning=F, out.width="80%",fig.align='center'}
# scatterplot: weight vs. volume and cover

ch = rep(NA, dim(allbacks)[1])
ch[allbacks$cover == "hb"] = 15
ch[allbacks$cover == "pb"] = 17

color = rep(NA, dim(allbacks)[1])
color[allbacks$cover == "hb"] = COL[1,2]
color[allbacks$cover == "pb"] = COL[2,2]

par(mar=c(4,4,0.25,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.8, cex.axis = 1.25)

plot(weight ~ volume, data = allbacks, col = color, xlab = expression(volume~(cm^{3})), ylab = "weight (g)", pch = ch, cex = 2)
legend("topleft", inset = 0.05, c("hardcover","paperback"), col = c(COL[1,2],COL[2,2]), pch = c(15,17), cex = 1.8)
```

## Modeling weights of books using volume \underline{and} cover type

\small
```{r}
m2 <- lm(weight ~ volume + cover, data = allbacks)
summary(m2)
```

## Determining the reference level

\alert{Based on the regression output below, which level of \texttt{cover} is the reference level? Note that \texttt{pb}: paperback.}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 197.9628 & 59.1927 & 3.34 & 0.0058 \\ 
  volume & 0.7180 & 0.0615 & 11.67 & 0.0000 \\ 
  cover:pb & -184.0473 & 40.4942 & -4.55 & 0.0007 \\ 
   \hline
\end{tabular}
\end{center}

A) paperback

B) hardcover


## Determining the reference level

\alert{Based on the regression output below, which level of \texttt{cover} is the reference level? Note that \texttt{pb}: paperback.}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 197.9628 & 59.1927 & 3.34 & 0.0058 \\ 
  volume & 0.7180 & 0.0615 & 11.67 & 0.0000 \\ 
  cover:pb & -184.0473 & 40.4942 & -4.55 & 0.0007 \\ 
   \hline
\end{tabular}
\end{center}

A) paperback

B) \alert{hardcover}

## Determining the reference level

\alert{Which of the below correctly describes the roles of variables in this regression model?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 197.9628 & 59.1927 & 3.34 & 0.0058 \\ 
  volume & 0.7180 & 0.0615 & 11.67 & 0.0000 \\ 
  cover:pb & -184.0473 & 40.4942 & -4.55 & 0.0007 \\ 
   \hline
\end{tabular}
\end{center}

A) response: weight, explanatory: volume, paperback cover.

B) response: weight, explanatory: volume, hardcover cover.

C) response: volume, explanatory: weight, cover type.

D) response: weight, explanatory: volume, cover type.

## Determining the reference level

\alert{Which of the below correctly describes the roles of variables in this regression model?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 197.9628 & 59.1927 & 3.34 & 0.0058 \\ 
  volume & 0.7180 & 0.0615 & 11.67 & 0.0000 \\ 
  cover:pb & -184.0473 & 40.4942 & -4.55 & 0.0007 \\ 
   \hline
\end{tabular}
\end{center}

A) response: weight, explanatory: volume, paperback cover.

B) response: weight, explanatory: volume, hardcover cover.

C) response: volume, explanatory: weight, cover type.

D) \alert{response: weight, explanatory: volume, cover type.}

## Linear model

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 197.96 & 59.19 & 3.34 & 0.01 \\ 
  volume & 0.72 & 0.06 & 11.67 & 0.00 \\ 
  cover:pb & -184.05 & 40.49 & -4.55 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}


\pause

\[ \widehat{weight} = 197.96 + 0.72~volume - 184.05~cover:pb  \]

\pause

\begin{enumerate}

\item For \textbf{hardcover} books: plug in \textcolor{red}{0} for \texttt{cover}
\begin{eqnarray*}
\widehat{weight} &=& 197.96 + 0.72~volume - 184.05 \times \textcolor{red}{0} \\
\pause
&=& 197.96 +  0.72~volume
\end{eqnarray*}

\pause

\item For \textbf{paperback} books: plug in \textcolor{red}{1} for \texttt{cover}
\begin{eqnarray*}
\widehat{weight} &=& 197.96 + 0.72~volume - 184.05 \times \textcolor{red}{1} \\
\pause
&=& 13.91 +  0.72~volume
\end{eqnarray*}

\end{enumerate}

## Visualizing the linear model

```{r, echo=F, message=F, warning=F, out.width="80%",fig.align='center'}
# scatterplot: weight vs. volume and cover with lines

par(mar=c(4,4,0.25,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.8, cex.axis = 1.25)

plot(weight ~ volume, data = allbacks, col = color, xlab = expression(volume~(cm^{3})), ylab = "weight (g)", pch = ch, cex = 3)
legend("topleft", inset = 0.05, c("hardcover","paperback"), col = c(COL[1,2],COL[2,2]), pch = c(15,17), cex = 1.8)
abline(a = m2$coefficients[1], b = m2$coefficients[2], col = COL[1], lwd = 2)
abline(a = m2$coefficients[1] + m2$coefficients[3], b = m2$coefficients[2], col = COL[2], lwd = 2, lty = 2.5)

```


## Interpretation of the regression coefficients

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 197.96 & 59.19 & 3.34 & 0.01 \\ 
  volume & 0.72 & 0.06 & 11.67 & 0.00 \\ 
  cover:pb & -184.05 & 40.49 & -4.55 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}


\pause

\begin{itemize}

\item \textbf{Slope of volume:} \underline{All else held constant}, books that are 1 more cubic centimeter in volume tend to weigh about 0.72 grams more.

\pause

\item \textbf{Slope of cover:} \underline{All else held constant}, the model predicts that paperback books weigh 184 grams lower than hardcover books.

\pause

\item \textbf{Intercept:} Hardcover books with no volume are expected on average to weigh 198 grams. \pause
\begin{itemize}
\item Obviously, the intercept does not make sense in context. It only serves to adjust the height of the line.
\end{itemize}

\end{itemize}

## Prediction

\alert{Which of the following is the correct calculation for the predicted weight of a paperback book that is $600~cm^3?$}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 197.96 & 59.19 & 3.34 & 0.01 \\ 
  volume & 0.72 & 0.06 & 11.67 & 0.00 \\ 
  cover:pb & -184.05 & 40.49 & -4.55 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}

A) $197.96 + 0.72 \times 600 - 184.05 \times 1$

B) $184.05 + 0.72 \times 600 - 197.96 \times 1$

C) $197.96 + 0.72 \times 600 - 184.05 \times 0$

D) $197.96 + 0.72 \times 1 - 184.05 \times 600$


## Prediction

\alert{Which of the following is the correct calculation for the predicted weight of a paperback book that is $600~cm^3?$}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 197.96 & 59.19 & 3.34 & 0.01 \\ 
  volume & 0.72 & 0.06 & 11.67 & 0.00 \\ 
  cover:pb & -184.05 & 40.49 & -4.55 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}

A) \alert{$197.96 + 0.72 \times 600 - 184.05 \times 1 = 445.91~grams$}

B) $184.05 + 0.72 \times 600 - 197.96 \times 1$

C) $197.96 + 0.72 \times 600 - 184.05 \times 0$

D) $197.96 + 0.72 \times 1 - 184.05 \times 600$


## Another example: Modeling kid's test scores

Predicting cognitive test scores of three- and four-year-old children using characteristics of their mothers. Data are from a survey of adult American women and their children - a subsample from the National Longitudinal Survey of Youth.

\begin{center}
\begin{tabular}{rrlrlr}
  \hline
 & kid\_score & mom\_hs & mom\_iq & mom\_work & mom\_age \\ 
  \hline
1 &  65 & yes & 121.12 & yes &  27 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
  5 & 115 & yes & 92.75 & yes &  27 \\ 
  6 &  98 & no & 107.90 & no &  18 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
  434 &  70 & yes & 91.25 & yes &  25 \\ 
   \hline
\end{tabular}
\end{center}

## Interpreting the slope

\alert{What is the correct interpretation of the \underline{slope for mom's IQ}?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 19.59 & 9.22 & 2.13 & 0.03 \\ 
  mom\_hs:yes & 5.09 & 2.31 & 2.20 & 0.03 \\ 
  mom\_iq & 0.56 & 0.06 & 9.26 & 0.00 \\ 
  mom\_work:yes & 2.54 & 2.35 & 1.08 & 0.28 \\ 
  mom\_age & 0.22 & 0.33 & 0.66 & 0.51 \\ 
   \hline
\end{tabular}
\end{center}

\pause

All else held constant, kids with mothers whose IQs are one point higher tend to score on average 0.56 points higher.

## Interpreting the slope

\alert{What is the correct interpretation of the \underline{intercept}?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 19.59 & 9.22 & 2.13 & 0.03 \\ 
  mom\_hs:yes & 5.09 & 2.31 & 2.20 & 0.03 \\ 
  mom\_iq & 0.56 & 0.06 & 9.26 & 0.00 \\ 
  mom\_work:yes & 2.54 & 2.35 & 1.08 & 0.28 \\ 
  mom\_age & 0.22 & 0.33 & 0.66 & 0.51 \\ 
   \hline
\end{tabular}
\end{center}

\pause

Kids whose moms haven't gone to HS, did not work during the first three years of the kid's life, have an IQ of 0 and are 0 yrs old are expected on average to score 19.59. Obviously, the intercept does not make any sense in context.


## Interpreting the slope

\alert{What is the correct interpretation of the slope for \texttt{mom_work}?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 19.59 & 9.22 & 2.13 & 0.03 \\ 
  mom\_hs:yes & 5.09 & 2.31 & 2.20 & 0.03 \\ 
  mom\_iq & 0.56 & 0.06 & 9.26 & 0.00 \\ 
  mom\_work:yes & 2.54 & 2.35 & 1.08 & 0.28 \\ 
  mom\_age & 0.22 & 0.33 & 0.66 & 0.51 \\ 
   \hline
\end{tabular}
\end{center}

All else being equal, kids whose moms worked during the first three years of the kid's life

A) are estimated to score 2.54 points lower.

B) are estimated to score 2.54 points higher.

than those whose moms did not work.

## Interpreting the slope

\alert{What is the correct interpretation of the slope for \texttt{mom_work}?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 19.59 & 9.22 & 2.13 & 0.03 \\ 
  mom\_hs:yes & 5.09 & 2.31 & 2.20 & 0.03 \\ 
  mom\_iq & 0.56 & 0.06 & 9.26 & 0.00 \\ 
  mom\_work:yes & 2.54 & 2.35 & 1.08 & 0.28 \\ 
  mom\_age & 0.22 & 0.33 & 0.66 & 0.51 \\ 
   \hline
\end{tabular}
\end{center}

All else being equal, kids whose moms worked during the first three years of the kid's life

A) are estimated to score 2.54 points lower.

B) \alert{are estimated to score 2.54 points higher.}

than those whose moms did not work.


## Revisit: Modeling poverty


```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
poverty = read.table("poverty.txt", h = T, sep = "\t")

# rename columns

names(poverty) = c("state", "metro_res", "white", "hs_grad", "poverty", "female_house")

# reorder columns

poverty = poverty[,c(1,5,2,3,4,6)]

# pairs plot

panel.cor <- function(x, y, digits=2, prefix="", cex.cor, ...){
	usr <- par("usr"); on.exit(par(usr))
	par(usr = c(0, 1, 0, 1))
	r <- abs(cor(x, y))
	rreal = cor(x, y)
	txtreal <- format(c(rreal, 0.123456789), digits=digits)[1]
	txt <- format(c(r, 0.123456789), digits=digits)[1]
	if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
	text(0.5, 0.5, txtreal, cex = 3 * cex.cor * r)
}

pairs(poverty[,c(2:6)], lower.panel = panel.cor, pch = 19, col = COL[1,2], cex = 1.5, cex.labels = 1.6, cex.axis = 1.5)
```

## Predicting poverty using % female householder

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 3.31 & 1.90 & 1.74 & 0.09 \\ 
  female\_house & 0.69 & 0.16 & 4.32 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}

\begin{columns}

\begin{column}{0.7\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
# poverty vs. female_house

pov_slr = lm(poverty ~ female_house, data = poverty)

par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 2, cex.axis = 1.5)
plot(poverty$poverty ~ poverty$female_house, ylab = "% in poverty", xlab = "% female householder", pch=19, col=COL[1,2], cex = 2)
abline(pov_slr, col = COL[4], lwd = 4)
```

\end{column}

\begin{column}{0.3\textwidth}

\begin{align*}
R &= 0.53 \\
R^2 &= 0.53^2 = 0.28
\end{align*}

\end{column}

\end{columns}

## Another look at $R^2$

$R^2$ can be calculated in three ways:

\pause

\begin{enumerate}

\item square the correlation coefficient of $x$ and $y$ {\small (how we have been calculating it)}

\pause

\item square the correlation coefficient of $y$ and $\hat{y}$

\pause

\item based on definition: 

\centering{$R^2 = \frac{\text{explained variability in }y}{\text{total variability in }y}$}

\end{enumerate}

\pause

\raggedright
Using \textbf{ANOVA} we can calculate the explained variability and total variability in $y$.


## Sum of squares

\begin{center}
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
female\_house & 1 & 132.57 & 132.57 & 18.68 & 0.00 \\ 
  Residuals & 49 & 347.68 & 7.10 &  &  \\ 
   \hline
   \hline
\end{tabular}
\end{center}


\pause

\small
\begin{eqnarray*}
\text{Sum of squares of y: } SS_{Total} &=& \sum(y - \bar{y})^2 = 480.25 \\
& & \textcolor{red}{ {\footnotesize ~\rightarrow~total~variability}} \\
\pause
\text{Sum of squares of residuals: } SS_{Error} &=& \sum e_i^2 = 347.68 \\
& &\textcolor{red}{ { \footnotesize ~\rightarrow~unexplained~variability}} \\
\pause
\text{Sum of squares of x: } SS_{Model} &=& SS_{Total} - SS_{Error} \\
& &\textcolor{red}{ {\footnotesize ~\rightarrow~explained~variability}} \\
&=& 480.25 - 347.68 = 132.57
\end{eqnarray*}

## Why bother?

\alert{Why bother with another approach for calculating $R^2$ when we had a perfectly good way to calculate it as the correlation coefficient squared?}

\pause

\begin{itemize}
\item For single-predictor linear regression, having three ways to calculate the same value may seem like overkill. 
\item However, in multiple linear regression, we can't calculate $R^2$ as the square of the correlation between $x$ and $y$ because we have multiple $x$s. 
\item And next we'll learn another measure of explained variability, $\mathbf{adjusted~R^2}$, that requires the use of the third approach, ratio of explained and unexplained variability. 
\end{itemize}

## Predicting poverty using % female hh + % white

\begin{center}
\begin{tabular}{rrrrr}
  \hline
\textbf{Linear model:}&  Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -2.58 & 5.78 & -0.45 & 0.66 \\ 
  female\_house & 0.89 & 0.24 & 3.67 & 0.00 \\ 
  white & 0.04 & 0.04 & 1.08 & 0.29 \\ 
   \hline
\end{tabular}
\end{center}

\vspace{0.1cm}

\begin{center}
\begin{tabular}{lrrrrr}
  \hline
\textbf{ANOVA:} & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
female\_house & 1 & 132.57 & 132.57 & 18.74 & 0.00 \\ 
  white & 1 & 8.21 & 8.21 & 1.16 & 0.29 \\ 
  Residuals & 48 & 339.47 & 7.07 &  &  \\ 
   \hline
Total & 50 &    480.25\\
   \hline
\end{tabular}
\end{center}

\pause

\[ R^2 = \frac{\text{explained variability}}{\text{total variability}} = \frac{132.57 + 8.21}{480.25} = 0.29 \]

##

\alert{Does adding the variable \texttt{white} to the model add valuable information that wasn't provided by \texttt{female\_house}?}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
pairs(poverty[,c(2:6)], lower.panel = panel.cor, pch = 19, col = COL[1,2], cex = 1.5, cex.labels = 1.6, cex.axis = 1.5)
```


## Collinearity between explanatory variables

\textbf{poverty vs. \% female head of household}
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 3.31 & 1.90 & 1.74 & 0.09 \\ 
  female\_house & 0.69 & 0.16 & 4.32 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}


\textbf{poverty vs. \% female head of household and \% female hh}
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -2.58 & 5.78 & -0.45 & 0.66 \\ 
  female\_house & 0.89  & 0.24 & 3.67 & 0.00 \\ 
  white & 0.04 & 0.04 & 1.08 & 0.29 \\   
   \hline
\end{tabular}
\end{center}


## Collinearity between explanatory variables

\textbf{poverty vs. \% female head of household}
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 3.31 & 1.90 & 1.74 & 0.09 \\ 
  female\_house & \alert{0.69} & 0.16 & 4.32 & 0.00 \\ 
   \hline
\end{tabular}
\end{center}


\textbf{poverty vs. \% female head of household and \% female hh}
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -2.58 & 5.78 & -0.45 & 0.66 \\ 
  female\_house & \alert{0.89}  & 0.24 & 3.67 & 0.00 \\ 
  white & 0.04 & 0.04 & 1.08 & 0.29 \\   
   \hline
\end{tabular}
\end{center}

## Collinearity between explanatory variables

\begin{itemize}

\item Two predictor variables are said to be collinear when they are correlated, and this \textbf{collinearity} complicates model estimation. \\

\footnotesize
\alert{Remember:} Predictors are also called explanatory or \underline{independent} variables. Ideally, they would be independent of each other.
\normalsize

\pause

\item We don't like adding predictors that are associated with each other to the model, because often times the addition of such variable brings nothing to the table. Instead, we prefer the simplest best model, i.e. \textbf{parsimonious} model.

\pause

\item While it's impossible to avoid collinearity from arising in observational data, experiments are usually designed to prevent correlation among predictors.

\end{itemize}

## $R^2$ vs. adjusted $R^2$

\renewcommand\arraystretch{1.25}
\begin{center}
\begin{tabular}{l | c  c}
			& $R^2$	& Adjusted $R^2$ \\
\hline
Model 1 (Single-predictor)	& 0.28	& 0.26 \\
Model 2 (Multiple)			& 0.29	& 0.26 	
\end{tabular}
\end{center}

\pause

\begin{itemize}

\item When \underline{any} variable is added to the model $R^2$ increases.

\pause

\item But if the added variable doesn't really provide any new information, or is completely unrelated, adjusted $R^2$ does not increase.

\end{itemize}

## Adjusted $R^2$

\centering{$R^2_{adj} = 1 - \left( \frac{ SS_{Error} }{ SS_{Total} } \times \frac{n - 1}{n - p - 1} \right)$}

\raggedright
where $n$ is the number of cases and $p$ is the number of predictors (explanatory variables) in the model.
}

\begin{itemize}

\item Because $p$ is never negative, $R^2_{adj}$ will always be smaller than $R^2$.

\item $R^2_{adj}$ applies a penalty for the number of predictors included in the model.

\item Therefore, we choose models with higher $R^2_{adj}$ over others.

\end{itemize}


## Calculate adjusted $R^2$

\begin{center}
\begin{tabular}{lrrrrr}
  \hline
\textbf{ANOVA:} & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
female\_house & 1 & 132.57 & 132.57 & 18.74 & 0.0001 \\ 
  white & 1 & 8.21 & 8.21 & 1.16 & 0.2868 \\ 
  Residuals & 48 & 339.47 & 7.07 &  &  \\ 
   \hline
Total & 50 &    480.25\\
   \hline
\end{tabular}
\end{center}

\begin{eqnarray*}
R^2_{adj} &=& 1 - \left( \frac{ SS_{Error} }{ SS_{Total} } \times \frac{n - 1}{n - p - 1} \right) \\
\pause
&=& 1 - \left( \frac{ 339.47 }{ 480.25 } \times \frac{51 - 1}{51 - 2 - 1} \right)   \\
\pause
&=& 1- \left( \frac{ 339.47 }{ 480.25 } \times \frac{50}{48} \right) \\
\pause
&=& 1 -  0.74 \\
\pause
&=& 0.26
\end{eqnarray*}

# Model selection

## Beauty in the classroom

\begin{itemize}

\item Data: Student evaluations of instructors' beauty and teaching quality for 463 courses at the University of Texas.

\item Evaluations conducted at the end of semester, and the beauty judgements were made later, by six students who had not attended the classes and were not aware of the course evaluations (2 upper level females, 2 upper level males, one lower level female, one lower level male).

\end{itemize}

## Professor rating vs. beauty

Professor evaluation score (higher score means better) vs. beauty score (a score of 0 means average, negative score means below average, and a positive score above average):

```{r, echo=F, message=F, warning=F, out.width="80%",fig.align='center'}
d = read.csv(file = "beauty.csv")

###############################################################################
# Do more beautiful profs get higher evaluations?
###############################################################################

# lm: beauty
beauty_profeval = lm(profevaluation ~ beauty, data = d)

par(mar=c(4,4,0.5,0.5))
plot(x = d$beauty, y = d$profevaluation, xlab="beauty", ylab="professor evaluation", pch = 19, col = COL[1,2], cex = 1, cex.lab = 1.5)
abline(beauty_profeval, col = COL[4], lwd = 4)
```

##

\alert{Which of the below is \underline{correct} based on the model output?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 4.19 & 0.03 & 167.24 & 0.00 \\ 
  beauty & 0.13 & 0.03 & 4.00 & 0.00 \\
   \hline
$R^2$ = 0.0336
\end{tabular}
\end{center}

\begin{enumerate}[(a)]
\item Model predicts 3.36\% of professor ratings correctly.
\item Beauty is not a significant predictor of professor evaluation.
\item Professors who score 1 point above average in their beauty score are tend to also score 0.13 points higher in their evaluation.
\item 3.36\% of variability in beauty scores can be explained by professor evaluation.
\item The correlation coefficient could be $\sqrt{0.0336} = 0.18$ or $-0.18$, we can't tell which is correct.
\end{enumerate}


##

\alert{Which of the below is \underline{correct} based on the model output?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 4.19 & 0.03 & 167.24 & 0.00 \\ 
  beauty & 0.13 & 0.03 & 4.00 & 0.00 \\
   \hline
$R^2$ = 0.0336
\end{tabular}
\end{center}

\begin{enumerate}[(a)]
\item Model predicts 3.36\% of professor ratings correctly.
\item Beauty is not a significant predictor of professor evaluation.
\item \alert{Professors who score 1 point above average in their beauty score are tend to also score 0.13 points higher in their evaluation.}
\item 3.36\% of variability in beauty scores can be explained by professor evaluation.
\item The correlation coefficient could be $\sqrt{0.0336} = 0.18$ or $-0.18$, we can't tell which is correct.
\end{enumerate}


## Exploratory analysis

\begin{columns}

\begin{column}{0.6\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
# lm: beauty + gender

d$pch = NA
d$col = NA
d$pch[d$gender == "male"] = 15
d$col[d$gender == "male"] = COL[1,2]
d$pch[d$gender == "female"] = 17
d$col[d$gender == "female"] = COL[2,2]

par(mar=c(4,4,0.5,0.5), las=1, mgp=c(3,0.7,0), cex.lab = 1.5, cex.axis = 1.5)
plot(d$profevaluation ~ d$beauty, pch = d$pch, col = d$col, xlab="beauty", ylab="professor evaluation")
legend("bottomright", c("male","female"), col = c(COL[1,2],COL[2,2]), pch = c(15,17), cex = 1.5, inset = 0.05)

```

\end{column}

\begin{column}{0.4\textwidth}

\alert{Any interesting features?}

\pause

Few females with very low beauty scores.

\pause

\alert{For a given beauty score, are male professors evaluated higher, lower, or about the same as female professors?}

\pause

Difficult to tell from this plot only.

\end{column}

\end{columns}

## Professor rating vs. beauty + gender

\alert{For a given beauty score, are male professors evaluated higher, lower, or about the same as female professors?}


\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 4.09 & 0.04 & 107.85 & 0.00 \\ 
  beauty & 0.14 & 0.03 & 4.44 & 0.00 \\ 
  gender.male & 0.17 & 0.05 & 3.38 & 0.00 \\ 
   \hline
$R^2_{adj}$ = 0.057
\end{tabular}
\end{center}


\begin{enumerate}[A)]
\item higher
\item lower
\item about the same
\end{enumerate}


## Professor rating vs. beauty + gender

\alert{For a given beauty score, are male professors evaluated higher, lower, or about the same as female professors?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 4.09 & 0.04 & 107.85 & 0.00 \\ 
  beauty & 0.14 & 0.03 & 4.44 & 0.00 \\ 
  gender.male & 0.17 & 0.05 & 3.38 & 0.00 \\ 
   \hline
$R^2_{adj}$ = 0.057
\end{tabular}
\end{center}


\begin{enumerate}[A)]
\item \alert{higher $\rightarrow$ Beauty held constant, male professors are rated 0.17 points higher on average than female professors.}
\item lower
\item about the same
\end{enumerate}

## Full model

\small
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 4.6282 & 0.1720 & 26.90 & 0.00 \\ 
  beauty & 0.1080 & 0.0329 & 3.28 & 0.00 \\ 
  gender.male & 0.2040 & 0.0528 & 3.87 & 0.00 \\ 
  age & -0.0089 & 0.0032 & -2.75 & 0.01 \\ 
formal.yes \footnote{\texttt{formal}: picture wearing tie\&jacket/blouse, levels: \texttt{yes}, \texttt{no}} & 0.1511 & 0.0749 & 2.02 & 0.04 \\ 
  lower.yes \footnote{\texttt{lower}: lower division course, levels: \texttt{yes}, \texttt{no}} & 0.0582 & 0.0553 & 1.05 & 0.29 \\ 
  native.non english & -0.2158 & 0.1147 & -1.88 & 0.06 \\ 
  minority.yes & -0.0707 & 0.0763 & -0.93 & 0.35 \\ 
  students \footnote{\texttt{students}: number of students} & -0.0004 & 0.0004 & -1.03 & 0.30 \\ 
  tenure.tenure track \footnote{\texttt{tenure}: tenure status, levels: \texttt{non-tenure track}, \texttt{tenure track}, \texttt{tenured}} & -0.1933 & 0.0847 & -2.28 & 0.02 \\ 
  tenure.tenured & -0.1574 & 0.0656 & -2.40 & 0.02 \\ 
   \hline
\end{tabular}


## Hypotheses

Just as the interpretation of the slope parameters take into account all other variables in the model,  the hypotheses for testing for significance of a predictor also takes into account all other variables.


\begin{itemize}
\item[$H_0:$] $B_i = 0$ when other explanatory variables are included in the model.
\item[$H_A:$] $B_i \ne 0$ when other explanatory variables are included in the model.
\end{itemize}

## Assessing significance: numerical variables

\alert{The p-value for age is 0.01. What does this indicate?}


\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
...\\
  age & -0.0089 & 0.0032 & -2.75 & 0.01 \\ 
...\\
   \hline
\end{tabular}
\end{center}


\begin{enumerate}[A)]
\item Since p-value is positive, higher the professor's age, the higher we would expect them to be rated.
\item If we keep all other variables in the model, there is strong evidence that professor's age is associated with their rating.
\item Probability that the true slope parameter for age is 0 is 0.01.
\item There is about 1\% chance that the true slope parameter for age is -0.0089.
\end{enumerate}


## Assessing significance: numerical variables

\alert{The p-value for age is 0.01. What does this indicate?}


\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
...\\
  age & -0.0089 & 0.0032 & -2.75 & 0.01 \\ 
...\\
   \hline
\end{tabular}
\end{center}


\begin{enumerate}[A)]
\item Since p-value is positive, higher the professor's age, the higher we would expect them to be rated.
\item \alert{If we keep all other variables in the model, there is strong evidence that professor's age is associated with their rating.}
\item Probability that the true slope parameter for age is 0 is 0.01.
\item There is about 1\% chance that the true slope parameter for age is -0.0089.
\end{enumerate}

## Assessing significance: categorical variables

\alert{Tenure is a categorical variable with 3 levels: non tenure track, tenure track, tenured. Based on the model output given, which of the below is \underline{false}?}


\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
... \\
  tenure.tenure track & -0.1933 & 0.0847 & -2.28 & 0.02 \\ 
  tenure.tenured & -0.1574 & 0.0656 & -2.40 & 0.02 \\ 
   \hline
\end{tabular}
\end{center}


\begin{enumerate}[(a)]
\item Reference level is non tenure track.
\item All else being equal, tenure track professors are rated, on average, 0.19 points lower than non-tenure track professors.
\item All else being equal, tenured professors are rated, on average, 0.16 points lower than non-tenure track professors.
\item All else being equal, there is a significant difference between the average ratings of tenure track and tenured professors.
\end{enumerate}


## Assessing significance: categorical variables

\alert{Tenure is a categorical variable with 3 levels: non tenure track, tenure track, tenured. Based on the model output given, which of the below is \underline{false}?}


\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
... \\
  tenure.tenure track & -0.1933 & 0.0847 & -2.28 & 0.02 \\ 
  tenure.tenured & -0.1574 & 0.0656 & -2.40 & 0.02 \\ 
   \hline
\end{tabular}
\end{center}


\begin{enumerate}[(a)]
\item Reference level is non tenure track.
\item All else being equal, tenure track professors are rated, on average, 0.19 points lower than non-tenure track professors.
\item All else being equal, tenured professors are rated, on average, 0.16 points lower than non-tenure track professors.
\item \alert{All else being equal, there is a significant difference between the average ratings of tenure track and tenured professors.}
\end{enumerate}

## Assessing significance

\alert{Which predictors do not seem to meaningfully contribute to the model, i.e. may not be significant predictors of professor's rating score?}

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 4.6282 & 0.1720 & 26.90 & 0.00 \\ 
  beauty & 0.1080 & 0.0329 & 3.28 & 0.00 \\ 
  gender.male & 0.2040 & 0.0528 & 3.87 & 0.00 \\ 
  age & -0.0089 & 0.0032 & -2.75 & 0.01 \\ 
  formal.yes & 0.1511 & 0.0749 & 2.02 & 0.04 \\ 
  
  \textcolor{red}{lower.yes} & \textcolor{red}{0.0582} & \textcolor{red}{0.0553} & \textcolor{red}{1.05} & \textcolor{red}{0.29} \\ 

  \textcolor{red}{native.non english} & \textcolor{red}{-0.2158} & \textcolor{red}{0.1147} & \textcolor{red}{-1.88} & \textcolor{red}{0.06} \\ 

  \textcolor{red}{minority.yes} & \textcolor{red}{-0.0707} & \textcolor{red}{0.0763} & \textcolor{red}{-0.93} & \textcolor{red}{0.35} \\ 

  \textcolor{red}{students} & \textcolor{red}{-0.0004} & \textcolor{red}{0.0004} & \textcolor{red}{-1.03} & \textcolor{red}{0.30} \\ 
  tenure.tenure track & -0.1933 & 0.0847 & -2.28 & 0.02 \\ 
  tenure.tenured & -0.1574 & 0.0656 & -2.40 & 0.02 \\ 
   \hline
\end{tabular}

## Model selection strategies

\alert{Based on what we've learned so far, what are some ways you can think of that can be used to determine which variables to keep in the model and which to leave out?}

## Backward-elimination

\begin{enumerate}

\item Start with the full model
\item Drop one variable at a time and record $R^2_{adj}$ of each smaller model
\item Pick the model with the highest increase in $R^2_{adj}$
\item Repeat until none of the models yield an increase in $R^2_{adj}$

\end{enumerate}

## Backward-elimination

\tiny
\begin{center}
\begin{tabular}{l | l | c}
Full		& beauty + gender + age + formal + lower + native + minority + students + tenure & \alert{0.0839} \pause \\
\hline
Step 1 	& gender + age + formal + lower + native + minority + students + tenure		& 0.0642 \\
		& beauty + age + formal + lower + native + minority + students + tenure		& 0.0557 \\
		& beauty + gender + formal + lower + native + minority + students + tenure	& 0.0706 \\
		& beauty + gender + age + lower + native + minority + students + tenure		& 0.0777 \\
		& beauty + gender + age + formal + native + minority + students + tenure		& 0.0837 \\
		& beauty + gender + age + formal + lower + minority + students + tenure		& 0.0788 \\
		& beauty + gender + age + formal + lower + native + students + tenure		& \alert{0.0842} \\
		& beauty + gender + age + formal + lower + native + minority + tenure		& 0.0838 \\
		& beauty + gender + age + formal + lower + native + minority + students		& 0.0733 \pause \\
\hline		
Step 2	& gender + age + formal + lower + native + students + tenure 				& 0.0647 \\
		& beauty + age + formal + lower + native + students + tenure 				& 0.0543 \\
		& beauty + gender + formal + lower + native + students + tenure 			& 0.0708 \\
		& beauty + gender + age + lower + native + students + tenure 				&0.0776  \\
		& beauty + gender + age + formal + native + students + tenure 			& \alert{0.0846} \\
		& beauty + gender + age + formal + lower + native + tenure 				& 0.0844 \\
		& beauty + gender + age + formal + lower + native + students 				& 0.0725 \pause \\
\hline
Step 3	& gender + age + formal + native + students + tenure 					& 0.0653 \\
		& beauty + age + formal + native + students + tenure					& 0.0534 \\
		& beauty + gender + formal + native + students + tenure					& 0.0707 \\
		& beauty + gender + age + native + students + tenure					& 0.0786 \\
		& beauty + gender + age + formal + students + tenure					& 0.0756 \\
		& beauty + gender + age + formal + native + tenure						& \alert{0.0855} \\
		& beauty + gender + age + formal + native + students					& 0.0713 \pause \\
\hline
Step 4	& gender + age + formal + native + tenure 							& 0.0667 \\
		& beauty + age + formal + native + tenure								& 0.0553 \\
		& beauty + gender + formal + native + tenure							& 0.0723 \\
		& beauty + gender + age + native + tenure							& 0.0806 \\
		& beauty + gender + age + formal + tenure							& 0.0773 \\
		& beauty + gender + age + formal + native							& 0.0713 \\
\end{tabular}
\end{center}

## \texttt{step} function in R

\tiny

```{r}
# full model
m = lm(profevaluation ~ beauty + gender + age + formal + lower + native + minority + students + tenure, data = d)
summary(m)
```

\normalsize

Best model: beauty + gender + age + formal + native + tenure

## Forward selection

\begin{enumerate}

\item Start with regressions of response vs. each explanatory variable
\item Pick the model with the highest $R^2_{adj}$
\item Add the remaining variables one at a time to the existing model, and once again pick the model  with the highest $R^2_{adj}$
\item Repeat until the addition of any of the remaining variables does not result in a higher $R^2_{adj}$

\end{enumerate}

##

\begin{itemize}

\item Backward elimination with the p-value approach: 
\begin{enumerate}
\item Start with the full model
\item Drop the variable with the highest p-value and refit a smaller model
\item Repeat until all variables left in the model are significant
\end{enumerate}

\item Forward selection with the p-value approach: 
\begin{enumerate}
\item Start with regressions of response vs. each explanatory variable
\item Pick the variable with the lowest significant p-value 
\item Add the remaining variables one at a time to the existing model, and pick the variable with the lowest significant p-value
\item Repeat until any of the remaining variables does not have a significant p-value
\end{enumerate}

\end{itemize}

## Backward-elimination: $p-value$ approach

\resizebox{\textwidth}{!}{%
\tiny
\begin{tabular}{l | rrrrrrrrrr }
\textbf{Step}		& \multicolumn{10}{c}{ \textbf{Variables included \& p-value} } \\
\hline
Full		& beauty	& gender		& age	& formal		& lower 	& native	 	& minority		& students	& tenure		& tenure \\
		& 		& male		& 		& yes		& yes	 & nonenglish	& yes		& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	& 0.04		& 0.29	& 0.06		& \alert{0.35}	& 0.30		& 0.02		& 0.02 \pause \\
\hline
Step 1	& beauty	& gender		& age	& formal		& lower 		& native	 	& 			& students	& tenure		& tenure \\
		& 		& male		& 		& yes		& yes		& nonenglish	& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	& 0.04		& \alert{0.38}	& 0.03		&			& 0.34		& 0.02		& 0.01 \pause\\
\hline
Step 2	& beauty	& gender		& age	& formal		& 	 		& native	 	& 			& students	& tenure		& tenure \\
		& 		& male		& 		& yes		& 			& nonenglish	& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	& 0.05		& 			& 0.02		&			& \alert{0.44}	& 0.01		& 0.01\pause \\
\hline
Step 3 	& beauty	& gender		& age	& formal		& 	 		& native	 	& 			& 			& tenure		& tenure \\
		& 		& male		& 		& yes		& 			& nonenglish	& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	& \alert{0.06}	& 			& 0.02		&			& 			& 0.01		& 0.01 \pause \\
\hline
Step 	4	& beauty	& gender		& age	& 			& 	 		& native	 	& 			& 			& tenure		& tenure \\
		& 		& male		& 		& 			& 			& nonenglish	& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	&			& 			& \alert{0.06}	&			& 			& 0.01		& 0.01 \pause \\
\hline
Step 5 	& beauty	& gender		& age	& 			& 	 		& 		 	& 			& 			& tenure		& tenure \\
		& 		& male		& 		& 			& 			&			& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	&			& 			& 			&			& 			& 0.01		& 0.01 \\ \pause
\end{tabular}
}


## Backward-elimination: $p-value$ approach

\resizebox{\textwidth}{!}{%
\tiny
\begin{tabular}{l | rrrrrrrrrr }
\textbf{Step}		& \multicolumn{10}{c}{ \textbf{Variables included \& p-value} } \\
\hline
Full		& beauty	& gender		& age	& formal		& lower 	& native	 	& minority		& students	& tenure		& tenure \\
		& 		& male		& 		& yes		& yes	 & nonenglish	& yes		& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	& 0.04		& 0.29	& 0.06		& \alert{0.35}	& 0.30		& 0.02		& 0.02 \\
\hline
Step 1	& beauty	& gender		& age	& formal		& lower 		& native	 	& 			& students	& tenure		& tenure \\
		& 		& male		& 		& yes		& yes		& nonenglish	& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	& 0.04		& \alert{0.38}	& 0.03		&			& 0.34		& 0.02		& 0.01\\
\hline
Step 2	& beauty	& gender		& age	& formal		& 	 		& native	 	& 			& students	& tenure		& tenure \\
		& 		& male		& 		& yes		& 			& nonenglish	& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	& 0.05		& 			& 0.02		&			& \alert{0.44}	& 0.01		& 0.01 \\
\hline
Step 3 	& beauty	& gender		& age	& formal		& 	 		& native	 	& 			& 			& tenure		& tenure \\
		& 		& male		& 		& yes		& 			& nonenglish	& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	& \alert{0.06}	& 			& 0.02		&			& 			& 0.01		& 0.01  \\
\hline
Step 	4	& beauty	& gender		& age	& 			& 	 		& native	 	& 			& 			& tenure		& tenure \\
		& 		& male		& 		& 			& 			& nonenglish	& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	&			& 			& \alert{0.06}	&			& 			& 0.01		& 0.01  \\
\hline
Step 5 	& beauty	& gender		& age	& 			& 	 		& 		 	& 			& 			& tenure		& tenure \\
		& 		& male		& 		& 			& 			&			& 			& 			& tenure track	& tenured \\
		&  0.00 	&  0.00 		& 0.01	&			& 			& 			&			& 			& 0.01		& 0.01 \\
\end{tabular}
}


Best model: beauty + gender + age + tenure


## Adjusted $R^2$ vs. p-value approaches

\begin{itemize}

\item The two approaches are similar, but they sometimes lead to different models, with the adjusted $R^2$ approach tending to include more predictors in the final model.

\item When the sole goal is to improve prediction accuracy, use $R^2$ . This is commonly the case in machine learning applications.

\item When we care about understanding which variables are statistically significant predictors of the response, or if there is interest in producing a simpler model at the potential cost of a little prediction accuracy, then the p-value approach is preferred.

\item Regardless of the approach we use, our job is not done after variable selection -- we must still verify the model conditions are reasonable.

\end{itemize}


# Checking model conditions using graphs

## Modeling conditions

\centering{$\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p$} 

\raggedright

The model depends on the following conditions
\begin{enumerate}
\item residuals are nearly normal (less important for larger data sets)
\item residuals have constant variability
\item residuals are independent
\item each variable is linearly related to the outcome \\
\end{enumerate}

We often use graphical methods to check the validity of these conditions, which we will go through in detail in the following slides.

## Nearly normal residuals

Normal probability plot and/or histogram of residuals:

```{r, echo=F, message=F, warning=F, out.width="80%",fig.align='center'}
d = read.csv(file = "beauty.csv")

# final model

m_final = lm(profevaluation ~ beauty + gender + age + formal + native + tenure, data = d)


# nearly normal residuals

par(mar=c(2,4,2,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.5, cex.main = 2, cex.axis = 1.25)

hist(m_final$residuals, main = "Histogram of residuals", col = COL[1])
```

\alert{Does this condition appear to be satisfied?}


## Constant variability in residuals

Scatterplot of residuals and/or absolute value of residuals vs. fitted (predicted):


```{r, echo=F, message=F, warning=F, out.width="80%",fig.align='center'}
par(mar=c(4,4,2,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.5, cex.main = 1.25,cex.axis = 1.25, mfrow = c(1,2))

plot(m_final$residuals ~ m_final$fitted, main = "Residuals vs. fitted", pch = 19, col = COL[1,2], ylab = "residuals", xlab = "fitted", cex = 2)
abline(h = 0, lty = 3, lwd = 4)

plot(abs(m_final$residuals) ~ m_final$fitted, main = "Absolute value of residuals vs. fitted", pch = 19, col = COL[1,2], ylab = "abs(residuals)", xlab = "fitted", cex = 2)

```

\alert{Does this condition appear to be satisfied?}

## Checking constant variance - recap

\begin{itemize}

\item When we did simple linear regression (one explanatory variable) we checked the constant variance condition using a plot of \textbf{residuals vs. x}.

\item With multiple linear regression (2+ explanatory variables) we checked the constant variance condition using a plot of \textbf{residuals vs. fitted}. 

\end{itemize}

\alert{Why are we using different plots?}

\pause 

In multiple linear regression there are many explanatory variables, so a plot of residuals vs. one of them wouldn't give us the complete picture.

## Independent residuals

Scatterplot of residuals vs. order of data collection:

```{r, echo=F, message=F, warning=F, out.height="70%",fig.align='center'}
par(mar=c(4,4,2,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.5, cex.axis = 1.25, mfrow = c(1,1))

plot(m_final$residuals, main = "Residuals vs. order of data collection", pch = 19, col = COL[1,2], ylab = "residuals", xlab = "order of data collection", cex = 2)

abline(h = 0, lty = 3, lwd = 4)
```

\alert{Does this condition appear to be satisfied?}


## More on the condition of independent residuals

\begin{itemize}

\item Checking for independent residuals allows us to indirectly check for independent observations.

\item If observations and residuals are independent, we would not expect to see an increasing or decreasing trend in the scatterplot of residuals vs. order of data collection.

\item This condition is often violated when we have time series data. Such data require more advanced time series regression techniques for proper analysis.

\end{itemize}


## Linear relationships

Scatterplot of residuals vs. each (numerical) explanatory variable:

```{r, echo=F, message=F, warning=F, out.width="50%",fig.align='center'}
par(mar=c(4,4,1,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.8, cex.axis = 1.25, mfrow = c(1,2))

plot(m_final$residuals ~ d$beauty, pch = 19, col = COL[1,2], xlab = "beauty", ylab = "professor evaluation", cex = 2)
abline(h = 0, lty = 3, lwd = 4)

plot(m_final$residuals ~ d$age, pch = 19, col = COL[1,2], xlab = "age", ylab = "professor evaluation", cex = 2)
abline(h = 0, lty = 3, lwd = 6)
```

\alert{Does this condition appear to be satisfied?}

\noindent\rule{4cm}{0.4pt}
\footnotesize
\alert{Note:} We use residuals instead of the predictors on the y-axis so that we can still check for linearity without worrying about other possible violations like collinearity between the predictors.

## Several options for improving a model

\begin{itemize}

\item Transforming variables
\item Seeking out additional variables to fill model gaps
\item Using more advanced methods that would account for challenges around inconsistent variability or nonlinear relationships between predictors and the outcome

\end{itemize}

## Transformations

If the concern with the model is non-linear relationships between the explanatory 
variable(s) and the response variable, transforming the response variable can be helpful. 

\begin{itemize}

\item Log transformation (log $y$)
\item Square root transformation ($\sqrt{y}$)
\item Inverse transformation ($1/y$)
\item Truncation (cap the max value possible)

\end{itemize}

It is also possible to apply transformations to the explanatory variable(s), however 
such transformations tend to make the model coefficients even harder to interpret.

## Models can be wrong, but useful

\begin{quote}
All models are wrong, but some are useful. - George Box
\end{quote}

\begin{itemize}

\item No model is perfect, but even imperfect models can be useful, as long as we are clear and report the model's shortcomings.

\item If conditions are grossly violated, we should not report the model results, but instead consider a new model, even if it means learning more statistical methods or hiring someone who can help.

\end{itemize}

# Logistic regression

## Regression so far...

\begin{itemize}
\item Simple linear regression
\begin{itemize}
\item Relationship between numerical response and a numerical or categorical predictor
\end{itemize}
\pause
\item Multiple regression
\begin{itemize}
\item Relationship between numerical response and multiple numerical and/or categorical predictors
\end{itemize}
\end{itemize}

\pause

What we haven't seen is what to do when the predictors are weird (nonlinear, complicated dependence structure, etc.) or when the response is weird (categorical, count data, etc.)

## Odds

Odds are another way of quantifying the probability of an event, commonly used in gambling (and logistic regression).\\

For some event $E$,

\[\text{odds}(E) = \frac{P(E)}{P(E^c)} = \frac{P(E)}{1-P(E)}\]

Similarly, if we are told the odds of E are $x$ to $y$ then

\[\text{odds}(E) = \frac{x}{y} = \frac{x/(x+y)}{y/(x+y)} \]

which implies

\[P(E) = x/(x+y),\quad P(E^c) = y/(x+y)\]

## Example - Donner Party

In 1846 the Donner and Reed families left Springfield, Illinois, for California by covered wagon. In July, the Donner Party, as it became known, reached Fort Bridger, Wyoming. There its leaders decided to attempt a new and untested rote to the Sacramento Valley. Having reached its full size of 87 people and 20 wagons, the party was delayed by a difficult crossing of the Wasatch Range and again in the crossing of the desert west of the Great Salt Lake. The group became stranded in the eastern Sierra Nevada mountains when the region was hit by heavy snows in late October. By the time the last survivor was rescued on April 21, 1847, 40 of the 87 members had died from famine and exposure to extreme cold.


## Example - Donner Party - Data

\begin{center}
\begin{tabular}{rlll}
  \hline
     & Age   & Sex    & Status \\ 
  \hline
   1 & 23.00 & Male   & Died \\ 
   2 & 40.00 & Female & Survived \\ 
   3 & 40.00 & Male   & Survived \\ 
   4 & 30.00 & Male   & Died \\ 
   5 & 28.00 & Male   & Died \\ 
$\vdots$ & ~~~$\vdots$ & ~~~$\vdots$ & ~~~$\vdots$ \\
  43 & 23.00 & Male   & Survived \\ 
  44 & 24.00 & Male   & Died \\ 
  45 & 25.00 & Female & Survived \\ 
   \hline
\end{tabular}
\end{center}


## Example - Donner Party - EDA

Status vs. Gender:

\begin{center}
\begin{tabular}{rrr}
\hline
         & Male & Female \\ 
\hline
Died     &  20  &   5 \\ 
Survived &  10  &  10 \\ 
   \hline
\end{tabular}
\end{center}

\pause 
\vfill

Status vs. Age:

```{r, echo=F, message=F, warning=F, fig.width=4, fig.height=2,fig.align='center'}
donner = case2001

par(mar=c(2,4,2,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.25, cex.axis = 1.25)

boxplot(donner$Age ~ donner$Status, xlab = "", ylab = "", cex.axis = 1.5)
```


## Example - Donner Party

It seems clear that both age and gender have an effect on someone's survival, how do we come up with a model that will let us explore this relationship?

\pause

Even if we set Died to 0 and Survived to 1, this isn't something we can transform our way out of - we need something more.

\pause

One way to think about the problem - we can treat Survived and Died as successes and failures arising from a binomial distribution where the probability of a success is given by a transformation of a linear model of the predictors.


## Generalized linear models

It turns out that this is a very general way of addressing this type of problem in regression, and the resulting models are called generalized linear models (GLMs). Logistic regression is just one example of this type of model.

\pause

All generalized linear models have the following three characteristics:

\begin{enumerate}
\item A probability distribution describing the outcome variable 
\item A linear model
\begin{itemize}
\item $\eta = \beta_0+\beta_1 X_1 + \cdots + \beta_n X_n$
\end{itemize}
\item A link function that relates the linear model to the parameter of the outcome distribution
\begin{itemize}
\item $g(p) = \eta$ or $p = g^{-1}(\eta)$
\end{itemize}
\end{enumerate}

## Logistic Regression

Logistic regression is a GLM used to model a binary categorical variable using numerical and categorical predictors.

We assume a binomial distribution produced the outcome variable and we therefore want to model $p$ the probability of success for a given set of predictors.

\pause

To finish specifying the Logistic model we just need to establish a reasonable link function that connects $\eta$ to $p$. There are a variety of options but the most commonly used is the logit function.

\[logit(p) = \log\left(\frac{p}{1-p}\right),\text{ for $0\le p \le 1$}\]


## The logistic regression model

The logit function takes a value between 0 and 1 and maps it to a value between $-\infty$ and $\infty$.

\[g^{-1}(x) = \frac{\exp(x)}{1+\exp(x)} = \frac{1}{1+\exp(-x)}\]

The inverse logit function takes a value between $-\infty$ and $\infty$ and maps it to a value between 0 and 1.

This formulation also has some use when it comes to interpreting the model as logit can be interpreted as the log odds of a success, more on this later.


## The logistic regression model

The three GLM criteria give us:

\begin{align*}
y_i &\sim \text{Binom}(p_i)\\
\\
\eta &= \beta_0+\beta_1 x_1 + \cdots + \beta_n x_n\\
\\
\text{logit}(p) &= \eta
\end{align*}

From which we arrive at,

\[ p_i = \frac{\exp(\beta_0+\beta_1 x_{1,i} + \cdots + \beta_n x_{n,i})}{1+\exp(\beta_0+\beta_1 x_{1,i} + \cdots + \beta_n x_{n,i})} \]


## Example - Donner Party - Model

In R we fit a GLM in the same was as a linear model except using \texttt{glm} instead of \texttt{lm} and we must also specify the type of GLM to fit using the \texttt{family} argument.

\tiny
```{r}
g1=glm(Status~Age, data=donner, family=binomial)
summary(g1)
```


## Example - Donner Party - Prediction

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & 1.8185 & 0.9994 & 1.82 & 0.0688 \\ 
  Age & -0.0665 & 0.0322 & -2.06 & 0.0391 \\ 
   \hline
\end{tabular}
\end{center}


Model:
\[\log\left(\frac{p}{1-p}\right) = 1.8185-0.0665\times \text{Age}\]

\pause

Odds / Probability of survival for a newborn (Age=0):
\pause

\begin{align*}
\log\left(\frac{p}{1-p}\right) &= 1.8185-0.0665\times \alert{0}\\
\frac{p}{1-p} &= \exp(1.8185) = 6.16 \\
p &= 6.16/7.16 = 0.86 
\end{align*}


## Example - Donner Party - Prediction

Model:
\small
\[\log\left(\frac{p}{1-p}\right) = 1.8185-0.0665\times \text{Age}\]
\normalsize

Odds / Probability of survival for a 25 year old:

\pause

\small
\begin{align*}
\log\left(\frac{p}{1-p}\right) &= 1.8185-0.0665\times \alert{25}\\
\frac{p}{1-p} &= \exp(0.156) = 1.17 \\
p &= 1.17/2.17 = 0.539 
\end{align*}
\normalsize

\pause

Odds / Probability of survival for a 50 year old:

\pause

\small
\begin{align*}
\log\left(\frac{p}{1-p}\right) &= 1.8185-0.0665\times \alert{50}\\
\frac{p}{1-p} &= \exp(-1.5065) = 0.222 \\
p &= 0.222/1.222 =  0.181
\end{align*}

## Example - Donner Party - Prediction

\scriptsize
\[\log\left(\frac{p}{1-p}\right) = 1.8185-0.0665\times \text{Age}\]
\normalsize

```{r, echo=F, message=F, warning=F, out.width="95%",fig.align='center'}
g=glm(Status~Age+Sex, data=donner, family=binomial)

par(mar=c(2,4,2,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.5, cex.axis = 1.25)

plot(donner$Age,as.numeric(donner$Status)-1+0.01*((as.numeric(donner$Sex)-1)*2-1),xlab="Age",ylab="Status",xlim=c(0,80),pch=c(15,17)[donner$Sex], col = c(COL[1,2], COL[2,2]), cex = 2)

legend("topright",c("Male","Female"),pch=c(15,17), col = c(COL[1,2], COL[2,2]), cex = 2, inset = 0.025)
```


## Example - Donner Party - Prediction

\scriptsize
\[\log\left(\frac{p}{1-p}\right) = 1.8185-0.0665\times \text{Age}\]
\normalsize

```{r, echo=F, message=F, warning=F, out.width="95%",fig.align='center'}
par(mar=c(2,4,2,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.5, cex.axis = 1.25)

plot(donner$Age,as.numeric(donner$Status)-1+0.01*((as.numeric(donner$Sex)-1)*2-1),xlab="Age",ylab="Status",xlim=c(0,80),pch=c(15,17)[donner$Sex], col = c(COL[1,2], COL[2,2]), cex = 2)

x=0:80
p = predict(g1,newdata=data.frame(Age=x))
lines(x,exp(p)/(1+exp(p)), col = COL[4], lwd = 4)

p1 = exp(p[1])/(1 + exp(p[1]))
p2 = exp(p[26])/(1 + exp(p[26]))
p3 = exp(p[51])/(1 + exp(p[51]))

points(x = c(0,25,50), y = c(p1,p2,p3), col = COL[4], pch = 19, cex = 2)

legend("topright",c("Male","Female"),pch=c(15,17), col = c(COL[1,2], COL[2,2]), cex = 2, inset = 0.025)
```


## Example - Donner Party - Interpretation

\scriptsize
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & 1.8185 & 0.9994 & 1.82 & 0.0688 \\ 
  Age & -0.0665 & 0.0322 & -2.06 & 0.0391 \\ 
   \hline
\end{tabular}
\end{center}
\normalsize

Simple interpretation is only possible in terms of log odds and log odds ratios for intercept and slope terms.

\textbf{Intercept}: The log odds of survival for a party member with an age of 0. From this we can calculate the odds or probability, but additional calculations are necessary.

\textbf{Slope}: For a unit increase in age (being 1 year older) how much will the log odds ratio change, not particularly intuitive. More often then not we  care only about sign and relative magnitude. 


## Example - Donner Party - Interpretation - Slope

\scriptsize
\begin{align*}
\log\left(\frac{p_1}{1-p_1}\right) &= 1.8185-0.0665 (x+1) \\
                                   &= 1.8185-0.0665 x-0.0665 \\
\log\left(\frac{p_2}{1-p_2}\right) &= 1.8185-0.0665 x \\
\\
\log\left(\frac{p_1}{1-p_1}\right) - \log\left(\frac{p_2}{1-p_2}\right) &= -0.0665 \\
\log\left(\left. \frac{p_1}{1-p_1} \right/ \frac{p_2}{1-p_2} \right) &= -0.0665 \\
\left. \frac{p_1}{1-p_1} \right/ \frac{p_2}{1-p_2} &= \exp(-0.0665) = 0.94
\end{align*}


## Example - Donner Party - Age and Gender

\tiny
```{r}
g=glm(Status~Age+Sex, data=donner, family=binomial)
summary(g)
```

\normalsize

**Gender slope:** When the other predictors are held constant this is the log odds ratio between the given level (Female) and the reference level (Male).


## Example - Donner Party - Gender Models

Just like MLR we can plug in gender to arrive at two status vs age models for men and women respectively.

General model:
\scriptsize
\[\log\left(\frac{p_1}{1-p_1}\right) = 1.63312 + -0.07820\times\text{Age} + 1.59729\times\text{Sex}\]
\normalsize

Male model:
\scriptsize
\begin{align*}
\log\left(\frac{p_1}{1-p_1}\right) &= 1.63312 + -0.07820\times\text{Age} + 1.59729\times\alert{0}\\
                                   &= 1.63312 + -0.07820\times\text{Age}
\end{align*}
\normalsize

Female model:
\scriptsize
\begin{align*}
\log\left(\frac{p_1}{1-p_1}\right) &= 1.63312 + -0.07820\times\text{Age} + 1.59729\times\alert{1}\\
                                   &= 3.23041 + -0.07820\times\text{Age}
\end{align*}

## Example - Donner Party - Gender Models

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
par(mar=c(2,4,2,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.5, cex.axis = 1.25)

plot(donner$Age,as.numeric(donner$Status)-1+0.01*((as.numeric(donner$Sex)-1)*2-1),xlab="Age",ylab="Status",xlim=c(0,80),pch=c(15,17)[donner$Sex], col = c(COL[1,2], COL[2,2]), cex = 2)

x=0:80
p_male = predict(g,newdata=data.frame(Age=x,Sex="Male"))
p_female = predict(g,newdata=data.frame(Age=x,Sex="Female"))

lines(x,exp(p_male)/(1+exp(p_male)), col = COL[1], lwd = 4)
lines(x,exp(p_female)/(1+exp(p_female)), col = COL[2], lwd = 2, lty = 4)

legend("topright",c("Male","Female"),pch=c(15,17), col = c(COL[1,2], COL[2,2]), inset = 0.025, cex = 2)

text(x = 45, y = 0.7, "Female", col = COL[2], cex = 2)
text(x = 20, y = 0.3, "Male", col = COL[1], cex = 2)
```


## Hypothesis test for the whole model

\tiny
```{r}
g=glm(Status~Age+Sex, data=donner, family=binomial)
summary(g)
```

\footnotesize

\noindent\rule{4cm}{0.4pt}

\alert{Note:} The model output does not include any F-statistic, as a general rule there are not single model hypothesis tests for GLM models.


## Hypothesis tests for a coefficient

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & 1.6331 & 1.1102 & 1.47 & 0.1413 \\ 
  Age & -0.0782 & 0.0373 & -2.10 & 0.0359 \\ 
  SexFemale & 1.5973 & 0.7555 & 2.11 & 0.0345 \\ 
   \hline
\end{tabular}
\end{center}

We are however still able to perform inference on individual coefficients, the basic setup is exactly the same as what we've seen before except we use a Z test.

\noindent\rule{4cm}{0.4pt}

\alert{Note:} The only tricky bit, which is way beyond the scope of this course, is how the standard error is calculated.


## Testing for the slope of Age

\small
\begin{center}
\begin{tabular}{rrrrr}
  \hline
            & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & 1.6331  & 1.1102 & 1.47 & 0.1413 \\ 
        Age & \alert{-0.0782} & \textcolor{green}{0.0373} & \alert{-2.10} & \textcolor{blue}{0.0359} \\ 
  SexFemale & 1.5973  & 0.7555 & 2.11 & 0.0345 \\ 
   \hline
\end{tabular}
\end{center}
\normalsize

\pause

\begin{align*}
H_0:~& \beta_{age} = 0 \\
H_A:~& \beta_{age} \ne 0
\end{align*}

\pause

\begin{align*}
Z &= \frac{\hat{\beta_{age}} - \beta_{age}}{SE_{age}} 
   = \frac{\alert{-0.0782} - 0}{\textcolor{green}{0.0373}} = \alert{-2.10} \\
\\
\text{p-value} &= P(|Z| > \alert{2.10}) = P(Z>\alert{2.10})+P(Z<\alert{-2.10})\\
               &= 2\times 0.0178 = \textcolor{blue}{0.0359}
\end{align*}

## Confidence interval for age slope coefficient

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & 1.6331 & 1.1102 & 1.47 & 0.1413 \\ 
  Age & -0.0782 & 0.0373 & -2.10 & 0.0359 \\ 
  SexFemale & 1.5973 & 0.7555 & 2.11 & 0.0345 \\ 
   \hline
\end{tabular}
\end{center}

Remember, the interpretation for a slope is the change in log odds ratio per unit change in the predictor.

\pause

Log odds ratio:
\[ CI = PE \pm CV \times SE = -0.0782 \pm 1.96 \times 0.0373 = (-0.1513,  -0.0051) \]

\pause

Odds ratio:
\[ \exp(CI) = (\exp{-0.1513}, \exp{-0.0051}) = (0.8596 0.9949) \]


## Example - Birdkeeping and Lung Cancer

A 1972 - 1981 health survey in The Hague, Netherlands, discovered an association between keeping pet birds and increased risk of lung cancer. To investigate birdkeeping as a risk factor, researchers conducted a case-control study of patients in 1985 at four hospitals in The Hague (population 450,000). They identified 49 cases of lung cancer among the patients who were registered with a general practice, who were age 65 or younger and who had resided in the city since 1965. They also selected 98 controls from a population of residents having the same general age structure.


## Example - Birdkeeping and Lung Cancer - Data

\scriptsize
\begin{tabular}{rllllrrr}
  \hline
    & \texttt{LC} & \texttt{FM} & \texttt{SS} & \texttt{BK} & \texttt{AG} & \texttt{YR} & \texttt{CD} \\ 
  \hline
  1 & LungCancer & Male & Low & Bird & 37.00 & 19.00 & 12.00 \\ 
  2 & LungCancer & Male & Low & Bird & 41.00 & 22.00 & 15.00 \\ 
  3 & LungCancer & Male & High & NoBird & 43.00 & 19.00 & 15.00 \\ 
\vdots & ~~~~~~\vdots & ~~~\vdots & ~~\vdots & ~~~\vdots & \vdots~~~ & \vdots~~~ & \vdots~~~ \\
147 & NoCancer & Female & Low & NoBird & 65.00 & 7.00 & 2.00 \\ 
   \hline
\end{tabular}

\small
\begin{center}
\begin{tabular}{ll}
\texttt{LC} & Whether subject has lung cancer \\
\texttt{FM} & Sex of subject \\
\texttt{SS} & Socioeconomic status \\
\texttt{BK} & Indicator for birdkeeping \\
\texttt{AG} & Age of subject (years) \\
\texttt{YR} & Years of smoking prior to diagnosis or examination \\
\texttt{CD} & Average rate of smoking (cigarettes per day)
\end{tabular}
\end{center}


\alert{Note:} NoCancer is the reference response (0 or failure), LungCancer is the non-reference response (1 or success) - this matters for interpretation.


## Example - Birdkeeping and Lung Cancer - EDA


```{r, echo=F, message=F, warning=F, out.width="80%",fig.align='center'}
birds = case2002


birds$pch = NA

birds$pch[birds$LC == "LungCancer" & birds$BK == "Bird"] = 17
birds$pch[birds$LC == "LungCancer" & birds$BK == "NoBird"] = 16
birds$pch[birds$LC == "NoCancer" & birds$BK == "Bird"] = 2
birds$pch[birds$LC == "NoCancer" & birds$BK == "NoBird"] = 1

birds$col[birds$LC == "LungCancer"] = COL[2,2]
birds$col[birds$LC == "NoCancer"] = COL[1,2]


# birds

par(mar=c(2,4,2,1), las=1, mgp=c(3,0.7,0), cex.lab = 1.5, cex.axis = 1.25)

plot(YR~AG, data = birds, pch = birds$pch, col = birds$col, lwd = 2)
```


\begin{center}
\begin{tabular}{r|cc}
               & Bird             & No Bird \\
\hline               
Lung Cancer    & \raisebox{0.5pt}{\tikz{\node[draw,scale=0.3,regular polygon, regular polygon sides=3,fill=green!10!green,rotate=0](){};}}  & \tikz\draw[green,fill=green] (0,0) circle (.5ex); \\
No Lung Cancer & \raisebox{0.5pt}{\tikz{\node[draw,scale=0.3,regular polygon, regular polygon sides=3,color = blue,fill=none,rotate=0](){};}}      & \tikz\draw[blue,fill=none] (0,0) circle (.5ex);
\end{tabular}
\end{center}

## Example - Birdkeeping and Lung Cancer - Model

\tiny
```{r}
summary(glm(LC ~ FM + SS + BK + AG + YR + CD, data=birds, family=binomial))
```


## Example - Birdkeeping and Lung Cancer - Interpretation

\scriptsize
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & -1.9374 & 1.8043 & -1.07 & 0.2829 \\ 
  FMFemale  & 0.5613 & 0.5312 & 1.06 & 0.2907 \\ 
  SSHigh    & 0.1054 & 0.4688 & 0.22 & 0.8221 \\

  \alert{BKBird}    & \alert{1.3626} & \alert{0.4113} & \alert{3.31} & \alert{0.0009} \\ 
  AG        & -0.0398 & 0.0355 & -1.12 & 0.2625 \\ 

  \alert{YR}        & \alert{0.0729} & \alert{0.0265} & \alert{2.75} & \alert{0.0059} \\ 
  CD        & 0.0260 & 0.0255 & 1.02 & 0.3081 \\ 
   \hline
\end{tabular}
\end{center}
\normalsize

\pause

Keeping all other predictors constant then,
\begin{itemize}
\pause
\item The odds ratio of getting lung cancer for bird keepers vs non-bird keepers is $\exp(1.3626) = 3.91$.
\pause
\item The odds ratio of getting lung cancer for an additional year of smoking is $\exp(0.0729) = 1.08$.
\end{itemize}


## What do the numbers not mean ...

The most common mistake made when interpreting logistic regression is to treat an odds ratio as a ratio of probabilities.\pause

Bird keepers are \underline{not} 4x more likely to develop lung cancer than non-bird keepers.

\pause

This is the difference between relative risk and an odds ratio.


\[RR = \frac{P(\text{disease} | \text{exposed})}{P(\text{disease} | \text{unexposed})} \]

\[OR = \frac{P(\text{disease} | \text{exposed}) / [1-P(\text{disease} | \text{exposed})]}{P(\text{disease} | \text{unexposed})/[1-P(\text{disease} | \text{unexposed})]} \]

## Back to the birds

What is probability of lung cancer in a bird keeper if we knew that $P(\text{lung cancer}|\text{no birds}) = 0.05$?

\begin{align*}
OR &= \frac{P(\text{lung cancer} | \text{birds}) / [1-P(\text{lung cancer} | \text{birds})]}{P(\text{lung cancer} | \text{no birds})/[1-P(\text{lung cancer} | \text{no birds})]} \\
\\
   &= \frac{P(\text{lung cancer} | \text{birds}) / [1-P(\text{lung cancer} | \text{birds})]}{0.05/[1-0.05]} = 3.91
\end{align*}

\pause

\[ P(\text{lung cancer} | \text{birds}) =  \frac{3.91 \times \frac{0.05}{0.95}}{1+3.91 \times \frac{0.05}{0.95}} = 0.171\]

\pause

\[ RR = P(\text{lung cancer} | \text{birds}) / P(\text{lung cancer} | \text{no birds})\]

\[= 0.171 / 0.05 = 3.41\]

## Bird OR Curve

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
# odds ratios

p = (1:99)/100

f = function(p) (3.91 * p/(1-p))/(1+3.91 * p/(1-p))

par(mar=c(5, 4, 2, 2) + 0.1)

plot(p,f(p),xlab="P(lung cancer | no birds)",ylab="P(lung cancer | birds)",type="l", cex.axis = 1.5, cex.lab = 1.5, lwd = 4)
abline(a=0,b=1,col='lightgrey', lwd = 4)
```


## Bird OR Curve

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
# odds ratios

par(mar=c(5, 4, 2, 2) + 0.1)

plot(p,f(p),xlab="P(lung cancer | no birds)",ylab="P(lung cancer | birds)",type="l", cex.axis = 1.5, cex.lab = 1.5, lwd = 4)
abline(a=0,b=1,col='lightgrey', lwd = 4)

points(0.05,f(0.05),col='red',pch=16, cex = 2)
lines(c(0.05,0.05),c(0,f(0.05)),col='red', lwd = 4)
```


## Bird OR Curve

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
# odds ratios

par(mar=c(5, 4, 2, 2) + 0.1)

plot(p,f(p),xlab="P(lung cancer | no birds)",ylab="P(lung cancer | birds)",type="l", cex.axis = 1.5, cex.lab = 1.5, lwd = 4)
abline(a=0,b=1,col='lightgrey', lwd = 4)

points(0.05,f(0.05),col='red',pch=16, cex = 2)
lines(c(0.05,0.05),c(0,f(0.05)),col='red', lwd = 4)
lines(c(-1,0.05),c(f(0.05),f(0.05)),col='red', lwd = 4)
```


## Bird OR Curve

![](all_OR.pdf)

## (An old) Example - \underline{House}

If you've ever watched the TV show \underline{House} on Fox, you know that Dr. House regularly states, "It's never lupus."

Lupus is a medical phenomenon where antibodies that are supposed to attack foreign cells to prevent infections instead see plasma proteins as foreign bodies, leading to a high risk of blood clotting. It is believed that 2\% of the population suffer from this disease.


The test for lupus is very accurate if the person actually has lupus, however is very inaccurate if the person does not. More specifically, the test is 98\% accurate if a person actually has the disease. The test is 74\% accurate if a person does not have the disease.


Is Dr. House correct even if someone tests positive for Lupus? 

## (An old) Example - \underline{House}

\centering
![](tree_lupus.pdf){width="85%"}


\begin{align*}
P(\text{Lupus} | +)  &= \frac{P(+,\text{Lupus})}{P(+,\text{Lupus})+P(+,\text{No Lupus})} \\
                     &= \frac{0.0196}{0.0196+0.2548} = 0.0714
\end{align*}


## Testing for lupus

It turns out that testing for Lupus is actually quite complicated, a diagnosis usually relies on the outcome of multiple tests, often including: a complete blood count, an erythrocyte sedimentation rate, a kidney and liver assessment, a urinalysis, and or an antinuclear antibody (ANA) test. 

It is important to think about what is involved in each of these tests (e.g. deciding if complete blood count is high or low) and how each of the individual tests and related decisions plays a role in the overall decision of diagnosing a patient with lupus.

## Testing for lupus

At some level we can view a diagnosis as a binary decision (lupus or no lupus) that involves the complex integration of various explanatory variables. 

The example does not give us any information about how a diagnosis is made, but what it does give us is just as important - the sensitivity and the specificity of the test. These values are critical for our understanding of what a positive or negative test result actually means.

## Sensitivity and specificity

\textbf{Sensitivity} - measures a tests ability to identify positive results.
\[P(\text{Test }+~|~\text{Conditon }+) = P(+ | \text{lupus}) = 0.98\]

\textbf{Specificity} - measures a tests ability to identify negative results.

\[P(\text{Test }-~|~\text{Condition }-) = P(- | \text{no lupus}) = 0.74\]

\pause

It is illustrative to think about the extreme cases - what is the sensitivity and specificity of a test that always returns a positive result? What about a test that always returns a negative result?


## Sensitivity and specificity

\centering
![](SenSpec.pdf){width="75%"}


\small

\begin{align*}
\textbf{Sensitivity} &= P(\text{Test }+~|~\text{Condition }+) = TP / (TP + FN) \\ 
\textbf{Specificity} &= P(\text{Test }-~|~\text{Condition }-)  = TN / (FP + TN) \\ 
\textbf{False negative rate} (\beta)  &= P(\text{Test }-~|~\text{Condition }+)  = FN / (TP + FN) \\ 
\textbf{False positive rate} (\beta) &= P(\text{Test }+~|~\text{Condition }-)  = FP / (FP + TN)
\end{align*}


\pause

\begin{align*}
\textbf{Sensitivity} &= 1 - \textbf{False negative rate} = \text{Power}\\
\textbf{Specificity} &= 1 - \textbf{False positive rate}
\end{align*}


## So what?

Clearly it is important to know the Sensitivity and Specificity of test (and or the false positive and false negative rates). Along with the incidence of the disease (e.g. $P(\text{lupus})$) these values are necessary to calculate important quantities like $P(\text{lupus} | + )$.

Additionally, our brief foray into power analysis before the first midterm should also give you an idea about the trade offs that are inherent in minimizing false positive and false negative rates (increasing power required either increasing $\alpha$ or $n$).

How should we use this information when we are trying to come up with a decision?


## Back to Spam

In lab this week, we examined a data set of emails where we were interesting in identifying the spam messages. We examined different logistic regression models to evaluate how different predictors influenced the probability of a message being spam.

These models can also be used to assign probabilities to incoming messages (this is equivalent to prediction in the case of SLR / MLR). However, if we were designing a spam filter this would only be half of the battle, we would also need to use these probabilities to make a decision about which emails get flagged as spam.

\pause

While not the only possible solution, we will consider a simple approach where we choose a threshold probability and any email that exceeds that probability is flagged as spam.

## Picking a threshold

![](spam1.pdf)

\pause

Lets see what happens if we pick out threshold to be \alert{0.75}.

## Picking a threshold

![](spam2.pdf)

Lets see what happens if we pick out threshold to be \alert{0.75}.

## Picking a threshold

![](spam3.pdf)

Lets see what happens if we pick out threshold to be \alert{0.75}.

## Picking a threshold

![](spam4.pdf)

Lets see what happens if we pick out threshold to be \alert{0.75}.

## Picking a threshold

![](spam5.pdf)

Lets see what happens if we pick out threshold to be \alert{0.75}.

## Consequences of picking a threshold

For our data set picking a threshold of 0.75 gives us the following results:

\begin{align*}
FN = 340 &\qquad TP = 27 \\
TN = 3545 &\qquad FP = 9
\end{align*}

\pause

\alert{What are the sensitivity and specificity for this particular decision rule?}

\pause


\begin{align*}
\text{Sensitivity} & = TP / (TP + FN) = 27 / (27+340) = 0.073 \\
\text{Specificity} & = TN / (FP + TN) = 3545 / (9+3545) = 0.997 \\
\end{align*}


## Trying other thresholds

![](spam5-1.pdf)

\begin{center}
\begin{tabular}{|c|ccccc|}
\hline
Threshold   & 0.75 & 0.625 & 0.5 & 0.375 & 0.25 \\
\hline
Sensitivity & 0.074 &  &  &  &  \\
Specificity & 0.997 &  &  &  &  \\
\hline
\end{tabular}
\end{center}

## Trying other thresholds

![](spam5-2.pdf)

\begin{center}
\begin{tabular}{|c|ccccc|}
\hline
Threshold   & 0.75 & 0.625 & 0.5 & 0.375 & 0.25 \\
\hline
Sensitivity & 0.074 & 0.106 &  &  &  \\
Specificity & 0.997 & 0.995 &  &  &  \\
\hline
\end{tabular}
\end{center}

## Trying other thresholds

![](spam5-3.pdf)

\begin{center}
\begin{tabular}{|c|ccccc|}
\hline
Threshold   & 0.75 & 0.625 & 0.5 & 0.375 & 0.25 \\
\hline
Sensitivity & 0.074 & 0.106 & 0.136 &  &  \\
Specificity & 0.997 & 0.995 & 0.995 &  &  \\
\hline
\end{tabular}
\end{center}


## Trying other thresholds

![](spam5-4.pdf)

\begin{center}
\begin{tabular}{|c|ccccc|}
\hline
Threshold   & 0.75 & 0.625 & 0.5 & 0.375 & 0.25 \\
\hline
Sensitivity & 0.074 & 0.106 & 0.136 & 0.305 &  \\
Specificity & 0.997 & 0.995 & 0.995 & 0.963 &  \\
\hline
\end{tabular}
\end{center}


## Trying other thresholds

![](spam5-5.pdf)

\begin{center}
\begin{tabular}{|c|ccccc|}
\hline
Threshold   & 0.75 & 0.625 & 0.5 & 0.375 & 0.25 \\
\hline
Sensitivity & 0.074 & 0.106 & 0.136 & 0.305 & 0.510 \\
Specificity & 0.997 & 0.995 & 0.995 & 0.963 & 0.936 \\
\hline
\end{tabular}
\end{center}


## Relationship between Sensitivity and Specificity

\begin{center}
\begin{tabular}{|c|ccccc|}
\hline
Threshold   & 0.75 & 0.625 & 0.5 & 0.375 & 0.25 \\
\hline
Sensitivity & 0.074 & 0.106 & 0.136 & 0.305 & 0.510 \\
Specificity & 0.997 & 0.995 & 0.995 & 0.963 & 0.936 \\
\hline
\end{tabular}
\end{center}

\centering
![](sen_vs_spec1.pdf){width="65%"}

## Relationship between Sensitivity and Specificity

\begin{center}
\begin{tabular}{|c|ccccc|}
\hline
Threshold   & 0.75 & 0.625 & 0.5 & 0.375 & 0.25 \\
\hline
Sensitivity & 0.074 & 0.106 & 0.136 & 0.305 & 0.510 \\
Specificity & 0.997 & 0.995 & 0.995 & 0.963 & 0.936 \\
\hline
\end{tabular}
\end{center}

\centering
![](sen_vs_spec2.pdf){width="65%"}


## Relationship between Sensitivity and Specificity

\begin{center}
\begin{tabular}{|c|ccccc|}
\hline
Threshold   & 0.75 & 0.625 & 0.5 & 0.375 & 0.25 \\
\hline
Sensitivity & 0.074 & 0.106 & 0.136 & 0.305 & 0.510 \\
Specificity & 0.997 & 0.995 & 0.995 & 0.963 & 0.936 \\
\hline
\end{tabular}
\end{center}

\centering
![](sen_vs_spec3.pdf){width="65%"}


## Receiver operating characteristic (ROC) curve

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
par(mar=c(5, 4, 2, 2) + 0.1)
g_full = glm(spam ~ ., data=email, family=binomial)
pred = prediction(g_full$fitted.values, email$spam)
perf = performance(pred,"tpr","fpr")
plot(perf, lwd = 4, cex.lab = 1.5)
abline(a=0,b=1,col="lightgrey", lwd = 4)
```

## Receiver operating characteristic (ROC) curve

Why do we care about ROC curves?

\begin{itemize}
\item Shows the trade off in sensitivity and specificity for all possible thresholds.

\item Straight forward to compare performance vs. chance.

\item Can use the area under the curve (AUC) as an assessment of the predictive ability of a model.

\end{itemize}


## Refining the Spam model

\tiny
```{r}
g_refined = glm(spam ~ to_multiple+cc+image+attach+winner+password+line_breaks+format+re_subj+urgent_subj+exclaim_mess, data=email, family=binomial)
summary(g_refined)
```


## Comparing models

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
pred_full = prediction( g_full$fitted.values, email$spam)
pred_refined = prediction( g_refined$fitted.values, email$spam)

aucs = round(c(performance(pred_full,"auc")@y.values[[1]], performance(pred_refined,"auc")@y.values[[1]]),3)

par(mar=c(5, 4, 2, 2) + 0.1, cex.lab = 1.5)

plot(performance(pred_full,"tpr","fpr"), lwd = 4)
plot(performance(pred_refined,"tpr","fpr"),add=TRUE, col=COL[1], lwd = 4)
legend("bottomright",paste0(c("Full","Refined"), " (AUC: ",aucs,")"),col=c("black",COL[1]), lwd = 2, lty=1, cex = 1.5)

abline(a=0,b=1,col="lightgrey", lwd = 4)
```


## Utility Functions

There are many other reasonable quantitative approaches we can use to decide on what is the "best" threshold.


If you've taken an economics course you have probably heard of the idea of utility functions, we can assign costs and benefits to each of the possible  outcomes and use those to calculate a utility for each circumstance.


## Utility function for our spam filter

To write down a utility function for a spam filter we need to consider the costs / benefits of each out.

\renewcommand\arraystretch{1.5}
\begin{center}
\begin{tabular}{l|c}
Outcome & Utility \\
\hline
True Positive  & 1 \\
True Negative  &  \\
False Positive &  \\
False Negative &  \\
\end{tabular}
\end{center}

## Utility function for our spam filter

To write down a utility function for a spam filter we need to consider the costs / benefits of each out.

\renewcommand\arraystretch{1.5}
\begin{center}
\begin{tabular}{l|c}
Outcome & Utility \\
\hline
True Positive  & 1 \\
True Negative  & 1 \\
False Positive &  \\
False Negative &  \\
\end{tabular}
\end{center}

## Utility function for our spam filter

To write down a utility function for a spam filter we need to consider the costs / benefits of each out.

\renewcommand\arraystretch{1.5}
\begin{center}
\begin{tabular}{l|c}
Outcome & Utility \\
\hline
True Positive  & 1 \\
True Negative  & 1 \\
False Positive & -50 \\
False Negative &  \\
\end{tabular}
\end{center}


## Utility function for our spam filter

To write down a utility function for a spam filter we need to consider the costs / benefits of each out.

\renewcommand\arraystretch{1.5}
\begin{center}
\begin{tabular}{l|c}
Outcome & Utility \\
\hline
True Positive  & 1 \\
True Negative  & 1 \\
False Positive & -50 \\
False Negative & -5 \\
\end{tabular}
\end{center}


## Utility function for our spam filter

To write down a utility function for a spam filter we need to consider the costs / benefits of each out.

\renewcommand\arraystretch{1.5}
\begin{center}
\begin{tabular}{l|c}
Outcome & Utility \\
\hline
True Positive  & 1 \\
True Negative  & 1 \\
False Positive & -50 \\
False Negative & -5 \\
\end{tabular}
\end{center}

\[U(p) = TP(p) + TN(p) - 50 \times FP(p) - 5 \times FN(p)\]


## Utility for the 0.75 threshold

For the email data set picking a threshold of 0.75 gives us the following results:

\begin{align*}
FN = 340 &\qquad TP = 27 \\
TN = 3545 &\qquad FP = 9
\end{align*}

\pause

\begin{align*}
U(p) &= TP(p) + TN(p) - 50 \times FP(p) - 5 \times FN(p) \\
     &= 27+3545-50 \times 9-5 \times 340 = 1422
\end{align*}

\pause

Not useful by itself, but allows us to compare with other thresholds.


## Utility curve

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
U = c()
p = c()
for(thres in seq(0,1,0.01))
{
    if (thres %in% c(0,1))
        next
    t = table(email$spam, g_full$fitted.values>thres)



    FN = t[2,1]
    FP = t[1,2]
    TP = t[2,2]
    TN = t[1,1]

    U = c(U, TP + TN - 50 * FP - 5 * FN)
    p = c(p,thres)   
}

par(mar=c(5, 4, 2, 2) + 0.1)
plot(U ~ p,type='l', lwd = 4, cex.lab = 1.5)

points(0.75,1422,col=COL[4],cex = 2, lwd = 3)
```


## Utility curve (zoom)

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
par(mar=c(5, 4, 2, 2) + 0.1)
plot(U[p>0.6] ~ p[p>0.6],type='l', lwd = 4, cex.lab = 1.5)

points(0.7535,1422,col=COL[4],cex = 2, lwd = 3)
```

## Utility curve (zoom)

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
par(mar=c(5, 4, 2, 2) + 0.1)
plot(U[p>0.6] ~ p[p>0.6],type='l', lwd = 4, cex.lab = 1.5)

points(0.7535,1422,col=COL[4],cex = 2, lwd = 3)
i=which.max(U)
points(p[i],U[i],col=COL[1],pch=16,cex = 2, lwd = 3)
```

## Maximum Utility

![](utility4.pdf)
