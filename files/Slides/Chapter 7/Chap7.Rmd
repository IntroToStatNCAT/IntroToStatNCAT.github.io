---
title: "Chapter 7"
subtitle: "Inference for numerical data^[These notes use content from OpenIntro Statistics Slides by Mine Cetinkaya-Rundel.]"
author: |
  | Department of Mathematics & Statistics
  | North Carolina A\&T State University
#date: "8/19/2020"
urlcolor: blue
header-includes:
    \usepackage{multirow}
    \usepackage{graphicx}
    \usepackage{subfigure}
    \usepackage{multicol}
    \usepackage[utf8]{inputenc}
    \usepackage[english]{babel}
    \usepackage{bm}
    \usepackage{amsmath}
    \usepackage{tikz}
    \usepackage{mathtools}
    \usepackage{textcomp}
    \usepackage{fdsymbol}
    \usepackage{siunitx}
    \usepackage{xcolor,pifont}
    \usepackage{hyperref}
    \usepackage{mathtools}
output: 
  beamer_presentation:
    fig_caption: true
    latex_engine: xelatex
    
    
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=F, message=F, warning=F}
library(readr)
library(openintro)
library(here)
library(tidyverse)
library(xtable)
data(COL)
```

# One-sample means with the _t_ distribution

## Heights

- According to the CDC, the mean height of U.S. adults ages 20 and older is about 66.5 inches (69.3 inches for males, and 63.8 inches for females).

- In our sample data, we have a sample of 430 college students from a single college.

```{r, echo=F, message=F, warning=F, out.width="75%",fig.align='center'}
set.seed(123)
col_heights = rnorm(n = 430, mean = 67, sd = 5)
hist(col_heights, xlab = "Height (inches)", main = "Histogram of college student heights")
```


## Summary statistics

\small
\begin{center}
\begin{tabular}{ccccc}
  \hline
n & $\bar x$ & s & minimum & maximum \\ 
  \hline
430 & 67.09 & 4.86 & 53.78 & 83.21 \\
   \hline
\end{tabular}
\end{center}

**Objective:** We would like to investigate if the mean height of students at this college is significantly different than 66.5 inches.


## Conditions

- **Independence:** We are told to assume that cases (rows) are independent.

\pause

- **Sample size / skew:** \pause

\begin{columns}

\begin{column}{0.7\textwidth}
\begin{itemize}
\item The sample distribution does not appear to be extremely skewed, but it's very difficult to assess with such a small sample size. We might want to think about whether we would expect the population distribution to be skewed or not $-$ probably not, it should be equally likely to have days with lower than average traffic and higher than average traffic.
\end{itemize}
\end{column}

\begin{column}{0.3\textwidth}

```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
friday = read.csv("friday.csv", h = T, na.strings = "")

# histogram
histPlot(friday$diff[friday$type == "traffic"], col="#22558833", border="#225588", xlab = "Difference in traffic flow", cex.lab = 1.5)

```

\end{column}

\end{columns}

- We do not know $\sigma$ and $n$ is too small to assume $s$ is a reliable estimate for $\sigma$.

\pause

\alert{So what do we do when the sample size is small?}

## Review: what purpose does a large sample serve?

As long as observations are independent, and the population distribution is not extremely skewed, a large sample would ensure that...

  - The sampling distribution of the mean is nearly normal.
  
  - The estimate of the standard error, as $\frac{s}{\sqrt{n}}$, is reliable.
  
## The normality condition

- The CLT, which states that sampling distributions will be nearly normal, holds true for **any** sample size as long as the population distribution is nearly normal.

\pause

- While this is helpful special case, it's inherently difficult to verify normality in small data sets.

\pause

- We should exercise caution when verifying the normality condition for small samples. It is important to not only examine the data but also think about where the data come from.

  - For example, ask: would I expect this distribution to be symmetric, and am I confident that outliers are rare?
  

## The _t_ distribution

- When the population standard deviation is unknown (almost always), the uncertainty of the standard error estimate is addressed by using a new distribution: the **_t_ distribution**.

\pause

- This distribution also has a bell shape, but its tails are **thicker** than the normal model's.

\pause

- Therefore observations are more likely to fall beyond two SDs from the mean than under the normal distribution.

\pause

- Extra thick tails are helpful for resolving our problem with a less reliable estimate the standard error (since _n_ is small).

```{r, echo=F, message=F, warning=F, fig.width=4, fig.height=1.4,fig.align='center'}
par(mar=c(2, 1, 1, 1), mgp=c(5, 0.6, 0))
plot(c(-4, 4), c(0, dnorm(0)), type='n', axes=FALSE, cex.axis=0.8)
axis(1)
abline(h=0)

X <- seq(-5, 5, 0.01)
Y <- dnorm(X)
lines(X, Y, lty=3, lwd=2)

Y <- dt(X, 2)
lines(X, Y, lwd=1.5, col = "#225588")

legend("topright", inset = 0.05, lty = c(3,1), lwd = c(2,1.5), c("normal","t"), col = c("black","#225588"), bty = "n")
```


## The _t_ distribution

- Always centered at zero, like the standard normal ($z$) distribution.

- Has a single parameter: **degrees of freedom** ($\mathbf{df}$).

```{r, echo=F, message=F, warning=F, fig.width=4, fig.height=1.7,fig.align='center'}
par(mar=c(2, 1, 1, 1), mgp=c(5, 0.6, 0))
plot(c(-3.5, 7), c(0, dnorm(0)), type='n', axes=FALSE)
axis(1, cex.axis = 0.75)
abline(h=0)

COL <- c('#000000FF', '#00000091', '#0000006F', '#0000004B', '#00000022')
DF <- c('normal', 10, 5, 2, 1)

X <- seq(-5, 8, 0.01)
Y <- dnorm(X)
lines(X, Y, col=COL[1])

for(i in 2:5){
	Y <- dt(X, as.numeric(DF[i]))
	lines(X, Y, col=COL[i])
}

legend('topright', legend=c(DF[1], paste('t, df=', DF[2:5], sep='')), col=COL, lty=rep(1, 5), bty = "n", cex = 0.85)

```

\pause

\alert{What happens to shape of the $t$ distribution as $df$ increases?}

\pause

Approaches normal.

## Back to the student heights survey

\small
\begin{center}
\begin{tabular}{ccccc}
  \hline
n & $\bar x$ & s & minimum & maximum \\ 
  \hline
430 & 67.09 & 4.86 & 53.78 & 83.21 \\
   \hline
\end{tabular}
\end{center}

**Objective:** We would like to investigate if the mean height of students at this college is significantly different than 66.5 inches.

## Hypotheses

\alert{What are the hypotheses for testing for the mean of college student heights being different from 67 inches?}

\begin{enumerate}[A)]
\item  $H_0: \mu = 66.5$ \\ $H_A: \mu \ne 66.5$
\item  $H_0: \mu = 66.5$ \\ $H_A: \mu > 66.5$
\item  $H_0: \mu = 66.5$ \\ $H_A: \mu < 66.5$ 
\item  $H_0: \mu \ne 66.5$ \\ $H_A: \mu > 66.5$
\end{enumerate}

## Hypotheses

\alert{What are the hypotheses for testing for the mean of college student heights being different from 66.5 inches?}

\begin{enumerate}[A)]
\item  $\textcolor{red}{H_0: \mu = 66.5}$ \\ $\textcolor{red}{H_A: \mu \ne 66.5}$
\item  $H_0: \mu = 66.5$ \\ $H_A: \mu > 66.5$
\item  $H_0: \mu = 66.5$ \\ $H_A: \mu < 66.5$ 
\item  $H_0: \mu \ne 66.5$ \\ $H_A: \mu > 66.5$
\end{enumerate}

## Finding the test statistic

The test statistic for inference on sample mean is the $T$ statistic with $df = n - 1$.

\centering{$T_{df} = \frac{\text{point estimate} - \text{null value}}{SE}$}

\pause

\raggedright
\alert{in context...}


\[\text{point estimate} = \bar{x} = 67.09\]
\pause
\[SE = \frac{s}{\sqrt{n}} = \frac{4.86}{\sqrt{430}} = 0.234 \]
\pause
\[T = \frac{67.09 - 66.5}{0.234} = 2.52\]
\pause
\[df = 430-1 = 429\]

\noindent\rule{4cm}{0.4pt}
\alert{Note:} Null value is 66.5 because in the null hypothesis we set $\mu = 66.5.$


## Finding the p-value

- The p-value is, once again, calculated as the area tail area under the $t$ distribution.

\pause

- Using R:

```{r, echo=T}
2 * pt(2.52, df = 429, lower.tail = FALSE)
```

\pause

- Using a web app:

  [https://gallery.shinyapps.io/dist_calc/](https://gallery.shinyapps.io/dist_calc/)

\pause

- Or when these aren't available, we can use a $t$-table.

## Conclusion of the test

\alert{What is the conclusion of this hypothese test?}

\pause

We saw that the p-value was extremely low. Thus, we reject the null hypothesis. Based on the p-value, we conclude that the survey provide strong evidence that the mean of the college students height is different from the mean height of U.S. adults over 20.

## What is the difference?

- We concluded that there is a difference in the mean heights of the college students compared to the mean height of U.S. adults

\pause

- But it would be more interesting to find out what exactly this difference is.

\pause

- We can use a confidence interval to estimate this difference.

## Confidence interval for a sample mean

- Confidence intervals are always of the form

\centering{$\text{point estimate} \pm ME$}

\pause

- ME is always calculated as the product of a critical value and SE.

\pause

- $ME = t^* \times SE$

\centering{$\text{point estimate} \pm t^* \times SE$}

## Finding the critical $t (t^*)$

- We want to find the 95% confidence interval.

- Using R:

```{r, echo=T}
qt(p = (1+0.95)/2, df = 429)
```

## Constructing a CI for a small sample mean

\alert{Which of the following is the correct calculation of a 95\% confidence interval for the heights of the college students?}

\centering{$\bar{x} = 67.09 \hspace{0.5cm} s = 4.86 \hspace{0.5cm} n = 430 \hspace{0.5cm} SE = 0.234$}

\begin{enumerate}[A)]
\item $66.5 \pm 1.96 \times 0.234$
\item $67.09 \pm 1.97 \times 0.234$
\item $67.09 \pm -2.26 \times 0.234$
\item $66.5 \pm 2.26 \times 4.86$
\end{enumerate}

## Constructing a CI for a small sample mean

\alert{Which of the following is the correct calculation of a 95\% confidence interval for the heights of the college students?}

\centering{$\bar{x} = 67.09 \hspace{0.5cm} s = 4.86 \hspace{0.5cm} n = 430 \hspace{0.5cm} SE = 0.234$}

\begin{enumerate}[A)]
\item $66.5 \pm 1.96 \times 0.234$
\item $\textcolor{red}{67.09 \pm 1.97 \times 0.234 \rightarrow (66.63, 67.55)}$
\item $67.09 \pm -2.26 \times 0.234$
\item $66.5 \pm 2.26 \times 4.86$
\end{enumerate}



## Synthesis

\alert{Does the conclusion from the hypothesis test agree with the findings of the confidence intereval?}

\pause

Yes, the hypothesis test found a significant difference, and the CI does not contain the null value of 66.5.



## Recap: Inference using the $t$-distribution

- If $\sigma$ is unknown, use $t$-distribution with $SE = \frac{s}{\sqrt{n}}$.

\pause

- Conditions:
  
  - Independence of observations (often verified by random sample, and if sampling w/o replacement, n < 10% of population).
  
  - No extreme skew.
  
\pause

- Hypothesis testing:

\centering{$T_{df} = \frac{\text{point estimate}-\text{null value}}{SE}, \text{where }df=n-1$}

\pause

- Confidence interval: $\text{point estimate} \pm t^*_{df} \times SE$



# Paired data

## 

\alert{200 observations were randomly sampled from the High School and Beyond survey. The same students took a reading and writing test and their scores are shown below. At a first glance, does there appear to be a difference between the average reading and writing test score?}

```{r, echo=F, message=F, warning=F, out.width="90%",fig.align='center'}
data(hsb2)
data(COL)
par(mar=c(3, 4, 0.5, 0.5), las=1, mgp=c(2.8, 0.7, 0), cex.axis = 1.1, cex.lab = 1.1)
scores <- c(hsb2$read, hsb2$write)
gp <- c(rep('read', nrow(hsb2)), rep('write', nrow(hsb2)))
openintro::dotPlot(scores, gp, vertical=TRUE, xlab="",ylab = "scores", at=1:2+0.13, col=fadeColor(COL[1],33), xlim=c(0.5,2.5), ylim=c(20, 80), axes = FALSE, cex.lab = 1.8, cex.axis = 1.8)
axis(1, at = c(1,2), labels = c("read","write"), cex.lab = 1.8, cex.axis = 1.8)
axis(2, at = seq(20,80,20), cex.axis = 1.25)
boxplot(scores ~ gp, add=TRUE, axes=FALSE)
abline(h=0)
```

## Practice

\alert{The same students took a reading and writing test and their scores are shown below. Are the reading and writing scores of each student independent of each other?}

\begin{center}
\begin{tabular}{rrrr}
  \hline
 & id & read & write \\ 
  \hline
1 & 70 & 57 & 52 \\ 
  2 & 86 & 44 & 33 \\ 
  3 & 141 & 63 & 44 \\ 
  4 & 172 & 47 & 52 \\ 
  $\vdots$ &   $\vdots$  &   $\vdots$ &   $\vdots$  \\
  200 & 137 & 63 & 65 \\ 
   \hline
\end{tabular}
\end{center}

A) Yes $\hspace{3.5cm}$ B) No

## Practice

\alert{The same students took a reading and writing test and their scores are shown below. Are the reading and writing scores of each student independent of each other?}

\begin{center}
\begin{tabular}{rrrr}
  \hline
 & id & read & write \\ 
  \hline
1 & 70 & 57 & 52 \\ 
  2 & 86 & 44 & 33 \\ 
  3 & 141 & 63 & 44 \\ 
  4 & 172 & 47 & 52 \\ 
  $\vdots$ &   $\vdots$  &   $\vdots$ &   $\vdots$  \\
  200 & 137 & 63 & 65 \\ 
   \hline
\end{tabular}
\end{center}

A) Yes $\hspace{3.5cm}$ B) \alert{No}

## Analyzed paired data

- When two sets of observations have this special correspondence (not independent), they are said to be **paired**.

\pause

- To analyze paired data, it is often useful to look at the difference in outcomes of each pair of observations.

\centering{diff = read $-$ write}

\pause

- It is important that we always subtract using a consistent order.

\begin{columns}

\begin{column}{0.5\textwidth}
\small
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & id & read & write & diff \\ 
  \hline
  1 & 70 & 57 & 52 & 5 \\ 
  2 & 86 & 44 & 33 & 11 \\ 
  3 & 141 & 63 & 44 & 19 \\ 
  4 & 172 & 47 & 52 & -5 \\ 
  $\vdots$ &   $\vdots$  &   $\vdots$ &   $\vdots$ &   $\vdots$ \\
  200 & 137 & 63 & 65 & -2 \\ 
   \hline
\end{tabular}
\end{center}
\normalsize
\end{column}

\begin{column}{0.5\textwidth}
```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
data(hsb2)
data(COL)
histPlot(hsb2$read - hsb2$write, col = COL[1], xlab = "Differences in scores (read - write)", ylab = "", cex.lab = 2, cex.axis = 3)
```
\end{column}

\end{columns}


## Parameter and point estimate

- **Parameter of interest:** Average difference between the reading and writing scores of **all** high school students.

\centering{$\mu_{diff}$}

\pause

- **Point estimate:** Average difference between the reading and writing scores of **sampled** high school students.

\centering{$\bar{x}_{diff}$}

## Setting the hypotheses

\alert{If in fact there was no difference between the scores on the reading and writing exams, what would you expect the average difference to be?}

\pause

0

\pause

\alert{What are the hypotheses for testing if there is a difference between the average reading and writing scores?}

\pause

\begin{itemize}

\item[$H_0:$] There is no difference between the average reading and writing score. \\
\centering{$\mu_{diff}=0$}

\item[$H_A:$] There is a difference between the average reading and writing score. \\
\centering{$\mu_{diff} \ne 0$}

\end{itemize}


## Nothing new here

- The analysis is no different than what we have done before.

- We have data from **one** sample: differences.

- We are testing to see if average difference is different than 0.

## Checking assumptions & conditions

\alert{Which of the following is true?}

A) Since students are sampled randomly and are less than 10% of all high school students, we can assume that the difference between the reading and writing scores of one student in the sample is independent of another.

B) The distribution of differences is bimodal, therefore we cannot continue with the hypothesis test.

C) In order for differences to be random we should have sampled with replacement.

D) Since students are sampled randomly and are less than 10% all students, we can assume that the sampling distribution of the average difference will be nearly normal.

## Checking assumptions & conditions

\alert{Which of the following is true?}

A) \alert{Since students are sampled randomly and are less than 10\% of all high school students, we can assume that the difference between the reading and writing scores of one student in the sample is independent of another.}

B) The distribution of differences is bimodal, therefore we cannot continue with the hypothesis test.

C) In order for differences to be random we should have sampled with replacement.

D) Since students are sampled randomly and are less than 10% all students, we can assume that the sampling distribution of the average difference will be nearly normal.

## Calculating the test-statistic and the p-value

\alert{The observed average difference between the two scores is -0.545 points and the standard deviation of the difference is 8.887 points. Do these data provide convincing evidence of a difference between the average scores on the two exams? Use $\alpha = 0.05$.}

\begin{columns}

\begin{column}{0.5\textwidth}
```{r, echo=F, message=F, warning=F, out.width="100%",fig.align='center'}
data(hsb2)
data(COL)
par(mar=c(2,0,0,0), las=1, mgp=c(3,1,0), mfrow = c(1,1))

m = 0
s = 8.887/sqrt(200)
l = -0.545
u = 0.545

normTail(m = m, s = s, L = l, U = u, axes = FALSE, col = COL[1])
axis(1, at = c(m - 3*s,l,m,u,m + 3*s), label = c(NA,l,m,u,NA), cex.axis = 2)
```
\end{column}

\pause

\begin{column}{0.5\textwidth}
\begin{eqnarray*}
T &=& \frac{-0.545 - 0}{\frac{8.887}{\sqrt{200}}} \\
&=& \frac{-0.545}{0.628} = -0.87 \\
df &=& 200 - 1 = 199 \\
\pause
p-value &=& 0.1927 \times 2 = 0.3854
\end{eqnarray*}
\end{column}

\end{columns}

\pause

Since p-value > 0.05, fail to reject, the data do \underline{not} provide convincing evidence of a difference between the average reading and writing scores.


## Interpretation of p-value

\alert{Which of the following is the correct interpretation of the p-value?}

A) Probability that the average scores on the reading and writing exams are equal.

B) Probability that the average scores on the reading and writing exams are different.

C) Probability of obtaining a random sample of 200 students where the average difference between the reading and writing scores is at least 0.545 (in either direction), if it fact the true average difference between the scores is 0.

D) Probability of incorrectly rejecting the null hypothesis if in fact the null hypothesis is true.

## Interpretation of p-value

\alert{Which of the following is the correct interpretation of the p-value?}

A) Probability that the average scores on the reading and writing exams are equal.

B) Probability that the average scores on the reading and writing exams are different.

C) \alert{Probability of obtaining a random sample of 200 students where the average difference between the reading and writing scores is at least 0.545 (in either direction), if it fact the true average difference between the scores is 0.}

D) Probability of incorrectly rejecting the null hypothesis if in fact the null hypothesis is true.

## HT $\leftrightarrow$ CI

\alert{Suppose we were to construct 95\% confidence interval for the average difference between the reading and writing scores. Would you expect this interval to include 0?}

A) Yes

B) No

C) Cannot tell from the information given.

## HT $\leftrightarrow$ CI

\alert{Suppose we were to construct 95\% confidence interval for the average difference between the reading and writing scores. Would you expect this interval to include 0?}

A) \alert{Yes}

B) No

C) Cannot tell from the information given.

\begin{eqnarray*} 
-0.545 \pm 1.97 \frac{8.887}{\sqrt{200}} &=& -0.545 \pm 1.97 \times 0.628 \\
&=& -0.545 \pm 1.24 \\
&=& (-1.785, 0.695)
\end{eqnarray*}

## Friday the $13^{th}$

Between 1990 - 1992 researchers in the UK collected data on traffic flow, accidents, hospital admissions on Friday $13^{th}$ and the previous Friday, Friday $6^{th}$. Below is an excerpt from this data set on traffic flow. We can assume that traffic flow on given day at locations 1 and 2 are independent.

\small
\begin{center}
\begin{tabular}{rllrrrl}
  \hline
 & type & date & 6$^{\text{th}}$ & 13$^{\text{th}}$ & diff & location  \\ 
  \hline
1 & traffic & 1990,  July & 139246 & 138548 & 698 & loc 1 \\
  2 & traffic & 1990,  July & 134012 & 132908 & 1104 & loc 2 \\
  3 & traffic & 1991,  September & 137055 & 136018 & 1037 & loc 1 \\
  4 & traffic & 1991,  September & 133732 & 131843 & 1889 & loc 2 \\
  5 & traffic & 1991,  December & 123552 & 121641 & 1911 & loc 1 \\
  6 & traffic & 1991,  December & 121139 & 118723 & 2416 & loc 2 \\
  7 & traffic & 1992,  March & 128293 & 125532 & 2761 & loc 1 \\
  8 & traffic & 1992,  March & 124631 & 120249 & 4382 & loc 2 \\
  9 & traffic & 1992,  November & 124609 & 122770 & 1839 & loc 1 \\
  10 & traffic & 1992,  November & 117584 & 117263 & 321 & loc 2 \\
   \hline
\end{tabular}
\end{center}


## Friday the $13^{th}$

- We want to investigate if people's behavior is different on Friday $13^{th}$ compared to Friday $6^{th}$.

\pause

- One approach is to compare the traffic flow on these two days.

\pause

\begin{itemize}

\item We want to investigate if people's behavior is different on Friday 13$^{\text{th}}$ compared to Friday 6$^{\text{th}}$.

\pause

\item $H_0:$ Average traffic flow on Friday 6$^{\text{th}}$ and 13$^{\text{th}}$ are equal. \\
$H_A:$ Average traffic flow on Friday 6$^{\text{th}}$ and 13$^{\text{th}}$ are different.

\pause

\end{itemize}

\alert{Each case in the data set represents traffic flow recorded at the same location in the same month of the same year: one count from Friday $6^{th}$ and the other Friday $13^{th}$. Are these two counts independent?}

\pause

No.

## Hypotheses

\alert{What are the hypotheses for testing for a difference between the average traffic flow between Friday $6^{th} \text{ and } 13^{th}$?}

\begin{enumerate}[A)]
\item  $H_0: \mu_{6th} = \mu_{13th}$ \\ $H_A: \mu_{6th} \ne \mu_{13th}$
\item  $H_0: p_{6th} = p_{13th}$ \\ $H_A: p_{6th} \ne p_{13th}$
\item  $H_0: \mu_{diff} = 0$ \\ $H_A: \mu_{diff} \ne 0$ 
\item  $H_0: \bar{x}_{diff} = 0$ \\ $H_A: \bar{x}_{diff} = 0$
\end{enumerate}


## Hypotheses

\alert{What are the hypotheses for testing for a difference between the average traffic flow between Friday $6^{th} \text{ and } 13^{th}$?}

\begin{enumerate}[A)]
\item  $H_0: \mu_{6th} = \mu_{13th}$ \\ $H_A: \mu_{6th} \ne \mu_{13th}$
\item  $H_0: p_{6th} = p_{13th}$ \\ $H_A: p_{6th} \ne p_{13th}$
\item  $\textcolor{red}{H_0: \mu_{diff} = 0}$ \\ $\textcolor{red}{H_A: \mu_{diff} \ne 0}$ 
\item  $H_0: \bar{x}_{diff} = 0$ \\ $H_A: \bar{x}_{diff} = 0$
\end{enumerate}

## Friday the $13^{th}$

\small
\begin{tabular}{rllrr || r || l}
  \hline
 & type & date & 6$^{\text{th}}$ & 13$^{\text{th}}$ & diff & location  \\ 
  \hline
  1 & traffic & 1990,  July & 139246 & 138548 & 698 & loc 1 \\
  2 & traffic & 1990,  July & 134012 & 132908 & 1104 & loc 2 \\
  3 & traffic & 1991,  September & 137055 & 136018 & 1037 & loc 1 \\
  4 & traffic & 1991,  September & 133732 & 131843 & 1889 & loc 2 \\
  5 & traffic & 1991,  December & 123552 & 121641 & 1911 & loc 1 \\
  6 & traffic & 1991,  December & 121139 & 118723 & 2416 & loc 2 \\
  7 & traffic & 1992,  March & 128293 & 125532 & 2761 & loc 1 \\
  8 & traffic & 1992,  March & 124631 & 120249 & 4382 & loc 2 \\
  9 & traffic & 1992,  November & 124609 & 122770 & 1839 & loc 1 \\
  10 & traffic & 1992,  November & 117584 & 117263 & 321 & loc 2 \\
   \hline
\end{tabular}
\normalsize

\begin{align*}
\hspace{6cm} \bar{x}_{diff} & = 1836 \\
 s_{diff} & = 1176 \\
 n &= 10
\end{align*}

## Finding the test statistic

The test statistic for inference on a small sample (n < 50) mean is the $T$ statistic with $df = n - 1$.

\centering{$T_{df} = \frac{\text{point estimate} - \text{null value}}{SE}$}

\pause

\raggedright
\alert{in context...}

\[\text{point estimate} = \bar{x}_{diff} = 1836\]
\pause
\[SE = \frac{s_{diff}}{\sqrt{n}} = \frac{1176}{\sqrt{10}} = 372 \]
\pause
\[T = \frac{1836-0}{372} = 4.94\]
\pause
\[df = 10-1 = 9\]

\noindent\rule{4cm}{0.4pt}
\alert{Note:} Null value is 0 because in the null hypothesis we set $\mu_{diff} = 0.$


## Finding the p-value

- The p-value is, once again, calculated as the area tail area under the $t$ distribution.

\pause

- Using R:

```{r, echo=T}
2 * pt(4.94, df = 9, lower.tail = FALSE)
```

\pause

- Using a web app:

  [https://gallery.shinyapps.io/dist_calc/](https://gallery.shinyapps.io/dist_calc/)

\pause

- Or when these aren't available, we can use a $t$-table.

## Conclusion of the test

\alert{What is the conclusion of this hypothese test?}

\pause

Since the p-value is quite low, we conclude that the data provide strong evidence of a difference between traffic flow on Friday $6^{th} \text{ and } 13^{th}.$

## What is the difference?

- We concluded that there is a difference in the traffic flow between Friday $6^{th} \text{ and } 13^{th}.$

\pause

- But it would be more interesting to find out what exactly this difference is.

\pause

- We can use a confidence interval to estimate this difference.

## Confidence interval for a small sample mean

- Confidence intervals are always of the form

\centering{$\text{point estimate} \pm ME$}

\pause

- ME is always calculated as the product of a critical value and SE.

\pause

- Since small sample means follow a $t$ distribution (and not a $z$ distribution), the critical value is a $t^*$ (as opposed to a $z^*$).

\centering{$\text{point estimate} \pm t^* \times SE$}

## Finding the critical $t (t^*)$

Using R:

```{r, echo=T}
qt(p = 0.975, df = 9)
```

## Constructing a CI for a small sample mean

\alert{Which of the following is the correct calculation of a 95\% confidence interval for the difference between the traffic flow between Friday $6^{th} \text{ and } 13^{th}$?}

\centering{$\bar{x}_{diff} = 1836 \hspace{0.5cm} s_{diff} = 1176 \hspace{0.5cm} n = 10 \hspace{0.5cm} SE = 372$}

\begin{enumerate}[A)]
\item $1836 \pm 1.96 \times 372$
\item $1836 \pm 2.26 \times 372$
\item $1836 \pm -2.26 \times 372$
\item $1836 \pm 2.26 \times 1176$
\end{enumerate}

## Constructing a CI for a small sample mean

\alert{Which of the following is the correct calculation of a 95\% confidence interval for the difference between the traffic flow between Friday $6^{th} \text{ and } 13^{th}$?}

\centering{$\bar{x}_{diff} = 1836 \hspace{0.5cm} s_{diff} = 1176 \hspace{0.5cm} n = 10 \hspace{0.5cm} SE = 372$}

\begin{enumerate}[A)]
\item $1836 \pm 1.96 \times 372$
\item $\textcolor{red}{1836 \pm 2.26 \times 372 \hspace{1cm}\rightarrow(995,2677)}$
\item $1836 \pm -2.26 \times 372$
\item $1836 \pm 2.26 \times 1176$
\end{enumerate}

## Interpreting the CI

\alert{Which of the following is the \textbf{best} interpretation for the confidence interval we just calculated?}

\centering{$\textcolor{red}{\mu_{diff: \text{ } 6th - 13th} = (995,2677)}$}

\raggedright We are 95% confident that...

A) The difference between the average number of cars on the road on Friday $6^{th} \text{ and } 13^{th}$ is between 995 and 2677.

B) On Friday $6^{th}$ there are 995 to 2677 fewer cars on the road than on the Friday $13^{th}$, on average.

C) On Friday $6^{th}$ there are 995 to 2677 more cars on the road than on the Friday $13^{th}$, on average.

D) On Friday $13^{th}$ there are 995 to 2677 fewer cars on the road than on the Friday $6^{th}$, on average.


## Interpreting the CI

\alert{Which of the following is the \textbf{best} interpretation for the confidence interval we just calculated?}

\centering{$\textcolor{red}{\mu_{diff: \text{ } 6th - 13th} = (995,2677)}$}

\raggedright We are 95% confident that...

A) The difference between the average number of cars on the road on Friday $6^{th} \text{ and } 13^{th}$ is between 995 and 2677.

B) On Friday $6^{th}$ there are 995 to 2677 fewer cars on the road than on the Friday $13^{th}$, on average.

C) On Friday $6^{th}$ there are 995 to 2677 more cars on the road than on the Friday $13^{th}$, on average.

D) \textcolor{red}{On Friday $13^{th}$ there are 995 to 2677 fewer cars on the road than on the Friday $6^{th}$, on average.}

## Synthesis

\alert{Does the conclusion from the hypothesis test agree with the findings of the confidence intereval?}

\pause

Yes, the hypothesis test found a significant difference, and the CI does not contain the null value of 0.

\pause

\alert{Do you think the findings of this study suggests that people believe Friday $13^{th}$ is a day of bad luck?}

\pause

No, this is an observational study. We have just observed a significant difference between the number of cars on the road on these two days. We have not tested for people's beliefs.

# Difference of two means

## Experiment

- A 2003 American Journal of Health Education Study investigated the effects of cell phone use on reaction time while driving.

- In the study, 60 participants were randomly selected and placed into one of two groups:

  - Treatment Group - Access to text documents on a cell phone.
  
  - Control Group - No distractions
  
- Participants in each group were then asked to take a computerized reaction time test.

- Researchers then recorded each subject's reaction time in seconds.

## Data Summary

\begin{center}
\begin{tabular}{l | c | c}
		& {\footnotesize \textcolor{blue}{Treatment}} &  {\footnotesize \textcolor{blue}{Control}}  \\
		      & Phone	  & No Phone \\
\hline
$\bar{x}$	& 0.546		& 0.356 \\
$s$		    & 0.213		& 0.245 \\
$n$		    & 30			& 30
\end{tabular}
\end{center}


## Parameter and point estimate

- **Parameter of interest:** Average difference between the reaction time of **all** drivers using a phone or not.

\centering{$\mu_{C} - \mu_{T}$}

\pause

- **Point estimate:** Average difference between the reaction time of **participants** in the treatment and control group.

\centering{$\bar{x}_{C} - \bar{x}_{T}$}

## Hypotheses

\alert{Which of the following is the correct set of hypotheses for testing if the average reaction time of the drivers using a phone ($\mu_T$) is higher than the average reaction time of the drivers not using a phone ($\mu_C$)?}

\begin{enumerate}[A)]

\item  $H_0: \mu_{C} = \mu_{T}$ \\
$H_A: \mu_{C} \ne \mu_{pT}$

\item  $H_0: \mu_{C} = \mu_{T}$ \\
$H_A: \mu_{C} > \mu_{T}$

\item  $H_0: \mu_{C} = \mu_{T}$ \\
$H_A: \mu_{C} < \mu_{T}$

\item  $H_0: \bar{x}_{C} = \bar{x}_{T}$ \\
$H_A: \bar{x}_{C} < \bar{x}_{T}$

\end{enumerate}

## Hypotheses

\alert{Which of the following is the correct set of hypotheses for testing if the average reaction time of the drivers using a phone ($\mu_T$) is higher than the average reaction time of the drivers not using a phone ($\mu_C$)?}

\begin{enumerate}[A)]

\item  $H_0: \mu_{C} = \mu_{T}$ \\
$H_A: \mu_{C} \ne \mu_{pT}$

\item  $H_0: \mu_{C} = \mu_{T}$ \\
$H_A: \mu_{C} > \mu_{T}$

\item  \textcolor{red}{$H_0: \mu_{C} = \mu_{T}$} \\
\textcolor{red}{$H_A: \mu_{C} < \mu_{T}$}

\item  $H_0: \bar{x}_{C} = \bar{x}_{T}$ \\
$H_A: \bar{x}_{C} < \bar{x}_{T}$

\end{enumerate}

## Conditions

\alert{Which of the following does \underline{not} need to be satisfied in order to conduct this hypothesis test using theoretical methods?}

A) The reaction time of drivers not using a phone in the sample should be independent of another, and the reaction time of drivers using a phone should independent of another as well.

B) The reaction times of drivers using and not using a phone in the sample should be independent.

C) Distributions of reaction times of drivers in both groups should not be extremely skewed.

D) Both sample sizes should be the same.

## Conditions

\alert{Which of the following does \underline{not} need to be satisfied in order to conduct this hypothesis test using theoretical methods?}

A) The reaction time of drivers not using a phone in the sample should be independent of another, and the reaction time of drivers using a phone should independent of another as well.

B) The reaction times of drivers using and not using a phone in the sample should be independent.

C) Distributions of reaction times of drivers in both groups should not be extremely skewed.

D) \alert{Both sample sizes should be the same.}

## Test statistics

The test statistic for inference on the difference of two means where $\sigma_1$ and $\sigma_2$ are unknown is the T statistic.

\centering{$T_{df} = \frac{\text{point estimate} - \text{null value}}{SE}$}

\raggedright where

\centering{$SE = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} \hspace{1cm} \text{and} \hspace{1cm} df = min(n_1 - 1, n_2 - 1)$}

\raggedright\noindent\rule{4cm}{0.4pt}

\alert{Note:} The calculation of the $df$ is actually much more complicated. For simplicity we'll use the above formula to \underline{estimate} the true $df$ when conducting the analysis by hand.

## Test statistic

\begin{center}
\begin{tabular}{l | c | c}
		& {\footnotesize \textcolor{blue}{Treatment}} &  {\footnotesize \textcolor{blue}{Control}}  \\
		      & Phone	  & No Phone \\
\hline
$\bar{x}$	& 0.546		& 0.356 \\
$s$		    & 0.213		& 0.245 \\
$n$		    & 30			& 30
\end{tabular}
\end{center}

\alert{in context...}

\pause

\begin{eqnarray*}
T &=& \frac{\text{point estimate} - \text{null value} }{SE} \\
\pause
&=& \frac{(0.356-0.546) - 0}{\sqrt{\frac{0.213^2}{30} + \frac{0.245^2}{30} }} \\
\pause
&=& \frac{-0.19}{0.0593} \\
\pause
&=& -3.20
\end{eqnarray*}

## Test statistic

- Coincidentally, in the experiment the number of participants in both groups is the same.

- Thus, $df = min(30-1, 30-1) = min(29, 29) = 29.$

## p-value

\alert{Which of the following is the correct p-value for this hypothesis test?}

\centering{$T = -3.20 \hspace{1cm} df = 29$}

A) Between 0.0005 and 0.001

B) Between 0.001 and 0.0025

C) Between 0.002 and 0.005

D) Between 0.01 and 0.02

## p-value

\alert{Which of the following is the correct p-value for this hypothesis test?}

\centering{$T = -3.20 \hspace{1cm} df = 29$}

A) Between 0.0005 and 0.001

B) \alert{Between 0.001 and 0.0025}

C) Between 0.002 and 0.005

D) Between 0.01 and 0.02

```{r, echo=T}
pt(q = -3.20, df = 29)
```

## Synthesis

\alert{What is the conclusion of the hypothesis test? How (if at all) would this conclusion change your behavior if you went diamond shopping?}

\pause

- p-value is small so reject $H_0$. The data provide convincing evidence to suggest that the average reaction time of drivers not using a phone is faster than the drivers using a phone while driving.

- Try not to use your phone while driving because you may never know when you would require the fast reaction time to avoid an accident.

## Equivalent confidence level

\alert{What is the equivalent confidence level for a one-sided hypothesis test at $\alpha = 0.05$?}

$\vspace{1cm}$

\begin{columns}

\begin{column}{0.3\textwidth}
A) 90%

B) 92.5%

C) 95%

D) 97.5%
\end{column}

\begin{column}{0.7\textwidth}

\end{column}

\end{columns}

## Equivalent confidence level

\alert{What is the equivalent confidence level for a one-sided hypothesis test at $\alpha = 0.05$?}

\begin{columns}

\begin{column}{0.3\textwidth}
A) \alert{90\%}

B) 92.5%

C) 95%

D) 97.5%
\end{column}

\begin{column}{0.7\textwidth}
```{r, echo=F, message=F, warning=F,out.width="100%",fig.align='center'}
plot(c(-4, 4), c(0, dnorm(0)), type='n', axes=FALSE, ylab = "", xlab = "")
axis(1, at = c(-5,-1.72, 0, 1.72, 5), labels = c(NA, NA, 0, NA, NA))
abline(h=0)

X <- seq(-8, 8, 0.01)
Y <- dt(X, 9)
lines(X, Y)

these <- which(X < -1.72)
yy <- c(0, Y[these], 0)
these <- c(these[1], these, rev(these)[1])
xx <- X[these]
polygon(xx, yy, col=COL[1])

these <- which(X > 1.72)
yy <- c(0, Y[these], 0)
these <- c(these[1], these, rev(these)[1])
xx <- X[these]
polygon(xx, yy, col=COL[1])

text(0,0.6*max(Y), "90%", col = "red", cex = 3)
text(2,0.05*max(Y), "5%", col = "red", cex = 2)
text(-2,0.05*max(Y), "5%", col = "red", cex = 2)
```
\end{column}

\end{columns}

## Critical value

\alert{What is the appropriate $t^*$ for a confidence interval for the average difference between using a phone and not using it while driving?}

A) 1.32

B) 1.70

C) 2.07

D) 2.82


## Critical value

\alert{What is the appropriate $t^*$ for a confidence interval for the average difference between using a phone and not using it while driving?}

A) 1.32

B) \alert{1.70}

C) 2.07

D) 2.82

```{r, echo=T}
qt(p = 0.95, df = 29)
```


## Confidence interval

**Calculate the interval, and interpret it in context**

\pause

\centering{$\text{point estimate} \pm ME$}

\pause

\begin{eqnarray*}
(\bar{x}_{C} - \bar{x}_{T}) \pm t^\star_{df} \times SE &=& (0.356-0.546) \pm 1.70 \times 0.0593 \\
\pause
&=& -0.19 \pm  0.1008 \\
\pause
&=& (-0.2908, -0.0892)
\end{eqnarray*}

\pause

\raggedright We are 90% confident that they average reaction time of a driver not using a phone is 0.0892 to 0.2908 seconds faster than the average reaction time of a driver using a phone.

## Recap: Inference using difference of two small sample means

- If $\sigma_1$ or $\sigma_2$ is unknown, difference between the sample means follow a $t$-distribution with $SE = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_1}}$.

\pause

- Conditions:

  - Independence within groups (often verified by a random sample, and if sampling without replacement, n < 10% of population) and between groups.
  
  - No extreme skew in either group.
  
\pause

- Hypothesis testing:

\centering{$T_{df} = \frac{\text{point estimate} - \text{null value}}{SE}, \text{ where } df = min(n_1-1, n_2-1).$}

\pause

- Confidence interval:

\centering{$\text{point estimate} \pm t^*_{df} \times SE$}


# Computing the power for a 2-sample test

## 

\begin{center}
\begin{tabular}{l l | c c}
\multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Decision}} \\
& & fail to reject $H_0$ &  reject $H_0$ \\
  \cline{2-4}
& $H_0$ true & \onslide<4->{\textcolor{blue}{$1 - \alpha$}} & \onslide<2->{\textcolor{orange}{Type 1 Error, $\alpha$}} \\
\raisebox{1.5ex}{\textbf{Truth}} & $H_A$ true &  \onslide<3->{\textcolor{orange}{Type 2 Error, $\beta$}} & \onslide<5->{\textcolor{blue}{Power, $1 - \beta$}} \\
  \cline{2-4}
\end{tabular}
\end{center}

\pause

\begin{itemize}
\item Type 1 error is rejecting $H_0$ when you shouldn't have, and the probability of doing so is $\alpha$ (significance level)

\pause 

\item Type 2 error is failing to reject $H_0$ when you should have, and the probability of doing so is $\beta$ (a little more complicated to calculate)

\pause 

\item \textbf{Power} of a test is the probability of correctly rejecting $H_0$, and the probability of doing so is $1 - \beta$

\pause 

\item In hypothesis testing, we want to keep $\alpha$ and $\beta$ low, but there are inherent trade-offs.

\end{itemize}

## Type 2 error rate

If the alternative hypothesis is actually true, what is the chance that we make a Type 2 Error, i.e. we fail to reject the null hypothesis even when we should reject it?

  - The answer is not obvious.
  
  - If the true population average is very close to the null hypothesis value, it will be difficult to detect a difference (and reject $H_0$).
  
  - If the true population average is very different from the null hypothesis value, it will be easier to detect a difference.
  
  - Clearly, $\beta$ depends on the **effect size** ($\delta$).
  
## Example - Blood Pressure (BP), hypotheses

\alert{Suppose a pharmaceutical company has developed a new drug for lowering blood pressure, and they are preparing a clinical trial to test the drug's effectiveness. They recruit people who are taking a particular standard blood pressure medication, and half of the subjects are given the new drug (treatment) and the other half continue to take their current medication through generic-looking pills to ensure blinding (control). What are the hypotheses for a two-sided hypothesis test in this context?}

\pause 

\begin{align*}
H_0&: \mu_{treatment} - \mu_{control} = 0 \\
H_A&: \mu_{treatment} - \mu_{control} \ne 0  
\end{align*}

## Example - BP, standard error

\alert{Suppose researchers would like to run the clinical trial on patients with systolic blood pressures between 140 and 180 mmHg. Suppose previously published studies suggest that the standard deviation of the patients' blood pressures will be about 12 mmHg and the distribution of patient blood pressures will be approximately symmetric. If we had 100 patients per group, what would be the approximate standard error for difference in sample means of the treatment and control groups?}

\pause

\centering{$SE = \sqrt{\frac{12^2}{100} + \frac{12^2}{100}} = 1.70$}

## Example - BP, minimum effect size required to reject $H_0$

\alert{For what values of the difference between the observed averages of blood pressure in treatment and control groups (effect size) would we reject the null hypothesis at the 5\% significance level?}

\pause

![](power_null_B_0_1-7_with_rejection_region.pdf){height="50%"}

\pause

The difference should be at least

\centering{$1.96 \times 1.70 = 3.332.$}

\raggedright or at most

\centering{$-1.96 \times 1.70 = -3.332.$}


## Example - BP, power

\alert{Suppose that the company researchers care about finding any effect on blood pressure that is 3 mmHg or larger vs the standard medication. What is the power of the test that can detect this effect?}

\pause

![](power_null_C_0_1-7_with_alt_at_3.pdf){height="50%"}

\pause

\centering{$Z = \frac{-3.332-(-3)}{1.70} = 0.20$}

\pause

\centering{$P(Z < -0.20) = 0.4207$}

## Example - BP, required sample size for 80% power

\alert{What sample size will lead to a power of 80\% for this test?}

\pause

![](power_null_0_0-76_with_alt_at_3_and_shaded.pdf){height="50%"}

\pause

\centering{$SE = \frac{3}{2.8} = 1.07142$}

\pause

\centering{$1.07142 = \sqrt{\frac{12^2}{n} + \frac{12^2}{n}}$}

\pause

\centering{$n = 250.88 \rightarrow n \geq 251$}

## Recap

- Calculate required sample size for a desired level of power.

- Calculate power for a range of sample sizes, then choose the sample size that yields the target power (usually 80% or 90%).

![](power_curve_neg-3.pdf){height="70%"}

## Achieving desired power

There are several ways to increase power (and hence decrease type 2 error rate):

\pause

\begin{enumerate}

\item Increase the sample size.

\pause

\item Decrease the standard deviation of the sample, which essentially has the same effect as increasing the sample size (it will decrease the standard error). With a smaller $s$ we have a better chance of distinguishing the null value from the observed point estimate. This is difficult to ensure but cautious measurement process and limiting the population so that it is more homogeneous may help.

\pause

\item Increase $\alpha$, which will make it more likely to reject $H_0$ (but note that this has the side effect of increasing the Type 1 error rate).

\pause

\item Consider a larger effect size. If the true mean of the population is in the alternative hypothesis but close to the null value, it will be harder to detect a difference.

\end{enumerate}
