---
title: "Chapter 5"
subtitle: "Foundations for inference^[These notes use content from OpenIntro Statistics Slides by Mine Cetinkaya-Rundel.]"
author: |
  | (Author Name)
  | Department of Mathematics & Statistics
  | North Carolina A\&T State University
#date: "8/19/2020"
urlcolor: blue
header-includes:
    \usepackage{multirow}
    \usepackage{graphicx}
    \usepackage{subfigure}
    \usepackage{multicol}
    \usepackage[utf8]{inputenc}
    \usepackage[english]{babel}
    \usepackage{bm}
    \usepackage{amsmath}
    \usepackage{tikz}
    \usepackage{mathtools}
    \usepackage{textcomp}
    \usepackage{fdsymbol}
    \usepackage{siunitx}
    \usepackage{xcolor,pifont}
output: 
  beamer_presentation:
    fig_caption: true
    latex_engine: xelatex
    
    
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=F, message=F, warning=F}
library(readr)
library(openintro)
library(here)
library(tidyverse)
data(COL)
```

# Point estimates and sampling variability

## Point estimates and error

- We are often interested in **population parameters**.

- Complete populations are difficult to collect data on, so we use **sample statistics** as **point estimates** for the unknown population parameters of interest.

- **Error** in the estimate $\rightarrow$ difference between population parameter and sample statistic.

- **Bias** is systematic tendency to over- or under-estimate the true population parameter.

- **Sampling error** describes how much an estimate will tend to vary from one sample to the next.

- Much of statistics is focused on understanding and quantifying sampling error, and **sample size** is helpful for quantifying this error.

## Practice

\alert{Suppose we randomly sample 1,000 adults from each state in the U.S. Would you expect the sample means of their heights to be the same, somewhat different, or very different?}

## Practice

\alert{Suppose we randomly sample 1,000 adults from each state in the U.S. Would you expect the sample means of their heights to be the same, somewhat different, or very different?}

Not the same, but only somewhat different.

## 

![](pew1.png)

![](pew2.png)

![](pew3.png)

## Margin of error

![](pew4.png)

- $41\% \pm 2.9\%:$ We are 95% confident that 38.1% to 43.9% of the public believe yound adults, rather than middle-aged or older adults, are having the toughest time in today's economy.

- $49\% \pm 4.4\%:$ We are 95% confident that 44.6% to 53.4% of 18-34 years olds have taken a job they didn't want just to pay the bills.

## Practice

\alert{Suppose the proportion of American adults who support the expansion of solar energy is p = 0.88, which is our parameter of interest. Is a randomly selected American adult more or less likely to supoort the expansion of solar energy?}

## Practice

\alert{Suppose the proportion of American adults who support the expansion of solar energy is p = 0.88, which is our parameter of interest. Is a randomly selected American adult more or less likely to supoort the expansion of solar energy?}

More likely.

##

\alert{Suppose that you don't have access to the population of all American adults, which is a quite likely scenario. In order to estimate the proportion of American adults who support solar power expansion, you might sample from the population and use your sample proportion as the best guess for the unknown population proportion.}

- Sample, with replacement, 1000 American adults from the population, and record whether they support or not solar power expansion.

- Find the same proportion.

- Plot the distribution of the sample proportions obtained by members of the class.

##

```{r, echo=T}
# 1. Create a set of 250 million entries, where 88%
# of them are "support" and 12% are "not".

pop_size <- 250000000
possible_entries <- c(rep("support", 0.88 * pop_size),
                      rep("not", 0.12 * pop_size))

# 2. Sample 1000 entries without replacement.

sampled_entries <- sample(possible_entries,
                          size = 1000, replace = F)

# 3. Compute p-hat: count the number that are
# "support", then divide by # of the sample size

sum(sampled_entries == "support")/1000
```

## Sampling distribution

Suppose you were to repeat this process many times and obtain many $\hat{p}$s. This distribution is called a sampling distribution.

```{r, echo=F, message=F, warning=F, fig.width=8, fig.height=4,fig.align='center'}
phat <- rep(NA, 10000)
for(i in 1:10000){
  sampled_entries <- sample(possible_entries, size = 1000)
  phat[i] <- sum(sampled_entries == "support") / 1000
}

# df

sampling <- tibble(phat = phat)

# plot

ggplot(sampling, aes(x = phat)) +
  geom_histogram(binwidth = 0.01, fill = COL[1]) +
  theme_minimal(base_size = 14) +
  labs(x = "Simulated sample proportion", y = "")
```

## Practice

\alert{What is the shape and center of this distribution? Based on this distribution, what do you think is the true population proportion?}

```{r, echo=F, message=F, warning=F, fig.width=8, fig.height=4,fig.align='center'}
ggplot(sampling, aes(x = phat)) +
  geom_histogram(binwidth = 0.01, fill = COL[1]) +
  theme_minimal(base_size = 14) +
  labs(x = "Simulated sample proportion", y = "")
```

## Practice

\alert{What is the shape and center of this distribution? Based on this distribution, what do you think is the true population proportion?}

```{r, echo=F, message=F, warning=F, fig.width=8, fig.height=4,fig.align='center'}
ggplot(sampling, aes(x = phat)) +
  geom_histogram(binwidth = 0.01, fill = COL[1]) +
  theme_minimal(base_size = 14) +
  labs(x = "Simulated sample proportion", y = "")
```

The distribution is unimodal and roughly symmetric. A reasonable guess for the true population proportion is the center of this distribution, approximately 0.88.

## Sampling distributions are never observed

- In real-world applications, we never actually observe the sampling distribution, yet it is useful to always think of a point estimate as coming from such a hypothetical distribution.

- Understanding the sampling distribution will help us characterize and make sense of the point estimates that we do observe.

## Central Limit Theorem

Sample proportions will be nearly normally distributed with mean equal to the population proportion, $p$, and standard error equal to $\sqrt{\frac{p(1-p)}{n}}$.

\centering{$\hat{p} \sim N \left(mean=p, SE = \sqrt{\frac{p(1-p)}{n}} \right)$}

- It wasn't a coincidence that the sampling distribution we saw earlier was symmetric, and centered at the true population proportion.

- We won't go through a detailed proof of why $SE = \sqrt{\frac{p(1-p)}{n}}$, but note that as $n$ increases $SE$ decreases.
    
  - As $n$ increases samples will yield more consistent $\hat{p}$s, i.e. variability among $\hat{p}$s will be lover.
  
## CLT - conditions

Certain conditions must be met for the CLT to apply:

\begin{enumerate}

\item \textbf{Independence:} Sampled observations must be independent. This is difficult to verify, but is more likely if
  
  \begin{itemize}
  \small
  \item random sampling/assignment is used, and
  \item if sampling without replacement, $n < 10\%$ of the population.
  \normalsize
  \end{itemize}
  
\item \textbf{Sample size:} There should be at least 10 expected successes and 10 expected failures in the observed sample. This is difficult to verify if you don't know the population proportion (or can't assume a value for it). In those cases we look for the number of observed successes and failures to be at least 10.

\end{enumerate}

## When $p$ is unknown

- The CLT states $SE = \sqrt{\frac{p(1-p)}{n}}$, with the condition that $np$ and $n(1-p)$ are at least 10, however we often don't know the value of $p$, the population proportion.

- In these cases we substitute $\hat{p}$ for $p$.

## When $p$ is low

\alert{Suppose we have a population where the true population proportion is $p=0.05,$ and we take random sample of size $n = 50$ from this population. We calculate the sample proportion in each sample and plot these proportions. Would you expect this distribution to be nearly normal? Why, or why not?}

## When $p$ is low

\alert{Suppose we have a population where the true population proportion is $p=0.05,$ and we take random sample of size $n = 50$ from this population. We calculate the sample proportion in each sample and plot these proportions. Would you expect this distribution to be nearly normal? Why, or why not?}

No, the success-failure condition is not met $(50 *0.05 = 2.5)$, hence we would not expect the sampling distribution to be nearly normal.

```{r, echo=F, message=F, warning=F, fig.width=4, fig.height=1.8,fig.align='center'}
p <- 0.05

N <- 100000000
ones <- N * p
zeros <- N * (1-p)
pop <- c(rep(1, ones), rep(0, zeros))

n <- 50

set.seed(12345)

sampling <- tibble(
  phat = rep(NA, 1000)
)

for(i in 1:nrow(sampling)){
  sampling$phat[i] <- sum(sample(pop, n)) / n
}

ggplot(sampling, aes(x = phat)) +
  geom_histogram(binwidth = 0.01, fill = COL[1]) +
  theme_minimal() +
  labs(x = "Simulated sample proportion", y = "")
```

##

\alert{What happens when $np$ and/or $n(1-p)<10$?}

\begin{multicols}{2}

\includegraphics[width=1\columnwidth]{clt_prop_grid_1.pdf}

\columnbreak

\includegraphics[width=1\columnwidth]{clt_prop_grid_2.pdf}

\end{multicols}

## When the conditions are not met...

- When either $np$ or $n(1-p)$ is small, the distribution is more discrete.

- When $np$ or $n(1-p)<10$, the distribution is more skewed. 

- The larger both $np$ and $n(1-p)$, the more normal the distribution.

- When $np$ and $n(1-p)$ are both very large, the discreteness of the distribution is hardly evident, and the distribution looks much more like a normal distribution.

## Extending the framework for other statistics

- The strategy of using sample statistic to estimate a parameter is quite common, and it's a strategy that we can apply to other statistics besides a proportion.

  - Take a random sample of students at a college and ask them how many extracurricular activities they are involved in to estimate the average number of extra curricular activities all students in this college are interested in.
  
- The principles and general ideas are from this chapter apply to other parameters as well, even if the details change a little.

# Confidence intervals for a proportion

## Confidence intervals

- A plausible range of values for the population parameter is called a **confidence interval**.

- Using only a sample statistic to estimate a parameter is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net.

\begin{multicols}{3}

\includegraphics[width=1\columnwidth]{spear.jpg}

\columnbreak

We can throw a spear where we saw a fish but we will probably miss. If we toss a net in that area, we have a good chance of catching the fish.

\columnbreak

\includegraphics[width=1\columnwidth]{net.jpg}

\end{multicols}

- If we report a point estimate, we probably won't hit the exact population parameter. If we report a range of plausible values we have a good shot at capturing the parameter.

## Facebook's categorization of user interests

Most commercial websites (e.g. social media platforms, news outlets, online retailers) collect a data about their users' behaviors and use these data to deliver targeted content, recommendations, and ads. To understand whether Americans think their lives line up with how the algorithm-driven classification systems categorizes them, Pew Research asked a representative sample of 850 American Facebook users how accurately they feel the list of categories Facebook has listed for them on the page of their supposed interests actually represents them and their interests. 67% of the respondents said that the listed categories were accurate. Estimate the true proportion of American Facebook users who think the Facebook cateforizes their interests accurately.

## Facebook's categorization of user interests

\centering{$\hat{p} = 0.67$ \text{    } $n=850$}

## Facebook's categorization of user interests

\centering{$\hat{p} = 0.67$ \text{    } $n=850$}

\raggedright The approximate 95% confidence interval is defined as

\centering{$point \text{ }estimate \pm 1.96 \times SE$}

## Facebook's categorization of user interests

\centering{$\hat{p} = 0.67$ \text{    } $n=850$}

\raggedright The approximate 95% confidence interval is defined as

\centering{$point \text{ }estimate \pm 1.96 \times SE$}

\centering{$SE = \sqrt{\frac{p(1-p)}{n}} = \sqrt{\frac{0.67 \times 0.33}{850}} \approx 0.016$}

## Facebook's categorization of user interests

\centering{$\hat{p} = 0.67$ \text{    } $n=850$}

\raggedright The approximate 95% confidence interval is defined as

\centering{$point \text{ }estimate \pm 1.96 \times SE$}

\centering{$SE = \sqrt{\frac{p(1-p)}{n}} = \sqrt{\frac{0.67 \times 0.33}{850}} \approx 0.016$}

\begin{align*}
\hat{p} \pm 1.96 \times SE &= 0.67 \pm 1.96 + 0.03\\
&= (0.67-0.03, 0.67+0.03)\\
&= (0.64, 0.70)
\end{align*}

## Practice

\alert{Which of the following is the correct interpretation of this confidence interval?}

We are 95% confident that

A) 64% to 70% of American Facebook users in this sample think Facebook categorizes their interests accurately.

B) 64% to 70% of all American Facebook users think Facebook categorizes their interests accurately.

C) There is a 64% to 70% chance that a randomly chosen American Facebook user's interests are categorized accurately.

D) There is a 64% to 70% chance that 95% of American Facebook users' interests are categorized accurately.

## Practice

\alert{Which of the following is the correct interpretation of this confidence interval?}

We are 95% confident that

A) 64% to 70% of American Facebook users in this sample think Facebook categorizes their interests accurately.

B) **64% to 70% of all American Facebook users think Facebook categorizes their interests accurately.**

C) There is a 64% to 70% chance that a randomly chosen American Facebook user's interests are categorized accurately.

D) There is a 64% to 70% chance that 95% of American Facebook users' interests are categorized accurately.

## What does 95% confident mean?

- Suppose we took many sample and built a confidence interval from each sampling using the equation

\centering{$point \text{ }estimate \pm 1.96 \times SE$}

- Then about 95% of those intervals would contain the true population proportion (p).

## Width of an interval

\alert{If we want to be more certain that we capture the population parameter, i.e. increase our confidence level, should we use a wider interval or a smaller interval?}

## Width of an interval

\alert{If we want to be more certain that we capture the population parameter, i.e. increase our confidence level, should we use a wider interval or a smaller interval?}

A wider interval.

## Width of an interval

\alert{If we want to be more certain that we capture the population parameter, i.e. increase our confidence level, should we use a wider interval or a smaller interval?}

A wider interval.

\alert{Can you see any drawbacks to using a wider interval?}

![](garfield.png)

## Width of an interval

\alert{If we want to be more certain that we capture the population parameter, i.e. increase our confidence level, should we use a wider interval or a smaller interval?}

A wider interval.

\alert{Can you see any drawbacks to using a wider interval?}

![](garfield.png)

If the interval is too wide it may not be very informative.

## Changing the confidence level

\centering{$point \text{ }estimate \pm z^* \times SE$}

- In a confidence interval, $z^* \times SE$ is called the **margin of error**, and for a given sample, the margin of error changes as the confidence level changes.

- In oder to change the confidence level we need to adjust $z^*$ in the above formula.

- Commonly used confidence levels in practice are 90%, 95%, 98% and 99%.

- For a 95% confidence interval, $z^* = 1.96$.

- However, using the standard normal $(z)$ distribution, it is possible to find the appropriate $z^*$ for any confidence level.

## Practice

\alert{Which of the below Z scores is the appropriate $z^*$ when calculating a 98\% confidence interval?}

A) Z = 2.05 B) Z = 1.96 C) Z = 2.33 D) Z = -2.33 E) Z = -1.65

## Practice

\alert{Which of the below Z scores is the appropriate $z^*$ when calculating a 98\% confidence interval?}

A) Z = 2.05 B) Z = 1.96 C) **Z = 2.33** D) Z = -2.33 E) Z = -1.65

```{r, echo=F, message=F, warning=F, fig.width=8, fig.height=4,fig.align='center'}
normTail(M = c(-2.33,2.33), col = COL[6,4], cex.axis = 1.25)
text(x = 0, y = 0.25, "0.98", col = COL[4], cex = 1.5)
lines(x = c(-2.33,-2.33), y = c(0, 0.15), lty = 2)
lines(x = c(2.33,2.33), y = c(0, 0.15), lty = 2)
text(x = -2.33, y = 0.20, "z = -2.33", col = COL[1], cex = 1.5)
text(x = 2.33, y = 0.20, "z = 2.33", col = COL[1], cex = 1.5)
text(x = -2.7, y = 0.03, "0.01", col = COL[4], cex = 1.5)
text(x = 2.7, y = 0.03, "0.01", col = COL[4], cex = 1.5)
```

## Interpreting confidence intervals

Confidence intervals are ...

- Always about the population.
- Not probability statements.
- Only about population parameters, not individual observation.
- Only reliable if the sample statistic they're based on is an unbiased estimator of the population parameter.

# Hypothesis testing for a proportion

## Remember when...

Gender discrimination experiment:

\begin{tabular}{ll  cc c} 
  		&				& \multicolumn{2}{c}{\textit{Promotion}} \\
\cline{3-4}
							&			& Promoted	& Not Promoted 	& Total	\\
\cline{2-5}
\multirow{2}{*}{\textit{Gender	}}	&Male 		& 21	 	& 3		& 24 	\\
							&Female		& 14	 	& 10 	 	& 24 \\
\cline{2-5}
							&Total		& 35		& 13		& 48 \\
\end{tabular}

## Remember when...

Gender discrimination experiment:

\begin{tabular}{ll  cc c} 
  		&				& \multicolumn{2}{c}{\textit{Promotion}} \\
\cline{3-4}
							&			& Promoted	& Not Promoted 	& Total	\\
\cline{2-5}
\multirow{2}{*}{\textit{Gender	}}	&Male 		& 21	 	& 3		& 24 	\\
							&Female		& 14	 	& 10 	 	& 24 \\
\cline{2-5}
							&Total		& 35		& 13		& 48 \\
\end{tabular}

\centering{$\hat{p}_{males}=21/24 \approx 0.88$ and $\hat{p}_{females}=14/24 \approx 0.58$}

## Remember when...

Gender discrimination experiment:

\begin{tabular}{ll  cc c} 
  		&				& \multicolumn{2}{c}{\textit{Promotion}} \\
\cline{3-4}
							&			& Promoted	& Not Promoted 	& Total	\\
\cline{2-5}
\multirow{2}{*}{\textit{Gender	}}	&Male 		& 21	 	& 3		& 24 	\\
							&Female		& 14	 	& 10 	 	& 24 \\
\cline{2-5}
							&Total		& 35		& 13		& 48 \\
\end{tabular}

\centering{$\hat{p}_{males}=21/24 \approx 0.88$ and $\hat{p}_{females}=14/24 \approx 0.58$}

\raggedright Possible explanations:

- Promotion and gender are **independent**, no gender discrimination, observed difference in proportions is simply due to chance. $\rightarrow$ \alert{null} - (nothing is going on)

- Promotion and gender are **dependent**, there is gender discrimination, observed difference in proportions is not due to chance. $\rightarrow$ \alert{alternative} - (something is going on)

## Result

```{r, echo=F, message=F, warning=F, fig.width=10, fig.height=5,fig.align='center'}
set.seed(8535)

gender <- c(rep('male', 24), rep('female', 24))
outcome <- c(rep(c('promoted', 'not promoted'), c(21, 3)), rep(c('promoted', 'not promoted'), c(14, 10)))

nsim  = 100
n     = length(gender)
group = gender
var1  = outcome
success = "promoted"
sim   = matrix(NA, nrow = n, ncol = nsim)
n1    = n2 = 24

statistic <- function(var1, group){	
	t1 <- var1 == success & group == levels(as.factor(group))[1]
	t2 <- var1 == success & group == levels(as.factor(group))[2]
	sum(t1)/n1 - sum(t2)/n2 
}

for(i in 1:nsim){
	sim[,i] = sample(group, n, replace = FALSE)
}

sim_dist = apply(sim, 2, statistic, var1 = outcome)
diffs    = sim_dist
pval     = sum(diffs >= 0.29) / nsim
values  <- table(sim_dist)


X <- c()
Y <- c()
for(i in 1:length(diffs)){
	x   <- diffs[i]
	rec <- sum(sim_dist == x)
	X   <- append(X, rep(x, rec))
	Y   <- append(Y, 1:rec)
}

plot(X, Y, xlim=range(diffs)+c(-1,1)*sd(diffs)/4, xlab = "Difference in promotion rates", ylab= "", axes = FALSE, ylim=c(0,max(Y)), col=COL[1], cex=2, cex.axis = 3,cex.lab=1.5, pch=20)
axis(1, at = seq(-0.4,0.4,0.1), labels = c(-0.4,"",-0.2,"",0,"",0.2,"",0.4))
abline(h=0)
```


## Result

```{r, echo=F, message=F, warning=F, fig.width=10, fig.height=5,fig.align='center'}
plot(X, Y, xlim=range(diffs)+c(-1,1)*sd(diffs)/4, xlab = "Difference in promotion rates", ylab= "", axes = FALSE, ylim=c(0,max(Y)), col=COL[1], cex=2, cex.axis = 3,cex.lab=1.5, pch=20)
axis(1, at = seq(-0.4,0.4,0.1), labels = c(-0.4,"",-0.2,"",0,"",0.2,"",0.4))
abline(h=0)
```

Since it was quite unlikely to obtain results like the actual data or something more extreme in the simulations (male promotions being 30% or more higher than female promotions), we decided to reject the null hypothesis in favor of the alternative.

## Recap: Hypothesis testing framework

- We start with a **null hypothesis** $\mathbf{(H_0)}$ that represents the status quo.

- We also have an **alternative hypothesis** $\mathbf{(H_A)}$ that represents our research question, i.r. what we're testing for.

- We conduct a hypothesis test under the assumption that the null hypothesis is true, either via simulation or traditional methods based on the central limit theorem (coming up next...).

- If the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we stick with the null hypothesis. If they do, then we reject the null hypothesis in favor of the alternative.

We'll formally introduce the hypothesis testing framework using an example on testing a claim about a proportion.

## Testing hypotheses using confidence intervals

\alert{Earlier we calculated a 95\% confidence interval for the proportion of American Facebook users who think Facebook categorizes their interests accurately as 64\% to 67\%. Based on this confidence interval, do the data support the hypothesis that majority of American Facebook users think facebook categorizes their interests accurately.}

## Testing hypotheses using confidence intervals

\alert{Earlier we calculated a 95\% confidence interval for the proportion of American Facebook users who think Facebook categorizes their interests accurately as 64\% to 67\%. Based on this confidence interval, do the data support the hypothesis that majority of American Facebook users think facebook categorizes their interests accurately.}

- The associated hypotheses are:
  
  $H_0: p = 0.50: 50\%$ of American Facebook users think Facebook categorizes their interests accurately.
  
  $H_A: p>0.50:$ More than 50% of American Facebook users think Facebook categorizes their interests accurately.
  
- Null value is not included in the interval $\rightarrow$ reject the null hypothesis.

- This is a quick-and-dirty approach for hypothesis testing, but it doesn't tell us the likelihood of certain outcomes under the null hypothesis (p-value).

## Decision errors

- Hypothesis tests are not flawless.

- In the court system innocent people are sometimes wrongly convicted and the guilty sometimes walk free.

- Similarly, we can make a wrong decision in statistical hypothesis tests as well.

0 The difference is that we have the tools necessary to quantify how often we make errors in statistics.

## Decision errors

There are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a decision about which might be true, but our choice might be incorrect.

\renewcommand{\arraystretch}{1.5}
\begin{center}
\begin{tabular}{l l | c c}
\multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Decision}} \\
& & fail to reject $H_0$ &  reject $H_0$ \\
  \cline{2-4}
& $H_0$ true & {{$\checkmark$}} &  {\textcolor{red}{Type 1 Error}} \\
\raisebox{1.5ex}{\textbf{Truth}} & $H_A$ true & {\textcolor{red}{Type 2 Error}} & {{$\checkmark$}} \\
  \cline{2-4}
\end{tabular}
\end{center}
\renewcommand{\arraystretch}{1}

- A **Type 1 Error** is rejecting the null hypothesis when $H_0$ is true.

- A **Type 2 Error** is failing to reject the null hypothesis when $H_A$ is true.

- We (almost) never know if $H_0$ or $H_A$ is true, but we need to consider all possibilities.

## Hypothesis Test as a trial

If we again think of a hypothesis test as a criminal trial then it makes sense to frame the verdict in terms of the null and alternative hypotheses:

\centering{$H_0:$ Defendant is innocent}

\centering{$H_A:$ Defendant is guilty}

\raggedright Which type of error is being committed in the following circumstances?

- Declaring the defendant innocent when they are actually guilty

- Declaring the defendant guilty when they are actually innocent

## Hypothesis Test as a trial

If we again think of a hypothesis test as a criminal trial then it makes sense to frame the verdict in terms of the null and alternative hypotheses:

\centering{$H_0:$ Defendant is innocent}

\centering{$H_A:$ Defendant is guilty}

\raggedright Which type of error is being committed in the following circumstances?

- Declaring the defendant innocent when they are actually guilty

\centering{\alert{Type 2 error}}

- Declaring the defendant guilty when they are actually innocent

\centering{\alert{Type 1 error}}

## Hypothesis Test as a trial

\alert{Which error do you think is the worse error to make?}

## Hypothesis Test as a trial

\alert{Which error do you think is the worse error to make?}

"Better that ten guilty person escape than that one innocent suffer"

\raggedleft{$-$ William Blackstone}

## Type 1 error rate

- As a general rule we reject $H_0$ when the p-value is less than 0.05, i.e. we use a **significance level** of 0.05, $\mathbf{\alpha = 0.05}$.

- This means that, for those cases where $H_0$ is actually true, we do not want to incorrectly reject it more than 5% of those times.

- In other words, when using a 5% significance level there is about 5% chance of making a Type 1 error if the null hypothesis is true.

\centering{$P$(Type 1 error | $H_0$ true) $= \alpha$}

- This is why we prefer small values of $\alpha -$ increasing $\alpha$ increasing the Type 1 error rate.

## Facebook interest categories

\alert{The same survey asked the 850 respondents how confortable they are with Facebook creating a list of categories for them. 41\% of the respondents said they are comfortable. Do these data provide convincing evidence that they proportion of American Facebook users are comfortable with Facebook creating a list of interest categories for them is different than 50\%?}



\tiny [https://www.pewinternet.org/2019/01/16/facebook-algorithms-and-personal-data/](https://www.pewinternet.org/2019/01/16/facebook-algorithms-and-personal-data/)

## Setting the hypotheses

- The **parameter of interest** is the proportion of \underline{all} American Facebook users who are comfortable with Facebook creating categories of interests for them.

- There may be two explanations why our sample proportion is lower than 0.50 (minority).
  - The true population proportion is different than 0.50.
  - The true population mean is 0.50, and the difference between the true population proportion and the sample proportion is simply due to natural sampling variability.
  
## Setting the hypotheses

- We start with the assumption that 50% of American Facebook users are comfortable with Facebook creating categories of interests for them

\centering{$\mathbf{H_0:} p = 0.50$}

- We test the claim that the proportion of American Facebook users who are comfortable with Facebook creating categories of interests for them is different than 50%

\centering{$\mathbf{H_A:} p \neq 0.50$}

## Facebook interest categories - conditions

\alert{Which of the following is \underline{not} a condition that needs to be met to proceed with this hypothesis test?}

A) Respondents in the sample should be independent of each other with respect to whether or not they feel comfortable with their interests being categorized by Facebook.

B) Sampling should have been done randomly.

C) The sample size should be less than 10% of the population of all American Facebook users.

D) There should be at least 30 respondents in the sample.

E) There should be at least 10 expected successes and 10 expected failure.

## Facebook interest categories - conditions

\alert{Which of the following is \underline{not} a condition that needs to be met to proceed with this hypothesis test?}

A) Respondents in the sample should be independent of each other with respect to whether or not they feel comfortable with their interests being categorized by Facebook.

B) Sampling should have been done randomly.

C) The sample size should be less than 10% of the population of all American Facebook users.

D) **There should be at least 30 respondents in the sample.**

E) There should be at least 10 expected successes and 10 expected failure.

## Test statistic

In order to evaluate if the observed sample proportion is unusual for the hypothesized sampling distribution, we determine how many standards error away from the null it is, which is also called the **test statistic**.

\centering{$\hat{p} \sim N \left( \mu = 0.50, SE = \sqrt{\frac{0.50 \times 50}{850}} \right)$}

\centering{$Z = \frac{0.41-0.50}{0.0171}=-5.26$}

## Test statistic

In order to evaluate if the observed sample proportion is unusual for the hypothesized sampling distribution, we determine how many standards error away from the null it is, which is also called the **test statistic**.

\centering{$\hat{p} \sim N \left( \mu = 0.50, SE = \sqrt{\frac{0.50 \times 50}{850}} \right)$}

\centering{$Z = \frac{0.41-0.50}{0.0171}=-5.26$}

\raggedright \alert{The sample proportion is 5.26 standard errors away from the hypothesized value. Is this considered unusually low? That is, is the result \textbf{statistically significant}?}

## Test statistic

In order to evaluate if the observed sample proportion is unusual for the hypothesized sampling distribution, we determine how many standards error away from the null it is, which is also called the **test statistic**.

\centering{$\hat{p} \sim N \left( \mu = 0.50, SE = \sqrt{\frac{0.50 \times 50}{850}} \right)$}

\centering{$Z = \frac{0.41-0.50}{0.0171}=-5.26$}

\raggedright \alert{The sample proportion is 5.26 standard errors away from the hypothesized value. Is this considered unusually low? That is, is the result \textbf{statistically significant}?}

Yes, and we can quantify how unusual it is using p-value.

## P-values

- We then use this test statistic to calculate the **p-value**, the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true.

- If the p-value is **low** (lower than the significance level, $\alpha$, which is usually 5%) we say it would be very unlikely to observe the data if the null hypothesis were true, and hence **reject** $\mathbf{H_0}$.

- If the p-value is **high** (higher than $\alpha$) we say that it is likely to observe the data even if the null hypothesis were true, and hence **do not reject** $\mathbf{H_0}$.

## Facebook interest categories - P-value

**p-value:** probability of observing data at least as favorable to $H_A$ as our current data set (a sample proportion lower than 0.41), if in fact $H_0$ were true (the true population proportion was 0.50)

\centering{P($\hat{p} < 0.41$ or $\hat{p} > 0.59$) = P($|Z| > 5.26$) $<0.0001$}

## Facebook interest categories - Making a decision

- p-value < 0.0001
  
  - If 50% of all American FB users are comfortable with FB creating these interest categories, there is less than a 0.01% chance these interest categories, there is less than a 0.01% chance of observing random sample of 850 American Facebook users where 41% or fewer or 59% or higher feel comfortable with it.
  
  - Pretty low probability to think that observed sample proportion, or something more extreme, is likely to happen by chance.
  
- Since p-value is **low** (lower than 5%), we reject $\mathbf{H_0}$.

- The data provide convincing evidence that the proportion of American FB users who are comfortable with FB creating a list of interest categories for them is different than 50%.

- The difference between the null value of 0.50 and observed sample proportion of 0.41 is **not due to chance** or sampling variability.

## Choosing a significance level

- While the traditional level is 0.05, it is helpful to adjust the significance level based on the application.

- Select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.

- If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01). Under this scenario we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring $H_A$ before we would reject $H_0$.

- If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g. 0.10). Here we want to be cautious about failing to reject $H_0$ when the null is actually false.

## One vs. Test sided hypothesis tests

- In two sided hypothesis tests we are interested in whether $p$ is either  above or below some null value $p_0: H_A: p \neq p_0$.

- In one sided hypothesis test we are interested in $p$ differing from the null value $p_0$ in one direction (and not the other):

  - If there is only value in detecting if population parameter is less than $p_0$, then $H_A: p<p_0$.
  
  - If there is only value in detecting if population parameter is greater than $p_0$ than $H_A: p>p_0$.
  
- Two-sided tests are often more appropriate as we often want to detect if the data goes clearly in the opposite direction of our alternative hypothesis as well.




